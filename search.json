[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Healthy Aging and Statistical Outcomes Lab (HASO Lab)",
    "section": "",
    "text": "Welcome!\nWelcome to this humble home! You will find materials related to my classes at CSU Stan under the Creative Commons License (CC). I have dedicated a large portion of my career to study applied statistics in psychology. I’m still learning new topics and methods, my new passion is Bayesian inference to estimate latent variable models.\nI also conduct research in healthy aging, and social factors affecting the risk of dementia. My aim is to expand this lab and share as much information as possible, not only with students but also with colleagues around the globe.\nI frequently work programming in R language, so you might find examples and materials to learn R along with data sets you can use to practice your R skills.\nThis repository is still under construction, it is a timid alpha version written with the help of Quarto. If you have suggestions on how to improve this site please contact me at emontenegro1@csustan.edu..",
    "crumbs": [
      "Home",
      "Content",
      "Healthy Aging and Statistical Outcomes Lab (HASO Lab)"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Esteban Montenegro",
    "section": "",
    "text": "I’m an Assistant Professor at the Psychology and Child Development Department. I focus my research ideas in problems related to quantitative psychology. I’m always doing research on latent variable modeling. I applied my passion for statistics in topics related to healthy aging and dementia.\nWhen I’m not doing something dorky, I enjoy hiking with my lovely wife and my dog. I used to play guitar and bass guitar, but academia is a place of many projects."
  },
  {
    "objectID": "assigment4.html",
    "href": "assigment4.html",
    "title": "Psych 3000",
    "section": "",
    "text": "Welcome to PSYC 3000! At this point you might be thinking: “I decided to study psychology to avoid numbers!”\nBut numbers are not avoidable if you want to be scientist or scientist practitioner. It might be boring sometimes, but other times you might have fun answering questions related to NATURE. Yes, NATURE!\nThis is a psychology course, and my aim is to study statistics as a science that helps other sciences to study NATURE. Psychological processes are also part of NATURE."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#big-research-families",
    "href": "lecture1/ResearchDesigns.html#big-research-families",
    "title": "Research Designs Review",
    "section": "Big research families",
    "text": "Big research families\n\nAlternative research designs (Creswell & Creswell, 2017)\n\n\n\n\n\n\n\nQuantitative\nQualitative\nMixed Methods\n\n\n\n\nExperimental designs\nNarrative Research\nConvergent\n\n\nNon-experimental\nPhenomenology\nExplanatory sequential\n\n\nLongitudinal Designs\nGrounded Theory\nExploratory sequential\n\n\n\nEthnographies\nComplex designs with embedded core designs\n\n\n\nCase Study"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#big-research-families-ii",
    "href": "lecture1/ResearchDesigns.html#big-research-families-ii",
    "title": "Research Designs Review",
    "section": "Big research families II",
    "text": "Big research families II\n\nSurvey research: provides a quantitative or numeric description of trends, attitudes, or opinions of a population by studying a sample of that population. It includes cross-sectional and longitudinal studies using questionnaires or structured interviews for data collection—with the intent of generalizing from a sample to a population.\nExperimental research: seeks to determine if a specific treatment influences an outcome. The researcher assesses this by providing a specific treatment to one group and withholding it from another and then determining how both groups scored on an outcome.\n\nTrue experiments\nQuasi-experiments\n\nSingle-subject designs"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#big-research-families-iii",
    "href": "lecture1/ResearchDesigns.html#big-research-families-iii",
    "title": "Research Designs Review",
    "section": "Big research families III",
    "text": "Big research families III\nQualitative designs\n\nNarrative research: The information retold or restoried by the researcher into a narrative chronology. Often, in the end, the narrative combines views from the participant’s life with those of the researcher’s life in a collaborative narrative\nPhenomenological research: the researcher describes the lived experiences of individuals about a phenomenon as described by participants.\nGrounded theory: is a design of inquiry from sociology in which the researcher derives a general, abstract theory of a process, action, or interaction grounded in the views of participants.\nEthnography: is a design of inquiry coming from anthropology and sociology in which the researcher studies the shared patterns of behaviors, language, and actions of an intact cultural group in a natural setting over a prolonged period of time.\nCase studies: in-depth analysis of a case, often a program, event, activity, process, or one or more individuals."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#what-is-first-a-or-b",
    "href": "lecture1/ResearchDesigns.html#what-is-first-a-or-b",
    "title": "Research Designs Review",
    "section": "What is first A or B ?",
    "text": "What is first A or B ?\n\nCausality means that we would expect variable X to cause variable Y.\n\nFor example: Does low self esteem cause depression? How do we know?\n\n\nLet’s take a look at some spurious correlations:"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#experiments-help-us",
    "href": "lecture1/ResearchDesigns.html#experiments-help-us",
    "title": "Research Designs Review",
    "section": "Experiments help us!",
    "text": "Experiments help us!\n\nCan we know if A causes B with a survey?\nCan we know if A causes B conducting an experiment?\nWe can manipulate a variable and observe what happens afterwards, but it is good enough?\nDo we need something more on our design?"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#experiments-help-us-ii",
    "href": "lecture1/ResearchDesigns.html#experiments-help-us-ii",
    "title": "Research Designs Review",
    "section": "Experiments help us! II",
    "text": "Experiments help us! II\n\nPure experiments need a control group to account for counterfactual information, this also helps to rule out possible confounding variables.\n\nExample: how would you measure the effect of physical activity on cardiovascular fitness? What would be a good experimental design?"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#can-we-assume-causality-in-survey-designs",
    "href": "lecture1/ResearchDesigns.html#can-we-assume-causality-in-survey-designs",
    "title": "Research Designs Review",
    "section": "Can we assume causality in survey designs ?",
    "text": "Can we assume causality in survey designs ?\n\nIn survey designs we cannot manipulate the independent variable, but some researchers claim that is possible to make causal inferences when you conduct a longitudinal study.\nIn longitudinal studies you satisfy the temporal requirement, you could evaluate if X = independent variable happens before Y = dependent variable.\n\nFor instance: You could measure a baby’s weight every month and evaluate how many times the baby is breastfed. But, do we need counterfactual information?"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect",
    "href": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect",
    "title": "Research Designs Review",
    "section": "But we haven’t defined cause, and effect",
    "text": "But we haven’t defined cause, and effect\nShadish et al. (2002) :\n\nLet’s try an example, consider a forest fire:\n\nMultiple causes: match tossed from a car, a lightning strike, or a smoldering campfire\nNone of these causes is necessary because a forest fire can start even when, say, a match is not present. Also, none of them is sufficient to start the fire. After all, a match must stay “hot” long enough to start combustion; it must contact\ncombustible material such as dry leaves. We also need oxygen!"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect-ii",
    "href": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect-ii",
    "title": "Research Designs Review",
    "section": "But we haven’t defined cause, and effect II",
    "text": "But we haven’t defined cause, and effect II\n\nA match can be consider part of multiple causes, therefore we can call it a “inus condition” (Mackie, 1974, p. 62) “an insufficient but nonredundant part of an unnecessary but sufficient condition” to cause a fire.\n\nShadish et al. (2002) :\n\nIt is insufficient because a match cannot start a fire without the other conditions.\nIt is nonredundant only if it adds something fire-promoting that is uniquely different from what the other factors in the constellation (e.g., oxygen, dry leaves) contribute to starting a fire. It is part of a sufficient condition to start a fire in combination with the full constellation of factors. But that condition is not necessary because there are other sets of conditions that can also start fires.\n\n\n\n\n\n\n\nImportant\n\n\nMany factors are usually required for an effect to occur, but we rarely know all of them and how they relate to each other. This is one reason that the causal relationships we discuss are not deterministic but only increase the probability that an effect will occur."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect-iii",
    "href": "lecture1/ResearchDesigns.html#but-we-havent-defined-cause-and-effect-iii",
    "title": "Research Designs Review",
    "section": "But we haven’t defined cause, and effect III",
    "text": "But we haven’t defined cause, and effect III\nWhat is an effect?\n\nAn effect is better define if we have a counterfactual model.\nA counterfactual is something that is contrary to fact.\nIn an experiment we observe what did happen when people received a treatment.\nThe counterfactual is knowledge of what would have happened to those same people if they simultaneously had not received treatment. An effect is the difference between what did happen and what would have happened.\n\n\n\n\n\n\n\nImportant\n\n\nWe could add a group of participants to a waiting list, do you have any example in mind?"
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#lets-finally-define-causal-relationship",
    "href": "lecture1/ResearchDesigns.html#lets-finally-define-causal-relationship",
    "title": "Research Designs Review",
    "section": "Let’s finally define causal relationship",
    "text": "Let’s finally define causal relationship\nShadish et al. (2002) :\n\nThis definition was first coined by John Stuart Mill (19th-century philosopher), a causal relationship exists if:\n\nThe cause preceded the effect.\nThe cause was related to the effect.\nWe can find no plausible alternative explanation for the effect other than the cause.\n\n\n\n\n\n\n\n\nWarning\n\n\nCorrelation does not prove causation!!! We will use this as a mantra in this class."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#independent-variable",
    "href": "lecture1/ResearchDesigns.html#independent-variable",
    "title": "Research Designs Review",
    "section": "Independent variable",
    "text": "Independent variable\n\nIndependent variables: are those that influence, or affect outcomes in experimental studies. They are described as “independent” because they are variables that are manipulated in an experiment and thus independent of all other influences.\nHowever, we will use this concept more vaguely, we won’t use it only when talking about experiments. It will be used also for correlational relationships, formally its name in survey designs is predictor."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#dependent-variable",
    "href": "lecture1/ResearchDesigns.html#dependent-variable",
    "title": "Research Designs Review",
    "section": "Dependent variable",
    "text": "Dependent variable\n\nDependent variables: are those that depend on the independent variables; they are the outcomes or results of the influence of the independent variables. It is also called outcome in survey designs or correlation designs."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#moderating-variables",
    "href": "lecture1/ResearchDesigns.html#moderating-variables",
    "title": "Research Designs Review",
    "section": "Moderating variables",
    "text": "Moderating variables\nModerating variables are predictor variables that affect the direction and/or the strength of the relationship between independent and the dependent variable."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#mediating-variables",
    "href": "lecture1/ResearchDesigns.html#mediating-variables",
    "title": "Research Designs Review",
    "section": "Mediating variables",
    "text": "Mediating variables\n\nMediating variables stand between the independent and dependent variables, and they transmit the effect of an independent variable on a dependent variable."
  },
  {
    "objectID": "lecture1/ResearchDesigns.html#references",
    "href": "lecture1/ResearchDesigns.html#references",
    "title": "Research Designs Review",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nCreswell, J. W., & Creswell, J. D. (2017). Research design: Qualitative, quantitative, and mixed methods approaches. Sage publications.\n\n\nMackie, J. (1974). Causation: The cement of the universe. Oxford: Oxford University Press.\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin; Company."
  },
  {
    "objectID": "PSYC3000.html",
    "href": "PSYC3000.html",
    "title": "PSYC3000",
    "section": "",
    "text": "If you are in this page is because you are an student in my PSYC 3000 class. Sometimes Canvas doesn’t like my presentation in html format. So, I decided to create my own site where you can access my presentations.\n\n\n\nYou might not be an student in my class, but you knew about me and this class from other sources. That’s totally fine! You are also welcome!\nI share all the content in this site under a Creative Commons license, specifically the license CC BY-NC-ND 4.0. This means, you cannot use this material for commercial purposes, but you can use my material, and you MUST cite or mention the original source for non-commercial projects such as academic work.\n\n\nAt this point you might be thinking: “I decided to study psychology to avoid numbers!”\nBut numbers are not avoidable if you want to be scientist or scientist practitioner. It might be boring sometimes, but other times you might have fun answering questions related to NATURE. Yes, NATURE!\nThis is a psychology course, and my aim is to study statistics as a science that helps other sciences to study NATURE. Psychological processes are also part of NATURE, of course!",
    "crumbs": [
      "Home",
      "Content",
      "PSYC3000"
    ]
  },
  {
    "objectID": "lecture2/Sampling.html",
    "href": "lecture2/Sampling.html",
    "title": "Sampling designs",
    "section": "",
    "text": "Let’s imagine you want to evaluate the levels of burnout in all companies in USA. Is it feasible?\nIf not, what should we do? How do we get enough information to evaluate burnout nationally?"
  },
  {
    "objectID": "lecture2/Sampling.html#what-is-a-good-sample",
    "href": "lecture2/Sampling.html#what-is-a-good-sample",
    "title": "Sampling designs",
    "section": "What is a good sample?",
    "text": "What is a good sample?\nLohr (2021): - A sample is representative if it can be used to “reconstruct” what the population looks like—and if we can provide an accurate assessment of how good that reconstruction is.\n\nLet’s define some concepts:\n\nObservation unit: An object on which a measurement is taken, sometimes called an element. In psychology we sample individuals most of the time, but in evolutionary psychology you might compare humans and animal models.\nTarget Population: The complete collection of observations we want to study. For instance: all voters? all technology companies in US?\nSample: A subset of a population."
  },
  {
    "objectID": "lecture2/Sampling.html#more-important-concepts",
    "href": "lecture2/Sampling.html#more-important-concepts",
    "title": "Sampling designs",
    "section": "More important concepts",
    "text": "More important concepts\n\nSampling unit: A unit that can be selected for a sample.\nSampling frame A list, map, or other specification of sampling units in the population from which a sample may be selected. For a telephone survey, the sampling frame might be a list of telephone numbers of registered voters, or simply the collection of all possible telephone numbers."
  },
  {
    "objectID": "lecture2/Sampling.html#selection-bias",
    "href": "lecture2/Sampling.html#selection-bias",
    "title": "Sampling designs",
    "section": "Selection Bias",
    "text": "Selection Bias\n\nSelection bias occurs when the target population does not coincide with the sampled population or, more generally, when some population units are sampled at a different rate than intended by the investigator.\n\nEx: overrepresentation of high income households."
  },
  {
    "objectID": "lecture2/Sampling.html#this-is-how-samplig-bias-happens",
    "href": "lecture2/Sampling.html#this-is-how-samplig-bias-happens",
    "title": "Sampling designs",
    "section": "This is how samplig bias happens!",
    "text": "This is how samplig bias happens!\n\nConvenience Samples: Some persons who are conducting surveys use the first set of population units they encounter as the sample. The problem is that the population units that are easiest to locate or collect may differ from other units in the population on the measures being studied.\nPurposive or Judgment Samples: the investigators use their judgment to select the specific units to be included in the sample.\nSelf-Selected Samples: A self-selected sample consists entirely of volunteers who select themselves to be in the sample.\nNonresponse: —failing to obtain responses from some members of the chosen sample—distorts the results of many surveys\n\n\n\n\n\n\n\nQuestion?\n\n\nCan we generalize the information to the population in these cases?"
  },
  {
    "objectID": "lecture2/Sampling.html#are-samples-with-selection-bias-good",
    "href": "lecture2/Sampling.html#are-samples-with-selection-bias-good",
    "title": "Sampling designs",
    "section": "Are samples with selection bias good?",
    "text": "Are samples with selection bias good?\n\nMany of the studies in psychology have selection bias, even classical studies.\nBut they are still useful, however the authors cannot claim representative conclusions.\nWe also struggle with measurement error such as: is people telling the truth? do participants understand the questions? can they remember details? socially desirable answers?"
  },
  {
    "objectID": "lecture2/Sampling.html#types-of-probability-sample",
    "href": "lecture2/Sampling.html#types-of-probability-sample",
    "title": "Sampling designs",
    "section": "Types of probability sample",
    "text": "Types of probability sample\n\nSimple random sample: All units have the same chance to be the sample.\nStratified random sample: the population is divided in subgroups, and then we sample observations randomly from each group or stratum. Example, social economical status.\nCluster sample: observation units in the population are aggregated into larger sampling units, called clusters. Example: university departments, companies.\nSystematic sample: a starting point is chosen from a list of population members using a random number. That unit, and every kth unit thereafter, is chosen to be in the sample. A systematic sample thus consists of units that are equally spaced in the list.\n\nExample: A systematic sample could be chosen by selecting an integer at random between 1 and 20; if the random integer is 16, say, then you would include professors in positions 16, 36, 56, and so on, in the list."
  },
  {
    "objectID": "lecture2/Sampling.html#remember",
    "href": "lecture2/Sampling.html#remember",
    "title": "Sampling designs",
    "section": "Remember !",
    "text": "Remember !\n\nThe sampling strategies mentioned at this point are a good option when you are conducting a correlational study in a survey design.\nThe sampling strategy in experimental and quasi-experimental designs is based on the theory and experimental design.\nA pure experiment aims to determine the causal relationship between X and Y. This means, we’ll have to generate an artificial condition that will affect the external generalizability of the experiment."
  },
  {
    "objectID": "lecture3/introProbAndStats.html",
    "href": "lecture3/introProbAndStats.html",
    "title": "Introduction to probability and statistics",
    "section": "",
    "text": "It might seem trivial to talk about nature and how science related to nature. However, we will study nature when we study statistics.\nAs Westfall & Henning (2013) mentioned in their book: “Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation—that is, it encompasses those things that we can agree we are observing” (p.1)\nAs psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study constructs, I will talk about constructs frequently.\nStatistics is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or Nature."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#reality-nature-science-and-models-ii",
    "href": "lecture3/introProbAndStats.html#reality-nature-science-and-models-ii",
    "title": "Introduction to probability and statistics",
    "section": "Reality, Nature, Science, and Models II",
    "text": "Reality, Nature, Science, and Models II\n\n\n\nTo study and understand Nature we must construct a model for how Nature works.\nA model helps you to understand Nature and also allows you to make predictions about Nature. There is no right or wrong model; they are all wrong! But some are better than others."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-models",
    "href": "lecture3/introProbAndStats.html#statistical-models",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Models",
    "text": "Statistical Models\n\n\n\nWe will focus on mathematical models integrated by equations and theorems.\nModels cannot reflect exactly how Nature works, but it is close enough to allow us to make predictions and inferences.\nLet’s create a model for driving distance.Imagine you drive at 100 km/hour, then we can predict your driving time \\(y\\) in hours by creating the following model: \\(y = x/100\\)\nYou could plug in any value to replace \\(x\\) and you’ll get a prediction:\nExample:\n\n\\[\\begin{align}\ny &= 300 km/100 \\\\\ny & = 3 hours \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lecture3/introProbAndStats.html#model-produces-data",
    "href": "lecture3/introProbAndStats.html#model-produces-data",
    "title": "Introduction to probability and statistics",
    "section": "Model produces data",
    "text": "Model produces data\n\nPay attention to our model. The model just created produces data, if we replace \\(x\\) by other values, the model will give us new information:\n\n\nR codePlotData\n\n\n\nkm <- c(200,300,400,500,600,900,1000)\n## Our model\ntime <- km/100\n### Let's plot the information\nlibrary(ggplot2)\n\np <- ggplot(data=data.frame(km,time), aes(x=km ,y=time)) +\n    geom_line() +\n    geom_point()+ \n    theme_classic()\n\n\n\n\np\n\n\n\n\n\n\n\ndata1 <- data.frame(km,time)\ndata1\n\n    km time\n1  200    2\n2  300    3\n3  400    4\n4  500    5\n5  600    6\n6  900    9\n7 1000   10"
  },
  {
    "objectID": "lecture3/introProbAndStats.html#model-produces-data-ii",
    "href": "lecture3/introProbAndStats.html#model-produces-data-ii",
    "title": "Introduction to probability and statistics",
    "section": "Model produces data II",
    "text": "Model produces data II\n\nA Model is like a recipe, it has some steps and rules that will help you to prepare a cake or a meal. The cake is your data.\n\n\n\n\n\n\n\nAlways remember!\n\n\nModels produce data, data does not produce models!\n\n\n\n\nYou can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data.\nYou can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-processes",
    "href": "lecture3/introProbAndStats.html#statistical-processes",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Processes",
    "text": "Statistical Processes\n\n\n\n\n\n\nAlways remember!\n\n\nThe order matters, Nature is there before our measurements,the data comes after we establish our way to measure Nature."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-processes-ii",
    "href": "lecture3/introProbAndStats.html#statistical-processes-ii",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Processes II",
    "text": "Statistical Processes II\nWestfall & Henning (2013) are doing an important distinction:\n\nThey present DATA with upper case to differentiate data with lower case. Why?\nWhen we talk about Nature we are talking about infinite numbers of observations, from these infinite number of possibilities we extract a portion of DATA, this is similar to the concept of population. For instance, your DATA could be all the college students in California.\ndata (lowercase) means that we already collected a sample from that DATA. For example, if you get information from only college students from valley, you’ll have one single instance of what is possible in your population.\nYour data is the only way we have to say something about the DATA.\nPrior collecting data, the DATA is random, unknown."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-processes-iii",
    "href": "lecture3/introProbAndStats.html#statistical-processes-iii",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Processes III",
    "text": "Statistical Processes III\nMore about models\n\nWe will use the term “probability model” , we will represent this term using: \\(p(y)\\) which translates into “probability of \\(y\\)”.\nLet’s use the flipping coin example. We know that the probability of flipping a coin and getting heads is 50%, same probability can be observed for tails (50%). Then, we can represent this probability by \\(p(heads) = 0.5\\) or \\(p(tail) = 0.5\\).\nThis is actually a good model! Every time, it produces as many random coin flips as you like. Models produce data means: that YOUR model is a model the explains how your DATA will be produced!\nYour model for Nature is that your DATA come from a probability model \\(p(y).\\)"
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-processes-iv",
    "href": "lecture3/introProbAndStats.html#statistical-processes-iv",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Processes IV",
    "text": "Statistical Processes IV\n\nOur model \\(p(y)\\) can be used to predict and explain Nature. A prediction is a guess about unknown events in the past, present or future or about events that might not happen at all.\nIt is more like asking “What-if” .For example, what if I had extra $3000 to pay my credit balance?"
  },
  {
    "objectID": "lecture3/introProbAndStats.html#deterministic-vs.-probabilistic",
    "href": "lecture3/introProbAndStats.html#deterministic-vs.-probabilistic",
    "title": "Introduction to probability and statistics",
    "section": "Deterministic vs. Probabilistic",
    "text": "Deterministic vs. Probabilistic\n\nIn science, you’ll find models that are deterministic, that means; our outcome is completely determined by our \\(x\\)\n\nLet’s see this example: Free Fall Calculator\n\nIn physics you’ll find several examples of deterministic models, especially from the Newtonian perspective.\nNormally these models are represented with \\(f(.)\\) instead of \\(p(.)\\), for example the driving distance example can be written as \\(f(x) = x/100\\).\nThis symbol \\(f(x)\\) means mathematical function. The function will give us only one solution in the case of a deterministic mode. We plug in values and we get a solution. (\\(y = f(x)\\))."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#deterministic-vs.-probabilistic-ii",
    "href": "lecture3/introProbAndStats.html#deterministic-vs.-probabilistic-ii",
    "title": "Introduction to probability and statistics",
    "section": "Deterministic vs. Probabilistic II",
    "text": "Deterministic vs. Probabilistic II\n\nProbabilistic models models assume variability , they are not deterministic, probabilistic models produce data that varies, therefore we’ll have distributions.\nProbabilistic models are more realistic to explain phenomena in psychology.\nThe following expresion represents a probabilistic model:\n\n\\[\\begin{equation}\nY \\sim p(y)\n\\end{equation}\\]\n\nThe symbol \\(\\sim\\) can be read aloud either as “produced by” or “distributed as.” In a complete sentence, the mathematical shorthand \\(Y \\sim p(y)\\) states that your \\(DATA\\) \\(Y\\) are produced by a probability model having mathematical form \\(p(y)\\)."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#variability",
    "href": "lecture3/introProbAndStats.html#variability",
    "title": "Introduction to probability and statistics",
    "section": "Variability",
    "text": "Variability\n\nWould a deterministic model explain how people feel after a traumatic event?\nCan we plug in values in a deterministic function to predict your attention span while driving?\nYou must use a probabilistic (stochastic) models to study natural variability."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#parameters",
    "href": "lecture3/introProbAndStats.html#parameters",
    "title": "Introduction to probability and statistics",
    "section": "Parameters",
    "text": "Parameters\n\nA parameter is a numerical characteristic of the data-generating process, one that is usually unknown but often can be estimated using data.\n\n\\[\\begin{equation}\nY \\sim \\beta_{0} + \\beta_{1}X\n\\end{equation}\\]\nFor instance, in the model showed above, we have two unknown parameters, this model produces data \\(Y\\). The unknown parameters are represented with greek letters, for instance the letter beta in the example above.\n\n\n\n\n\n\nMantra\n\n\nModel produces data.\nModel has unknown parameters.\nData reduce the uncertainty about the unknown parameters."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models",
    "href": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models",
    "title": "Introduction to probability and statistics",
    "section": "Purely Probabilistic Statistical Models",
    "text": "Purely Probabilistic Statistical Models\n\nIn a probabilistic model the variable \\(Y\\) is produced at random. This statement is represented by \\(Y \\sim p(y)\\).\n\\(p(y)\\) is called a probability density function (pdf).\nA pdf assigns a likelihood to your values. I will explain this in the next sessions.\nIMPORTANT: A purely probabilistic statistical model states that a variable \\(Y\\) is produced by a pdf having unknown parameters. In symbolic shorthand, the model is given as \\(Y \\sim p(y|\\theta )\\). This greek letter \\(\\theta\\) is called “theta”."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models-ii",
    "href": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models-ii",
    "title": "Introduction to probability and statistics",
    "section": "Purely Probabilistic Statistical Models II",
    "text": "Purely Probabilistic Statistical Models II\n\nLet’s think that Nature is also observable in our class. Our class belongs to the data generating process of grades in stats classes in the world. Let’s also assume that the data generating process of grades is normally distributed with a mean of 78 (out of 100) and standard deviation of 1. We can also imagine that we have a single sample of 100 students that have taken stats classes. Let’s check the graph…\n\n\nR codeFigure\n\n\n\nset.seed(1234)\n\ndataProcess <- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades <- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)"
  },
  {
    "objectID": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models-iii",
    "href": "lecture3/introProbAndStats.html#purely-probabilistic-statistical-models-iii",
    "title": "Introduction to probability and statistics",
    "section": "Purely Probabilistic Statistical Models III",
    "text": "Purely Probabilistic Statistical Models III\n\nI just used the word “assume” , probabilistic models have assumptions. In the previous example we assumed:\n\nThe data generating process is normally distributed.\nWe assumed a value for the mean.\nWe assumed a value for the standard deviation."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-inference",
    "href": "lecture3/introProbAndStats.html#statistical-inference",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nNormally if we flip a coin we have around 0.50 probability of getting tails, and also around 0.50 of getting heads. But, what would happen if we bend the coin? Are you sure you’ll get 0.50 probability? Can we assume the same probability?\n\n\nProbability distribution for a Bent Coin\n\n\nOutcome, \\(y\\)\n\\(p(y)\\)\n\n\n\n\nTails\n1 - \\(\\pi\\)\n\n\nHeads\n\\(\\pi\\)\n\n\nTotal\n1.00\n\n\n\n\nNow we have uncertainty, we can only assume that there is a probability represented by \\(\\pi\\). That’s the only think we know."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#statistical-inference-ii",
    "href": "lecture3/introProbAndStats.html#statistical-inference-ii",
    "title": "Introduction to probability and statistics",
    "section": "Statistical Inference II",
    "text": "Statistical Inference II\n\nWe need to collect data to reduce the uncertainty about the unknown parameters.\nThe reduction in uncertainty about model parameters that you achieve when you collect data is called statistical inference"
  },
  {
    "objectID": "lecture4/Distributions.html",
    "href": "lecture4/Distributions.html",
    "title": "Probability distributions and random variables",
    "section": "",
    "text": "In our last session we talked about DATA (uppercase) and data (lowercase).\nWe studied that DATA means all the possible values in Nature for instance, all the college students in US versus a single fix observation called data (lowercase)\nIn this session is important to remember this convention.\nDATA means random variable whereas data means fixed quantities.\nFor instance, in a dice you have 6 possible values, when roll a dice ; let’s imagine you get 5 , then your data is y = 5 while you DATA is any possible value from 1 to 6.\nRandom variables have a distribution, this means DATA is random."
  },
  {
    "objectID": "lecture4/Distributions.html#types-of-data",
    "href": "lecture4/Distributions.html#types-of-data",
    "title": "Probability distributions and random variables",
    "section": "Types of DATA",
    "text": "Types of DATA\n\nNominal DATA: These are DATA whose possible values are essentially labels (or, as the word nominal suggests, names), with no numerical value. Ex: marital status, job title, political affiliation.\nContinuous DATA: These are numerical DATA whose possible values lie in a continuum, or in a continuous range. For instance the duration of this class in seconds.\nOrdinal DATA: These types of DATA are intermediate between nominal and continuous. Unlike nominal data, which can be numbers without intrinsic order such as 1 = male and 2 = female, ordinal DATA are numbers that reflect an intrinsic order, hence, the name ordinal. Example, education level, ranking of your favorite dessert."
  },
  {
    "objectID": "lecture4/Distributions.html#nominal-data",
    "href": "lecture4/Distributions.html#nominal-data",
    "title": "Probability distributions and random variables",
    "section": "Nominal data",
    "text": "Nominal data\n\nlibrary(ggplot2)  ### <- this is a package in R to create pretty plots.\n## setwd() is a function to set my working directory (folder where the data is located)\nsetwd(\"C:/Users/emontenegro1/Documents/MEGA/stanStateDocuments/PSYC3000/lecture4\")\nrumination <- read.csv(\"ruminationComplete.csv\") \nrumination$sex <- factor(rumination$sex, labels = c(\"female\", \"male\"))\nggplot(data = rumination, aes(x = sex)) + \n  geom_bar()+ theme_bw()"
  },
  {
    "objectID": "lecture4/Distributions.html#continuous-data",
    "href": "lecture4/Distributions.html#continuous-data",
    "title": "Probability distributions and random variables",
    "section": "Continuous data",
    "text": "Continuous data\n\nggplot(data = rumination, aes(x = ageMonths)) + \n  geom_histogram() + theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lecture4/Distributions.html#ordinal-data",
    "href": "lecture4/Distributions.html#ordinal-data",
    "title": "Probability distributions and random variables",
    "section": "Ordinal data",
    "text": "Ordinal data\n\nrumination$grade <- factor(rumination$grade, \n                           labels = c(\"seventh\", \n                                      \"eight\", \n                                      \"nineth\", \n                                      \"tenth\", \n                                      \"eleventh\"))\nggplot(data = rumination, aes(x = grade)) + \n  geom_bar()"
  },
  {
    "objectID": "lecture4/Distributions.html#discrete-vs.-continuous",
    "href": "lecture4/Distributions.html#discrete-vs.-continuous",
    "title": "Probability distributions and random variables",
    "section": "Discrete vs. Continuous",
    "text": "Discrete vs. Continuous\n\nNominal and Ordinal DATA are considered discrete DATA, this means that values can be listed.\nContinuous DATA cannot be listed because all possible values lie in a continuum."
  },
  {
    "objectID": "lecture4/Distributions.html#discrete-probability-distribution-functions",
    "href": "lecture4/Distributions.html#discrete-probability-distribution-functions",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions",
    "text": "Discrete Probability Distribution Functions\n\nIn our previous class I mentioned the concept of probability density function (pdf), in simple words, the pdf is the model for a random variable.\nFor example, you can assume that the pdf of the normal distribution produces you DATA. DATA such as grades in all the statistics classes in US can be assumed to be generated by a normal distributed model.\nWe will revisit more about the normal distribution in this class."
  },
  {
    "objectID": "lecture4/Distributions.html#discrete-probability-distribution-functions-1",
    "href": "lecture4/Distributions.html#discrete-probability-distribution-functions-1",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions",
    "text": "Discrete Probability Distribution Functions\n\nBut let’s focus on discrete distributions first.\nAs I said before, a discrete variable can be listed as showed below:\n\n\n\nIn this table \\(p(y)\\) represents the probability of any variable \\(y\\).\nWhen we talk about discrete distributions we talk about probability mass function , when the variable is continuous we used the concept probability density function."
  },
  {
    "objectID": "lecture4/Distributions.html#discrete-probability-distribution-functions-ii",
    "href": "lecture4/Distributions.html#discrete-probability-distribution-functions-ii",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions II",
    "text": "Discrete Probability Distribution Functions II\n\nA Discrete pdf or probability mass function has several requirements:\n\nThe probabilities are NEVER negative, but they could be zero.\nThe total probability is 1.0 or 100%"
  },
  {
    "objectID": "lecture4/Distributions.html#discrete-probability-distribution-functions-iii",
    "href": "lecture4/Distributions.html#discrete-probability-distribution-functions-iii",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions III",
    "text": "Discrete Probability Distribution Functions III\n\nProbability distributions are the most important component in statistics.\nWe always talk about the normal distribution model, however there several different distributions in the universe. But we are going to study only a few of them.\nDo you remember the coin example? The right model for the coin problem is the pdf discovered by Bernoulli. In fact, many people call it the Bernoulli distribution. Its formal representation is as follows:\n\n\\[\\begin{equation}\n\np(y|\\pi) = \\pi^{y}(1-\\pi)^{1-y}\n\n\\end{equation}\\]\n\nNo worries about the interpretation of the formula, we might have time to study this distribution at the end of this course. Let’s focus on how we can use this model."
  },
  {
    "objectID": "lecture4/Distributions.html#simulation-time",
    "href": "lecture4/Distributions.html#simulation-time",
    "title": "Probability distributions and random variables",
    "section": "Simulation time!",
    "text": "Simulation time!\n\nRemember the mantra: Models produce data, this means we can produce data using the Bernoulli model:\nLet’s think about the tossing the coin, we can assign numerical values to the the results, for instance each time we get a head we will code it as 1.00, when we get tail we’ll code it as 0.00.\n\n\nset.seed(1236)\ntossCoin <- rbinom(100,1,0.50)\nsum(tossCoin)/length(tossCoin)\n\n[1] 0.48\n\n\n\nIn the R code above, I’m simulating tossing a coin randomly 100 times using the discrete Bernoulli distribution, after doing that I get exactly zeros and ones, as we expected. Now, following the coin toss model, we should have a probability \\(p(heads)= 0.50\\) in the long run.\nBut wait? Why we don’t get 0.50? According to our simulated data we have a probability of 0.48.\n\nLet’s try again but this time lets generate 1006 values:\n\nset.seed(1236)\ntossCoin <- rbinom(1006,1,0.50)\nsum(tossCoin)/length(tossCoin)\n\n[1] 0.5\n\n\n\nThat’s magic! Now we have 0.50 probability of getting heads, what happened?\nWhen we have more observed data we are closer to the DATA generating process value. The value for our unknown parameter is 0.50 according to our model, after adding more data, we are close to that number. This is also called the Law of Large Numbers."
  },
  {
    "objectID": "lecture4/Distributions.html#simulation-time-ii",
    "href": "lecture4/Distributions.html#simulation-time-ii",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! II",
    "text": "Simulation time! II\n\nHopefully, at this point the notion of DATA versus data starts to make sense.\nI want to introduce another model that we might study further if time allows, this new model is the Poisson pdf, a.k.a Poisson distribution.\nThe function form of the Poisson distribution is:\n\n\\[\\begin{equation}\np(y|\\lambda) = \\frac{\\lambda^{y}e^{-\\lambda}}{y!}\n\\end{equation}\\]\n\nLet’s untangle the formula:\n\nThe symbol \\(\\lambda\\) is the Greek letter “lambda”, it represents the theoretical average.\nThe letter \\(e\\) is called Euler’s constant, its numerical value is approximately \\(e = 2.71828...\\). You might remember this symbol when you did exponential functions in high school or college. -The term \\(y!\\) is “y factorial” , you might remember factorials from high school or college, they are used a lot in probability. It can be defined as:\n\n\\[\\begin{equation}\n\ny! =  \\textrm{1 x 2 x 3 x 4 x ...x y}\n\\end{equation}\\]"
  },
  {
    "objectID": "lecture4/Distributions.html#simulation-time-iii",
    "href": "lecture4/Distributions.html#simulation-time-iii",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! III",
    "text": "Simulation time! III\nAs always, it is not required to remember the Poisson formulation, but it is more important to understand how we can use this model to explain our observed data:\n\nLet’s imagine that you want to find a model to describe attendance in high school. We could try to generate data that assumes that a high school student has on average 8 absences.\n\n\nset.seed(1236)\nabsences <- 8\nN <- 300\ndataGenerated <- rpois(N, absences)\ncat(\"The mean is \", mean(dataGenerated))\n\nThe mean is  7.98\n\n\n\nOur model generated 300 observations, with an average number of absences = 7.98. It is likely that this number will be closer to 8 if you simulate more observations."
  },
  {
    "objectID": "lecture4/Distributions.html#simulation-time-iv",
    "href": "lecture4/Distributions.html#simulation-time-iv",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! IV",
    "text": "Simulation time! IV\nThe probability of each value in our tiny Poisson simulation can be represented in list form as:\n\n\nThe Poisson pdf is a good model to represent counts such as the number of times you successfully wake up early and do exercise, the number of shoes, the number of family members in each household and many other counting variables."
  },
  {
    "objectID": "lecture4/Distributions.html#continuous-probability-distribution-functions",
    "href": "lecture4/Distributions.html#continuous-probability-distribution-functions",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions",
    "text": "Continuous Probability Distribution Functions\n\n\n\n\n\nContinuous distributions have several differences compare to discrete distributions.\n\nWhen the density plot corresponds to a continuous distribution, the \\(y\\)-axis does not show probability.\nWhen you estimate the density of continuous distribution you are estimating the “relative likelihood” For example, in this plot 50 kg has a likelihood of around 0.014, 55 kg has a likelihood of around 0.021. This means that is more likely to observe 55 kg compare to 50 kg."
  },
  {
    "objectID": "lecture4/Distributions.html#continuous-probability-distribution-functions-ii",
    "href": "lecture4/Distributions.html#continuous-probability-distribution-functions-ii",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions II",
    "text": "Continuous Probability Distribution Functions II\n\n\n\n\nImportant: - The probability of a value in a density plot can only be estimated by calculating by estimating the area under the curve.\n\nThe total area under the curve in discrete and continuous functions is always equal to 1.0."
  },
  {
    "objectID": "lecture4/Distributions.html#continuous-probability-distribution-functions-iii",
    "href": "lecture4/Distributions.html#continuous-probability-distribution-functions-iii",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions III",
    "text": "Continuous Probability Distribution Functions III\n\n\n\n\n\nIn this graph Westfall & Henning (2013) changed the metric from kg to metric tons.\nThis helps to show you how to estimate the probability of 0.08 metric tons from a density plot.\nThen the probability of observing a weight (in metric tons) in the range \\(0.08 \\pm 0.005\\) is approximately 17 × 0.01 = 0.17. Or in other words, about 17 out of 100 people will weigh between 0.075 and 0.085 metric tons, equivalently, between 75 and 85 kg.\nBut this is is just an approximation, integrals would give us the exact probability, but we will avoid calculus today."
  },
  {
    "objectID": "lecture4/Distributions.html#continuous-probability-distribution-functions-iv",
    "href": "lecture4/Distributions.html#continuous-probability-distribution-functions-iv",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions IV",
    "text": "Continuous Probability Distribution Functions IV\n\n\nRequirements for a Continuous pdf\n\n\nThe values on the \\(y\\)-axis are always positive, it means the likelihood is always positive.\nGiven that the area under curve represents PROBABILITY, the total area is always equals to 1.0."
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution",
    "text": "More distributions: normal distribution\n\n\n\\[\\begin{align*}\np(y|\\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma}} exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}\\\\\n\\end{align*}\\]\nAt this point it looks a little bit esoteric and dark, I will untangle the components of the normal distribution formulation.\n\nThe component \\(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\) is what we call a constant, it is a fixed quantity. Every probability density function will have a constant. This constant number helps to make sure that the area under the curve of a distribution is equal to 1 (believe me this is true!).\n\n\n\nThe second component \\(exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}\\) is what is called kernel it is like the “core” of the area under the curve.\nThe third important component is \\(p(y|\\mu, \\sigma^2)\\), in this expression; \\(y\\) represents any numeric value, \\(\\mu\\) represents the mean of the distribution and \\(\\sigma\\) is the variance of the distribution. Now we can read this third component as “probability of any number given a mean, and a variance”"
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution-ii",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution-ii",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution II",
    "text": "More distributions: normal distribution II\n\nAs always, let me simulate some values using R, imagine we need to find a model that best describes the weight distribution in College students. For this task, we could assume the normal distribution model with a mean of 65 kg and standard deviation equals to 1.0.\n\n\n### Random starting number\nset.seed(359)\n### Mean or average in Kg \nMean <- 65\n## Standard Deviation\nSD <- 1\n## Number of observations\nN <-  300\n### Generated values from the normal distribution\ndata_1 <-  rnorm(n = N, mean = Mean, sd = SD )\ndata_1\n\n  [1] 65.90960 64.71476 63.83066 63.44413 65.46502 65.31280 66.48675 65.87327\n  [9] 64.84400 64.67924 62.12089 63.97994 64.42622 65.01382 64.85000 65.35106\n [17] 64.86783 64.98873 64.54516 64.14890 63.47823 64.45281 66.13445 64.45036\n [25] 65.66304 65.57285 62.93050 63.46320 63.99792 64.49344 65.89572 64.24975\n [33] 65.90253 65.29621 65.85397 64.34964 65.16825 64.80732 65.56075 66.39590\n [41] 65.68950 65.33977 64.06039 66.64493 64.47345 65.04675 65.85947 67.92296\n [49] 66.34708 65.26185 64.95761 65.18777 64.03615 65.27033 65.84343 64.16392\n [57] 65.61956 64.48135 67.40605 67.23284 65.12713 64.07411 63.98220 65.70498\n [65] 64.88308 65.93083 64.72232 66.42429 65.23638 64.81533 66.98712 63.34341\n [73] 64.93697 63.42742 65.41067 64.40579 64.70454 64.31235 65.16420 63.88185\n [81] 64.43832 64.63143 64.33169 66.17669 65.05522 64.00396 65.43599 66.05379\n [89] 65.39605 63.63870 66.18342 66.61002 65.84883 64.91560 65.79335 65.32865\n [97] 65.93242 64.77581 64.21611 65.83586 67.52724 64.90828 64.33488 64.42051\n[105] 65.95834 66.25977 64.91274 65.06262 64.47752 64.62498 65.13740 64.24417\n[113] 65.36593 67.36820 65.70180 64.33536 65.96453 64.97927 68.24824 65.39837\n[121] 64.00358 65.19193 64.43649 64.03446 63.96788 64.99354 64.32949 66.39454\n[129] 65.46359 66.03655 64.66212 64.30517 64.21899 64.55146 63.81233 65.18873\n[137] 65.30952 64.32071 65.17876 64.50110 64.66146 65.01361 64.74170 65.08205\n[145] 64.42346 64.32447 65.23062 64.46689 66.69426 64.94310 65.24772 65.00605\n[153] 63.93441 67.81550 64.76292 66.26200 64.15244 64.70025 65.06429 65.57770\n[161] 63.39030 65.24833 64.62538 65.88314 64.93269 65.01867 64.83311 64.16538\n[169] 65.08892 65.60057 65.15178 64.51507 64.65132 63.98354 64.43078 63.79769\n[177] 64.79643 65.25849 63.31048 65.06860 64.70367 64.48503 64.58913 65.50374\n[185] 62.96262 64.38087 66.59937 66.05083 63.79779 66.04224 64.68900 65.01352\n[193] 64.49640 63.70097 63.88770 67.28460 66.72947 63.65179 65.44370 65.67320\n[201] 65.26623 64.82400 64.67668 64.78615 64.89735 65.16560 64.86284 63.69684\n[209] 64.36736 65.50050 64.68430 67.10004 65.13415 65.07864 65.75652 66.96074\n[217] 64.67984 65.33965 64.32788 64.99155 63.72848 64.85579 64.81778 64.67416\n[225] 66.54865 65.22833 65.81325 64.69429 66.10983 64.80612 63.39200 65.88671\n[233] 64.01276 64.33288 63.46912 66.61400 64.11193 63.11924 64.71432 63.55637\n[241] 65.91217 64.50794 64.35599 67.67487 66.97199 65.09739 66.49791 65.46359\n[249] 65.92001 65.91692 63.40351 64.27549 66.08439 64.05049 65.84046 63.69535\n[257] 64.29846 65.15456 63.72143 66.81415 65.73656 63.93622 63.74965 65.40493\n[265] 64.74493 64.89682 64.95052 62.67580 65.33750 65.48994 64.24487 65.38974\n[273] 66.34487 63.93986 66.39666 64.63785 65.39375 65.40279 65.28578 65.01735\n[281] 66.74379 66.87585 64.91307 66.32705 64.25904 64.07473 65.96503 65.33417\n[289] 63.96063 65.42224 62.30688 64.32297 66.52509 63.99097 64.86705 65.34434\n[297] 64.17483 65.01194 64.99918 65.14846"
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution-iii",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution-iii",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution III",
    "text": "More distributions: normal distribution III\n\nWe can check the density distribution of this sample that comes from the normal distribution model:\n\n\nplot(density(data_1), \n     xlab = \"Weight (kg)\", \n     ylab = \"p(y) or likelihood\", \nmain = \"Normal density function of college student's weight\" \n)"
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution-iv",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution-iv",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution IV",
    "text": "More distributions: normal distribution IV\n\nWe can also imagine that we need a model to describe the weight of hospital patients. But, this time probably, we will see more overweight individuals because of diverse health problems. Then, a mean = 65 kg is not realistic (143.3 lbs), probably we should assume a mean = 90 kg (198.42 lbs), and probably the observed data will be spread out, so we can use a larger values for the standard deviation, perhaps something around 10.\n\n\n### Random starting number\nset.seed(359)\n### Mean or average in Kg \nMean <- 90\n## Standard Deviation\nSD <- 10\n## Number of observations\nN <-  300\n### Generated values from the normal distribution\ndata_2 <-  rnorm(n = N, mean = Mean, sd = SD )\ndata_2\n\n  [1]  99.09603  87.14758  78.30664  74.44127  94.65025  93.12797 104.86753\n  [8]  98.73265  88.43998  86.79235  61.20891  79.79938  84.26221  90.13819\n [15]  88.49998  93.51056  88.67827  89.88726  85.45157  81.48895  74.78235\n [22]  84.52814 101.34451  84.50357  96.63045  95.72855  69.30499  74.63198\n [29]  79.97919  84.93441  98.95721  82.49745  99.02532  92.96213  98.53972\n [36]  83.49641  91.68247  88.07319  95.60747 103.95905  96.89498  93.39768\n [43]  80.60386 106.44925  84.73451  90.46749  98.59471 119.22961 103.47075\n [50]  92.61846  89.57613  91.87772  80.36150  92.70328  98.43425  81.63920\n [57]  96.19561  84.81348 114.06050 112.32838  91.27126  80.74112  79.82199\n [64]  97.04981  88.83082  99.30830  87.22319 104.24293  92.36383  88.15335\n [71] 109.87124  73.43411  89.36973  74.27417  94.10665  84.05789  87.04545\n [78]  83.12347  91.64199  78.81849  84.38325  86.31432  83.31688 101.76688\n [85]  90.55224  80.03963  94.35986 100.53789  93.96049  76.38699 101.83422\n [92] 106.10019  98.48829  89.15598  97.93345  93.28650  99.32421  87.75811\n [99]  82.16114  98.35859 115.27235  89.08279  83.34876  84.20506  99.58339\n[106] 102.59770  89.12740  90.62618  84.77518  86.24982  91.37400  82.44167\n[113]  93.65933 113.68198  97.01798  83.35357  99.64534  89.79270 122.48244\n[120]  93.98373  80.03576  91.91926  84.36490  80.34458  79.67885  89.93544\n[127]  83.29487 103.94544  94.63586 100.36548  86.62125  83.05169  82.18990\n[134]  85.51461  78.12330  91.88730  93.09520  83.20710  91.78756  85.01100\n[141]  86.61460  90.13608  87.41698  90.82048  84.23460  83.24472  92.30620\n[148]  84.66887 106.94258  89.43101  92.47715  90.06048  79.34405 118.15496\n[155]  87.62915 102.61997  81.52439  87.00250  90.64290  95.77699  73.90302\n[162]  92.48329  86.25377  98.83139  89.32694  90.18671  88.33110  81.65385\n[169]  90.88921  96.00573  91.51776  85.15071  86.51324  79.83544  84.30776\n[176]  77.97693  87.96432  92.58489  73.10476  90.68604  87.03665  84.85029\n[183]  85.89134  95.03740  69.62624  83.80868 105.99374 100.50826  77.97789\n[190] 100.42236  86.88997  90.13518  84.96399  77.00973  78.87698 112.84603\n[197] 107.29474  76.51790  94.43699  96.73195  92.66227  88.23995  86.76681\n[204]  87.86150  88.97352  91.65598  88.62836  76.96842  83.67362  95.00498\n[211]  86.84302 111.00035  91.34154  90.78639  97.56522 109.60737  86.79841\n[218]  93.39651  83.27879  89.91554  77.28481  88.55791  88.17783  86.74163\n[225] 105.48651  92.28331  98.13252  86.94293 101.09826  88.06121  73.91998\n[232]  98.86712  80.12760  83.32878  74.69121 106.14002  81.11932  71.19242\n[239]  87.14321  75.56373  99.12171  85.07938  83.55992 116.74873 109.71989\n[246]  90.97392 104.97909  94.63592  99.20012  99.16920  74.03514  82.75492\n[253] 100.84388  80.50494  98.40457  76.95353  82.98457  91.54557  77.21432\n[260] 108.14149  97.36556  79.36224  77.49653  94.04927  87.44927  88.96823\n[267]  89.50524  66.75803  93.37497  94.89937  82.44871  93.89736 103.44874\n[274]  79.39864 103.96662  86.37846  93.93755  94.02794  92.85782  90.17349\n[281] 107.43790 108.75847  89.13072 103.27050  82.59045  80.74730  99.65032\n[288]  93.34172  79.60634  94.22238  63.06883  83.22968 105.25093  79.90966\n[295]  88.67048  93.44337  81.74835  90.11941  89.99179  91.48458"
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution-iv-1",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution-iv-1",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution IV",
    "text": "More distributions: normal distribution IV\n\nWe can check now the probability density function of our simulated hospital sample:\n\n\nplot(density(data_2), \n     xlab = \"Weight (kg)\", \n     ylab = \"p(y) or likelihood\", \nmain = \"Normal density function hospital patients' weight\" \n)"
  },
  {
    "objectID": "lecture4/Distributions.html#more-distributions-normal-distribution-v",
    "href": "lecture4/Distributions.html#more-distributions-normal-distribution-v",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution V",
    "text": "More distributions: normal distribution V\n\nR codePlot\n\n\n\nlibrary(ggplot2) ### <- this is a package in R to create pretty plots.\n\ndataMerged <- data.frame(\n  group =c(rep(\"College\", 300),\n           rep(\"Hospital\", 300)),\n  weight = c(data_1, data_2))\n\nggplot(dataMerged , aes(x=weight, fill=group)) +\n  geom_density(alpha=.25) + \n  theme_bw()+\n  labs(title = \"College and Hospital Weight Density Function\") + \n  xlab(\"Weight (kg)\") + \n  ylab(\"p(y) or likelihood\")"
  },
  {
    "objectID": "lecture4/Distributions.html#what-is-the-main-message-here",
    "href": "lecture4/Distributions.html#what-is-the-main-message-here",
    "title": "Probability distributions and random variables",
    "section": "What is the main message here?",
    "text": "What is the main message here?\n\nThe main message is: Models produce data!\nRemember we were talking about NATURE and DATA (uppercase)?\n\nOur statistical model (e.g. normal distribution) is our approximation to DATA, we create models that will generate observed data. That’s why in simulations sometimes our model is called “population model”, and from our data generating model we produce observed data.\n\nRemember when we talked about random variables vs. fixed quantities?\n\nWhen we refer to the DATA (population model) the variables are random, but when we generate data like we did in R that data are fixed quantities similar to collect data on campus related to dating strategies, once you collect your data, those numbers are fixed quantities that were generate by a natural process (data generating process). We cannot control the natural process like we do in simulation.\n\nWe reduce uncertainty when we add more observations. We will study more about it in the next classes."
  },
  {
    "objectID": "lecture5/centralTendency.html",
    "href": "lecture5/centralTendency.html",
    "title": "Central tendency and variability",
    "section": "",
    "text": "In this lecture we will study several basic concepts, but don’t fool yourselves by thinking that these topics are less important.\nThese concepts are the foundations to understand what is coming in this class.\nWe will learn about several measures to describe and understand continuous distributions.\nRemember that this are just a few measures, if time allows we will study more options to describe distributions.\nLet’s focus on the most common type of average you’ll see in psychology:\n\n\\[\\begin{equation}\n\\bar{X} = \\frac{\\sum X}{n}\n\\end{equation}\\]\nWhere:\n\nthe letter X with a line above it (also sometimes called “X bar”) is the mean value of the group of scores or the mean.\nthe \\(\\sum\\) or the Greek letter sigma, is the summation sign, which tells you to add together whatever follows it to obtain a total or sum.\nthe X is each observation\nthe \\(n\\) is the size of the sample from which you are computing the mean.7\n\nExample\n\nsetwd(\"C:/Users/emontenegro1/Documents/MEGA/stanStateDocuments/PSYC3000/lecture5\")\n\nrum <- read.csv(\"ruminationComplete.csv\")\n\nmean(rum$age)\n\n[1] 15.34906\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nWe will use \\(M=\\) to represent the word mean"
  },
  {
    "objectID": "lecture5/centralTendency.html#central-tendency-measures-iii",
    "href": "lecture5/centralTendency.html#central-tendency-measures-iii",
    "title": "Central tendency and variability",
    "section": "Central tendency measures III",
    "text": "Central tendency measures III\nLet’s do it by hand:\n\n\\[\\begin{align}\n\\bar{X} &= \\frac{\\sum X}{n}\\\\\n&= \\frac{18+21+24+23+22+24+25}{7}\\\\\n&= \\frac{157}{7}\\\\\n&= 22.43\\\\\n\\end{align}\\]"
  },
  {
    "objectID": "lecture5/centralTendency.html#central-tendency-measures-iii-1",
    "href": "lecture5/centralTendency.html#central-tendency-measures-iii-1",
    "title": "Central tendency and variability",
    "section": "Central tendency measures III",
    "text": "Central tendency measures III\n\nOr we could do it in R:\n\n\n(18+21+24+23+22+24+25)/7\n\n[1] 22.42857"
  },
  {
    "objectID": "lecture5/centralTendency.html#central-tendency-measures-iv",
    "href": "lecture5/centralTendency.html#central-tendency-measures-iv",
    "title": "Central tendency and variability",
    "section": "Central tendency measures IV",
    "text": "Central tendency measures IV\n\nMedian: The median is defined as the midpoint in a set of scores. It’s the point at which one half, or \\(50%\\), of the scores fall above and one half, or \\(50%\\), fall below.\nTo calculate the median we need to order the information. Let’s imagine you have the following values from different households:\n\n\n\n\n$135,456 | $25,500 | $32,456 | $54,365 | $37,668\n\n\n\n\nNow, we’ll need to sort the income from highest to lowest\n\n\n\n\n$135,456| $54,365| $37,668| $32,456| $25,500\n\n\n\n\nWhich value is in the middle?\nThe median is also known as the 50th percentile, because it’s the point below which 50% of the cases in the distribution fall. Other percentiles are useful as well, such as the 25th percentile, often called Q1, and the 75th percentile, referred to as Q3. The median would be Q2."
  },
  {
    "objectID": "lecture5/centralTendency.html#median-to-the-rescue",
    "href": "lecture5/centralTendency.html#median-to-the-rescue",
    "title": "Central tendency and variability",
    "section": "Median to the rescue",
    "text": "Median to the rescue\n\nAs you might remember, the mean is strongly affected by the extreme cases, whereas the median is more “robust” to extreme cases. This means the median is less affected by extreme values.\nLet’s use simulation to find out if it is true, imagine you have data related to a depression score:\n\n\nset.seed(1256)\n\nM <- 25\n\nSD <- 1\n\nn <- 50\n\n## Simulated depression score\ndepressionScore <- rnorm(n = n, mean = M, sd = SD)\n\nhist(depressionScore)\n\n\n\n\n\nLet’s check the mean of these generated values:\n\n\nmean(depressionScore)\n\n[1] 24.97403\n\n\n\nNow, let’s add a very extreme case, imagine one person has a diagnosis of bipolar disorder, and that person is experiencing a depressive episode when she or he filled out your test:\n\n\ndepressionScore.2  <- depressionScore \ndepressionScore.2[50] <- 100\nhist(depressionScore.2)\n\n\n\n\n\nLet’s compare the mean on both cases:\n\n\ncat(\"Mean before the extreme case:\", mean(depressionScore))\n\nMean before the extreme case: 24.97403\n\ncat(\"Mean after the extreme case:\", mean(depressionScore.2))\n\nMean after the extreme case: 26.48716\n\n\n\nWe can check now if the median was heavely affected by th extreme case:\n\n\ncat(\"Median before the extreme case:\", median(depressionScore))\n\nMedian before the extreme case: 24.82834\n\ncat(\"Median after the extreme case:\", median(depressionScore.2))\n\nMedian after the extreme case: 24.89818\n\n\n\nGreat the median is robust enough! It remain practically intact!"
  },
  {
    "objectID": "lecture5/centralTendency.html#the-fashionable-mode",
    "href": "lecture5/centralTendency.html#the-fashionable-mode",
    "title": "Central tendency and variability",
    "section": "The fashionable Mode",
    "text": "The fashionable Mode\n\nThe mode is the value that occurs most frequently. There is no formula for computing the mode.\n\nTo compute the mode, follow these steps:\n\nList all the values in a distribution but list each value only once.\nTally the number of times that each value occurs.\nThe value that occurs most often is the mode.\n\n\ntable(rum$grade)\n\n\n 7  8  9 10 11 \n30 25 24 72 61 \n\n\n\nThe most frequent grade is 10th, therefore the mode is 10th grade.\n\n\n\n\n\n\n\nNote\n\n\nThe function table() helps to calculate frequencies, this means; how many times a value appears in our data. It looks ugly, but it is helpful. The first row are the values observed in the data, and the second row are the frequencies."
  },
  {
    "objectID": "lecture5/centralTendency.html#the-fashionable-mode-ii",
    "href": "lecture5/centralTendency.html#the-fashionable-mode-ii",
    "title": "Central tendency and variability",
    "section": "The fashionable Mode II",
    "text": "The fashionable Mode II\n\nHowever, things can get messy when you have two modes!\n\n\n\nWhen you have a bimodal distibution you are dealing with a mixture of two or more distributions."
  },
  {
    "objectID": "lecture5/centralTendency.html#summary-function-as-a-good-option",
    "href": "lecture5/centralTendency.html#summary-function-as-a-good-option",
    "title": "Central tendency and variability",
    "section": "Summary() function as a good option",
    "text": "Summary() function as a good option\n\nIn R we can count on a handy function to describe a distribution, this function is summary().\n\n\nsummary(rum$age)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  12.00   14.00   16.00   15.35   16.00   18.00 \n\n\n\nThis function shows the minimum value, the 1st quantile, the median, the 3rd quantile, mean and maximum value.\n\n\nR codePlot\n\n\n\nlibrary(ggplot2) ### package to create pretty plots\n\ndens <- density(rum$age)\n\ndf <- data.frame(x=dens$x, y=dens$y)\n\nprobs <- c(0, 0.25, 0.5, 0.75, 1)\n\nquantiles <- quantile(rum$age, prob=probs)\n\ndf$quant <- factor(findInterval(df$x,quantiles))\n\nfigure <- ggplot(df, aes(x,y)) + \n    geom_line() + \n    geom_ribbon(aes(ymin=0, ymax=y, fill=quant)) + \n     scale_x_continuous(breaks=quantiles) +\n     scale_fill_brewer(guide=\"none\") + \n     geom_vline(xintercept=mean(rum$age), linetype = \"longdash\", color = \"red\") +\n     annotate(\"text\", x = 14, y = 0.2, label = \"Q1 = 14 years\") +\n     annotate(\"text\", x = 17, y = 0.3, label = \"Median = 16 years\") +\n     annotate(\"text\", x = 15.35, y = 0.33, label = \"Mean = 15.35 years\") +\n     ylab(\"Likelihood\") + \n     xlab(\"Age in years\")+\n     ggtitle(\"Quantiles and mean of Age\")+\n     theme_classic()\n\n\n\n\nfigure"
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability",
    "text": "Time to talk more about variability\n\nIn psychology we love variability, this is true also for science itself!\nWe care a lot about variability, the whole point of doing research is to explain or observe how variability happens. For instance, if you had data about life expectancy in the world you could detect which cases are far from the mean. Wait! We do have this type of data, check this webpage from the World Bank.\nAccording to the World Bank the global life expectancy at birth is 73 years old.\nWe could use the World Bank map and think, well we could which countries are far from the mean, For example: Costa Rica is 80.47, that means that Costa Rica is (\\(80.47-73 = 7.47\\)) 7.47 expected years above the mean. That’s good, these people have longer life that many people in the world."
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-ii",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-ii",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability II",
    "text": "Time to talk more about variability II\n\nLet’s check more information. In the next table:\n\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(DT)\n\nlife <- read.csv(\"lifeExpect.csv\", header = TRUE) %>%\n  select(Country.Name, Country.Code, X2020) %>%\n  filter(!is.na(X2020))\n##rmarkdown::paged_table(life)\n\ndatatable(life, filter = \"top\", style = \"auto\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-iii",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-iii",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability III",
    "text": "Time to talk more about variability III\n\nAccording the table, countries such as Central African Republic, Chad, Lesotho, Nigeria, and Sierra Leone have the lowest life expectancy. Let’s see how far they are from the global mean.\n\n\nlowLife <- life %>% \n  filter(Country.Code %in% c(\"CAF\", \"TCD\", \"LSO\", \"NGA\", \"SLE\")) %>%\n  mutate(difference = X2020 -73)\n\ndatatable(lowLife, filter = \"top\", style = \"auto\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-iv",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-iv",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability IV",
    "text": "Time to talk more about variability IV\n\nWhat we just did is called the absolute difference from the mean, and it is one of the variability measures we can use. Just by computing how far these countries are from the mean, we can draw worrisome conclusions. People are dying at a very young age in those places! And the difference compare to the World’s mean is up to 19.32 years less!\nFollowing the same logic we could estimate something call variance, time to check some math formulas:\n\n\\[\\begin{equation}\ns^2 = \\frac{\\sum (X_{i} - \\bar{X})^2}{n-1}\n\\end{equation}\\]\n\nIn this formula \\(X_{i}\\) is each value you have in your observed distribution, in our example it would be the life expectancy of each country. The symbol \\(\\bar{X}\\) represents the mean of your observed distribution or data (lowercase).\nCan you see what we are doing? We are calculating the absolute difference from the mean, and secondly we square the difference, next we sum the result and divide it finally by \\(n-1\\).\nBut wait! What is \\(n-1\\)? Given that we are working with a possible sample out of infinite samples, \\(n-1\\) helps to account that we are not working with the data generating process itself, it is just one instance generated by the data process.\nThe variance is hard to interpret but itself, but it is a concept that will help you to understand other models.\nThis concept is a measure of variability that depends on the metric of your observations.\nFor instance, you cannot compare the variability in kilometers with miles."
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-v",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-v",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability V",
    "text": "Time to talk more about variability V\n\nLet’s use the data set called mtcars already included inside R, you don’t have to import any data set into R:\n\n\ndatatable(mtcars, filter = \"top\", style = \"auto\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-vi",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-vi",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability VI",
    "text": "Time to talk more about variability VI\n\nWe will convert miles per gallon to kilometers per liter, we just need to multiply 1 mpg by 0.425 km/l.\n\n\ncarsData <- mtcars %>% select(mpg) %>%\n  mutate(kml = mpg*0.425)\n\ndatatable(carsData, filter = \"top\", style = \"auto\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#time-to-talk-more-about-variability-vii",
    "href": "lecture5/centralTendency.html#time-to-talk-more-about-variability-vii",
    "title": "Central tendency and variability",
    "section": "Time to talk more about variability VII",
    "text": "Time to talk more about variability VII\n\nWe can try to compare the variance of miles per gallon and kilometers per liter:\n\n\n## Variance mpg\ncat(\"Miles per gallon variance:\", var(carsData$mpg))\n\nMiles per gallon variance: 36.3241\n\n### Variance kml\ncat(\"kilometers per liter variance\", var(carsData$kml))\n\nkilometers per liter variance 6.561041\n\n\n\nIf we were very naive, we would conclude that miles per gallon has more variance compare to kilometers per liter, just because the estimation gives a larger number this conclusion would be wrong.\nThe variance looks larger because the measurement unit has larger numbers, compare to km/l.\nWhen you compare variances you need to compare apples to apples, both variables should follow the same units."
  },
  {
    "objectID": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse",
    "href": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse",
    "title": "Central tendency and variability",
    "section": "The Standard Deviation will be a better horse",
    "text": "The Standard Deviation will be a better horse\n\nThe standard deviation is an improved measure to describe continuous distribution.\nIt is the average distance from the mean. The larger the standard deviation, the larger the average distance each data point is from the mean of the distribution, and the more variable the set of values is.\n\n\\[\\begin{equation}\ns = \\sqrt{\\frac{\\sum (X_{i} - \\bar{X})^2}{n-1}}\n\\end{equation}\\]\n\nThe good thing about the standard deviation is that now we can compare different distributions and answer questions such as: which distribution has more variability? In simple words, we can conclude which distribution has values further away from the mean."
  },
  {
    "objectID": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-ii",
    "href": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-ii",
    "title": "Central tendency and variability",
    "section": "The Standard Deviation will be a better horse II",
    "text": "The Standard Deviation will be a better horse II\n\nAs always we can study an example:\n\nRemember when we were simulating data days ago? Well, we’ll do it again!\n\n\nHospital Example\n\nR codePlot\n\n\n\nlibrary(ggplot2) ### <- this is a package in R to create pretty plots.\n\nset.seed(359)\n\n### Non-hospital observations \n### Mean or average in Kg \nMean <- 65\n## Standard Deviation\nSD <- 1\n## Number of observations\nN <-  300\n### Generated values from the normal distribution\ndata_1 <-  rnorm(n = N, mean = Mean, sd = SD )\ndata_1\n\n### Hospital group\n### Mean or average in Kg \nMean <- 90\n## Standard Deviation\nSD <- 10\n## Number of observations\nN <-  300\n### Generated values from the normal distribution\ndata_2 <-  rnorm(n = N, mean = Mean, sd = SD )\ndata_2\n\n\n\ndataMerged <- data.frame(\n  group =c(rep(\"College\", 300),\n           rep(\"Hospital\", 300)),\n  weight = c(data_1, data_2))\n\nggplot(dataMerged , aes(x=weight, fill=group)) +\n  geom_density(alpha=.25) + \n  theme_bw()+\n  labs(title = \"College and Hospital Weight Density Function\") + \n  xlab(\"Weight (kg)\") + \n  ylab(\"p(y) or likelihood\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-iii",
    "href": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-iii",
    "title": "Central tendency and variability",
    "section": "The Standard Deviation will be a better horse III",
    "text": "The Standard Deviation will be a better horse III\n\nThanks to the standard deviation, we have a measurement unit to describe better the data.\nWe could also know at what point we consider a case to be extreme or select observations above or below any specific value based on the standard deviation.\nWe can start from the mean and add or subtract standard deviations from the mean. For example, the mean of age in our rumination data set is 15.35 years old, the standard deviation is 1.43 years old."
  },
  {
    "objectID": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-iv",
    "href": "lecture5/centralTendency.html#the-standard-deviation-will-be-a-better-horse-iv",
    "title": "Central tendency and variability",
    "section": "The Standard Deviation will be a better horse IV",
    "text": "The Standard Deviation will be a better horse IV"
  },
  {
    "objectID": "lecture5/centralTendency.html#lets-continue-learning-important-concepts",
    "href": "lecture5/centralTendency.html#lets-continue-learning-important-concepts",
    "title": "Central tendency and variability",
    "section": "Let’s continue learning important concepts",
    "text": "Let’s continue learning important concepts\n\nI already mentioned the concept of “quantiles”, this concept is in fact related to probabilities.\nWe will revisit the household data presented before, but this time we’ll order the income starting from the lowest value up to the highest value:\n\n\n\n\n$25,500| $32,456| $37,668| $54,365| $135,456\n\n\n\n\n\n\nNow, we can follow this formula to estimate our quantiles (Westfall & Henning, 2013):\n\n\\[\\begin{equation}\n\\hat{y}_{(i-0.5)/n} = y_{(i)}\n\\end{equation}\\]\n\nThe little hat \\(\\hat{}\\) on top of \\(y\\) means “estimate of”, this is used in statistics to comunicate that you are estimating a value form “data” (lowercase). This means you are estimating a value from your observed fixed data. The right-hand side is the \\(ith\\) ordered value of the data, all together we can read the formula as:\n\nThe \\((i − 0.5)/n\\) quantile of the distribution is estimated by the \\(ith\\) ordered value of the data\n\nWe can see an example:\n\n\nQuantile example\n\n\n\\(i\\)\n\\(y(i)\\)\n(\\(i\\)-0.5)/\\(n\\)\n\\[\\hat{y}_{(i-0.5)/n} = y(i)\\]\n\n\n\n\n1\n25500\n(1-0.5)/5 =0.10\n25500\n\n\n2\n32456\n(2-0.5)/5 =0.30\n32456\n\n\n3\n37668\n(3-0.5)/5 =0.50\n37668\n\n\n4\n54365\n(4-0.5)/5 =0.70\n54365\n\n\n5\n135456\n(5-0.5)/5 =0.90\n135456\n\n\n\n\nThen, we can say in plain English: “The 70th percentile of the distribution is measured by $54,365”.\nNow notice something, why we don’t have data representing the 75th percentile?\nGiven that these are estimates , these numbers are approximations to the true value, If you collect more data you’ll have data in different percentiles, also more precision to capture the real value."
  },
  {
    "objectID": "lecture5/centralTendency.html#another-example-related-to-quantiles",
    "href": "lecture5/centralTendency.html#another-example-related-to-quantiles",
    "title": "Central tendency and variability",
    "section": "Another example related to quantiles",
    "text": "Another example related to quantiles\n\n### mtcars data set is included in R\n\ndatatable(mtcars %>% select(mpg), filter = \"top\", style = \"auto\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#another-example-related-to-quantiles-ii",
    "href": "lecture5/centralTendency.html#another-example-related-to-quantiles-ii",
    "title": "Central tendency and variability",
    "section": "Another example related to quantiles II",
    "text": "Another example related to quantiles II\n\nWe can estimate the percentiles using the formula showed before, this time we will find the estimate for the quantiles in mpg variable inside the data mtcars:\n\n\nDataPlot\n\n\n\nlibrary(dplyr)\n\nestimatePercent <- mtcars %>% select(mpg) %>% \n  mutate(cars = row.names(mtcars)) %>%\n  arrange(mpg) %>%\n  mutate(order = 1:n()) %>%\n  mutate(prob = (order-0.50)/n()) %>%\n  mutate(percent = prob*100)\n\ndatatable(estimatePercent, filter = \"top\", style = \"auto\")\n\n\n\n\n\n\n\n\n\nggplot(data = estimatePercent, \n       aes(x = percent, \n           y = factor(cars, \n                      levels = row.names(estimatePercent)))) +\n  geom_point(size=3, color=\"orange\") +\n  geom_segment(aes(xend=percent, yend=0) ) +\n  xlab(\"Percentile\")+\n  ylab(\"Car Model\")+\n  ggtitle(\"Car percentiles\")+\n  theme_bw()"
  },
  {
    "objectID": "lecture5/centralTendency.html#another-example-related-to-quantiles-iii",
    "href": "lecture5/centralTendency.html#another-example-related-to-quantiles-iii",
    "title": "Central tendency and variability",
    "section": "Another example related to quantiles III",
    "text": "Another example related to quantiles III\n\nInstead of estimating the percentiles by hand we can use the function quantile() in R:\n\n\nquantile(mtcars$mpg, type = 5, probs = 0.265625)\n\n26.5625% \n    15.5 \n\n\n\nThis function will require a vector with numbers, and the probability you are interested.\nIf you run ?quantile you’ll see there are different ways to estimate the observed percentiles, all those are possible models to get an estimate.\n\n\n?quantile\n\nstarting httpd help server ... done"
  },
  {
    "objectID": "lecture5/centralTendency.html#cumulative-density-function-cdf",
    "href": "lecture5/centralTendency.html#cumulative-density-function-cdf",
    "title": "Central tendency and variability",
    "section": "Cumulative Density Function (CDF)",
    "text": "Cumulative Density Function (CDF)\n\nWe have been studied Probability Density Functions (PDF), now I’m going to introduce a concept that is related to PDF.\nI said that the area under the curve of the PDF is actually probability, even though the y-axis is showing likelihood instead of probability.\nI also said you can use calculus to get that probability in a easier way.\nThose calculus formulas will give you an easy way to estimate the probability under that curve. The final result is something we call “Cumulative Density Function CDF”."
  },
  {
    "objectID": "lecture5/centralTendency.html#cumulative-density-function-cdf-1",
    "href": "lecture5/centralTendency.html#cumulative-density-function-cdf-1",
    "title": "Central tendency and variability",
    "section": "Cumulative Density Function (CDF)",
    "text": "Cumulative Density Function (CDF)\n\nAs the name says, we are like “stacking” the whole density, therefore it changes the shape of the curve, but at the end is the same information in a different metric.\nIn fact, you get the derivative of a CDF, th calculation will give you the PDF back.\nBut no worries, I won’t ask you to do it… you are safe!"
  },
  {
    "objectID": "lecture5/centralTendency.html#cumulative-density-function-cdf-2",
    "href": "lecture5/centralTendency.html#cumulative-density-function-cdf-2",
    "title": "Central tendency and variability",
    "section": "Cumulative Density Function (CDF)",
    "text": "Cumulative Density Function (CDF)\n\nAll continuous distributions will have a CDF, and we are going to use very often the normal CDF.\nThe normal distribution is also called “Gaussian Distribution” , I prefer this name instead of “normal distribution”.\nAnyhow, let’s check some properties here.\n\nWe can also understand the importance of the Gaussian CDF using R:\n\nWhen we assume that the Gaussian distribution has a mean = 0 and standard deviation = 1, the CDF looks like this:\n\n\nCodePlot\n\n\n\n## sequence of x-values \njustSequence <- seq(-4, 4, .01)\n#calculate normal CDF probabilities \nprob <- pnorm(justSequence)\n#plot normal CDF \nplot(justSequence , \n     prob,\n     type=\"l\",\n     xlab = \"Generated Values\",\n     ylab = \"Probability\",\n     main = \"CDF of the Standard Gaussian Distribution\")\n\nabline(v=1.96, h = 0.975, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see that the probability of observing a value less or equal than 1.96 is 0.975."
  },
  {
    "objectID": "lecture5/centralTendency.html#cumulative-density-function-cdf-ii",
    "href": "lecture5/centralTendency.html#cumulative-density-function-cdf-ii",
    "title": "Central tendency and variability",
    "section": "Cumulative Density Function (CDF) II",
    "text": "Cumulative Density Function (CDF) II\n\nLet’s do something more intersting, remember the example of weight where we simulated the weight of two groups: hospital patients vs. college students?\nWe could now get the probability of observing a particular value.\nLet’s imagine again that the distribution of weight among college students has a mean of 65 kg, and standard deviation of 20 kg.\n\n\nCodePlot\n\n\n\nweight <- seq(40, 90, 0.1)\n\nprobability <- pnorm(weight, \n                     mean = 65,\n                     sd = 20)\nplot(weight, \n     probability, type = \"l\")\nabline(v=76, h =  0.7088403, col = \"red\")\n\n\n\n\n\n\n\n\n\n\n\n\nA weight of 76 kg has a probability of 0.71, it is likely to see this weight in the college students under my assumptions."
  },
  {
    "objectID": "lecture5/centralTendency.html#skewness",
    "href": "lecture5/centralTendency.html#skewness",
    "title": "Central tendency and variability",
    "section": "Skewness",
    "text": "Skewness\n\nI left some concepts behind because I got excited talking about the CDF.\nOne important concept to describe a distribution is skewness."
  },
  {
    "objectID": "lecture5/centralTendency.html#skewness-ii",
    "href": "lecture5/centralTendency.html#skewness-ii",
    "title": "Central tendency and variability",
    "section": "Skewness II",
    "text": "Skewness II\n\nWe say that a distribution is right skewed when the tail is longer to the right:\n\n\nset.seed(5696)\n\nN <- 1000\n\n### Number of times people check Instagram\nweight <- rnbinom(N, 10, .5)\n\nplot(density(weight, kernel = \"gaussian\" ),\n     ylab = \"p(y) or likelihood\",\n     xlab = \"How many times people check Instagram?\",\n     main = \"Density plot of How many times people check Instagram?\")\n\n\n\n\n\nWe say that a distribution is left skewed when the left tail is longer:\n\n\nset.seed(5696)\n\nplot(density(rbeta(300,60,2)),\n     ylab = \"p(y) or likelihood\",\n     xlab = \"Percentage of people who pass this class\",\n     main = \"Density plot of Percentage of people who pass this class\")"
  },
  {
    "objectID": "lecture5/centralTendency.html#central-tendency-measures",
    "href": "lecture5/centralTendency.html#central-tendency-measures",
    "title": "Central tendency and variability",
    "section": "Central tendency measures",
    "text": "Central tendency measures\n\nIn this lecture we will study several basic concepts, but don’t fool yourselves by thinking that these topics are less important.\nThese concepts are the foundations to understand what is coming in this class.\nWe will learn about several measures to describe and understand continuous distributions.\nRemember that this are just a few measures, if time allows we will study more options to describe distributions.\nLet’s focus on the most common type of average you’ll see in psychology:\n\n\\[\\begin{equation}\n\\bar{X} = \\frac{\\sum X}{n}\n\\end{equation}\\]\nWhere:\n\nthe letter X with a line above it (also sometimes called “X bar”) is the mean value of the group of scores or the mean.\nthe \\(\\sum\\) or the Greek letter sigma, is the summation sign, which tells you to add together whatever follows it to obtain a total or sum.\nthe X is each observation\nthe \\(n\\) is the size of the sample from which you are computing the mean.7\n\nExample\n\nsetwd(\"C:/Users/emontenegro1/Documents/MEGA/stanStateDocuments/PSYC3000/lecture5\")\n\nrum <- read.csv(\"ruminationComplete.csv\")\n\nmean(rum$age)\n\n[1] 15.34906\n\n\n\n\n\n\n\n\nNote\n\n\nWe will use \\(M=\\) to represent the word mean"
  },
  {
    "objectID": "lecture5/centralTendency.html#references",
    "href": "lecture5/centralTendency.html#references",
    "title": "Central tendency and variability",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html",
    "href": "lecture7/hypothesisTesting.html",
    "title": "Introduction to Hypothesis Testing",
    "section": "",
    "text": "We will study the concept of p-value a.k.a significance test.\nI will introduce the concept of hypothesis testing.\nI will also introduce the mean comparison for independent groups.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#what-is-a-p-value",
    "href": "lecture7/hypothesisTesting.html#what-is-a-p-value",
    "title": "Introduction to Hypothesis Testing",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\np-value stands for probability value.\nIt was born as a measure to reject a hypothesis.\nIn statistics and science, we always have hypothesis in mind. Statistics translates our hypothesis into evaluations of our hypothesis.\nFor example, we often ask questions to our selves such as: why my boyfriend won’t express her feelings? and then we ask, is this related to gender? Is it true that women share easily their emotions compare to men? If so, does it happen only to me? Is this a coincidence?\nWe can create a hypothesis with these questions, let’s try to write one:\n\n\\(H_{1}\\) = There is a difference in emotional expression between cisgender women and cisgender men.\n\nThis is our alternaive hypothesis but we need a null hypothesis:\n\n\\(H_{0}\\) = There is no difference in emotional expression between cisgender women and cisgender men.\n\nThat was easy you might think, but why do we need a null statement?\n\nScience always starts with a null believe, and what we do as scientist is to collect evidence that might help to reject the null hypothesis. If you collect data to support your alternative hypothesis you would be doing something called “confirmation bias”.\nConfirmation bias consist of collecting information that only supports your alternative hypothesis.\nFor example, you start collecting data that only proofs that all swans are white, instead of looking at information that helps to reject the null: not all swans are white."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#what-is-a-p-value-ii",
    "href": "lecture7/hypothesisTesting.html#what-is-a-p-value-ii",
    "title": "Introduction to Hypothesis Testing",
    "section": "What is a p-value? II",
    "text": "What is a p-value? II\n\nWe can write out null hypothesis in a statistical statement:\n\n\\(H_{0}\\) = The mean difference in emotional expression between cisgender women and cisgender men is equal to zero.\n\nIn the previous hypothesis we know that we are focusing in the mean difference, it is more specific.\nThe very first step to test our null hypothesis is to create a null model.\nA null model is a model where there is not any difference between groups, or there is not relationship between variables."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#what-is-a-p-value-iii",
    "href": "lecture7/hypothesisTesting.html#what-is-a-p-value-iii",
    "title": "Introduction to Hypothesis Testing",
    "section": "What is a p-value? III",
    "text": "What is a p-value? III\n\nIn my new model I will find a null model for the correlation between rumination and depression.\nTo create our new model we will re sample and shuffle our observed data. This is similar to have two decks of cards and you shuffle your cards multiple times until it is hard to guess which card will come next, and imagine cards have equal probability to be selected.\nThis procedure is called permutations, this will help us to create a distribution of null correlations. This means, all the correlations produced by my null model are produced by chance."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#what-is-a-p-value-iv",
    "href": "lecture7/hypothesisTesting.html#what-is-a-p-value-iv",
    "title": "Introduction to Hypothesis Testing",
    "section": "What is a p-value? IV",
    "text": "What is a p-value? IV\n\nLet’s see the following example, remember the estimated correlation between rumination and depression is \\(r= 0.58\\). This null model will help us to know if the correlation is explained by chance.\n\n\nCodeFigure\n\n\n\nrum <- read.csv(\"ruminationComplete.csv\", na.string = \"99\") ## Imports the data into R\n\nrum_scores <- rum %>% mutate(rumination = rowSums(across(CRQS1:CRSQ13)),\n                             depression =  rowSums(across(CDI1:CDI26))) ### I'm calculating\n                                                                       ## total scores\n\n\ncorr <- cor(rum_scores$rumination, rum_scores$depression,\n            use =  \"pairwise.complete.obs\") ## Correlation between rumination and depression\n\n### Let's create a distribution of null correlations\n\nnsim <- 100000\n\ncor.c <- vector(mode = \"numeric\", length = nsim)\n\nfor(i in 1:nsim){\ndepre <- sample(rum_scores$depression, \n                212, \n                replace = TRUE)\n\nrumia <- sample(rum_scores$rumination, \n                212, \n                replace = TRUE)\n\ncor.c[i] <- cor(depre, rumia, use =  \"pairwise.complete.obs\")\n}\n\n\nhist(cor.c, breaks = 120, \n     xlim= c(min(cor.c), 0.70),\n     main = \"Histograma of null correlations\")\nabline(v = corr, col = \"darkblue\", lwd = 2, lty = 1)\nabline(v = c(quantile(cor.c, .025),quantile(cor.c, .975) ),\n col= \"red\",\n lty = 2,\n lwd = 2)"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#what-is-a-p-value-v",
    "href": "lecture7/hypothesisTesting.html#what-is-a-p-value-v",
    "title": "Introduction to Hypothesis Testing",
    "section": "What is a p-value? V",
    "text": "What is a p-value? V\nLet’s estimate the probability of seeing \\(r = 0.58\\) according to our null model.\n\npVal <- 2*mean(cor.c >= corr)\npVal\n\n[1] 0\n\n\n\nThe probability is a number close to \\(0.00\\).\nWe now conclude that a correlation as extreme as \\(r = 0.58\\) is not explained by chance alone.\n\n\n\n\n\n\n\nNote:\n\n\nThe ugly rule of thumb is to consider a p-value <= .05 as evidence of small probability."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#mean-difference",
    "href": "lecture7/hypothesisTesting.html#mean-difference",
    "title": "Introduction to Hypothesis Testing",
    "section": "Mean difference",
    "text": "Mean difference\n\nWe can also do the same for the difference between means.\nIn this example, my null model is a model with null differences.\nI also conducted permutations on this example.\nWe will estimate if the difference by sex in rumination is explained by chance.\nIn this sample the mean difference in rumination between males and females is \\(\\Delta M\\) = 2.74.\n\n\nCodeFigure\n\n\n\nset.seed(1236)\n\nob.diff <- mean(rum_scores$rumination[rum_scores$sex==0], na.rm = TRUE )- mean(rum_scores$rumination[rum_scores$sex==1], na.rm = TRUE)\n\n### let's create a distribution of null differences\n\nnsim <- 100000\n\ndiff.c <- vector(mode = \"numeric\", length = nsim)\n\n### This is something called \"loop\", you don't have to pay attention to this.\n\nfor(i in 1:nsim){\nwomen <- sample(rum_scores$rumination[rum_scores$sex==0], \n                length(rum_scores$rumination[rum_scores$sex==0]), \n                replace = TRUE)\n\nmen <- sample(rum_scores$rumination[rum_scores$sex==1], \n                length(rum_scores$rumination[rum_scores$sex == 1]), \n                replace = TRUE)\n\ndiff.c[i] <- mean(women,  na.rm = TRUE)-mean(men,  na.rm = TRUE)\n} \n\n\nhist(diff.c, breaks = 120, \n     #xlim= c(min(diff.c), 0.70),\n     main = \"Histogram of Null Differences\")\nabline(v = ob.diff, col = \"darkblue\", lwd = 2, lty = 1)\nabline(v = c(quantile(diff.c, .025), quantile(diff.c, .975) ),\n col= \"red\",\n lty = 2,\n lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see the probability of seeing a value as large as 2.74\n\n\npVal <- 2*mean(diff.c >= ob.diff)\npVal\n\n[1] 0.99652"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#mean-difference-ii",
    "href": "lecture7/hypothesisTesting.html#mean-difference-ii",
    "title": "Introduction to Hypothesis Testing",
    "section": "Mean difference II",
    "text": "Mean difference II\n\nIn real life, we don’t have to estimate a null model “by hand” as I did before.\nR and JAMOVI will help us on that because the null model is already programmed.\nIn addition, when we compare independent means, we don’t usually do permutations. We follow something called the \\(t\\) - distribution , let’s study more about this distribution: t-distribution."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#mean-difference-iii",
    "href": "lecture7/hypothesisTesting.html#mean-difference-iii",
    "title": "Introduction to Hypothesis Testing",
    "section": "Mean difference III",
    "text": "Mean difference III\n\nThe \\(t\\)-distribution helped to develop the test named t-Student Test.\nIn this test we use the \\(t\\)-distribution as our model to calculate the probability to observe a value as extreme as 2.74.\nBut this probbaility will be estimated following the Cumulative Density Function (CDF) of the \\(t\\)-distribution."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#t-test",
    "href": "lecture7/hypothesisTesting.html#t-test",
    "title": "Introduction to Hypothesis Testing",
    "section": "\\(t\\)-Test",
    "text": "\\(t\\)-Test\n\nThe Student’s test is also known as a the “t-test”.\nIn this test, we will transform the mean difference of both groups into a \\(t\\) value.\n\n\\[\\begin{equation}\nt= \\frac{\\bar{X}_{1} - \\bar{X}_{2}}{\\sqrt{\\Big [\\frac{(n_{1}-1)s^{2}_{1}+(n_{2}-1)s^{2}_{2}}{n_{1} + n_{2}-2}\\Big ]\\Big [\\frac{n_{1}+n_{2}}{n_{1}n_{2}} \\Big ]}}\n\\end{equation}\\]\n\nIn this transformation \\(n_{1}\\) is the sample size for group 1, \\(n_{2}\\) is the sample size for group 2, \\(s^2\\) means variance. The \\(\\bar{X}\\) represents the mean.\nThis formula will help us to tranform the oberved difference in means to a value that comes from the \\(t\\)-distribution."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#t-test-iii",
    "href": "lecture7/hypothesisTesting.html#t-test-iii",
    "title": "Introduction to Hypothesis Testing",
    "section": "\\(t\\)-Test III",
    "text": "\\(t\\)-Test III\n\nRemember we talked about the \\(t\\)-distribution’s CDF. This CDF will help us to estimate the probability of seeing a value. the y-axis represents probability values.\n\n\nvalores <- seq(-4,4, by = 0.1)\n\nprobabilidad <- pt(valores, df = 10)\n\nplot(valores, \n     probabilidad, \n     type = \"l\",\n     xlab = \"t-value\",\n     ylab = \"t-value probability\",\n     main = \"t-distribution's CDF\")"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#t-test-iv",
    "href": "lecture7/hypothesisTesting.html#t-test-iv",
    "title": "Introduction to Hypothesis Testing",
    "section": "\\(t\\)-Test IV",
    "text": "\\(t\\)-Test IV\n\nWe can see how useful is a \\(t\\)-test by presenting a applied example.\nIn this example we will try to reject the null hypothesis that says:\n\n“The rumination score in males is equal to the rumination score in females”\n\nWe represent this hypothesis in statitistic like this:\n\n\\[\\begin{equation}\nH_{0}: \\mu_{1} = \\mu_{0}\n\\end{equation}\\]\n\nAlso in this example I’m introducing a new function in R named t.test(). This is the function that will helps to know if we can reject the null hypothesis.\nThe function t.test() requires a formula created by using tilde ~.\nIn R the the variable on the right side of ~ is the independent variable, the variable on the left side of ~ is the dependent variable.\nIn a independent samples \\(t\\)-test the independent variable is always the group, and the dependent variable is always any continuous variable.\n\n\nCodePlot\n\n\n\nt.test(rumination ~ sex, data = rum_scores, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2457, df = 203, p-value = 0.0258\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3347849 5.1535481\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nIn this example, we found that the \\(p\\)-value is 0.03 and the \\(t\\)-value is 2.25. This means:\n\n***“IF we repeat the same analysis with multiple samples the probability of finding a*** \\(t\\)-value = 2.25 is p = 0.03, under the assumptions of the null model”.\n\nThis is a very small probability, what do you think? Is 2.25 a value explainable by chance alone?"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#t-test-assumptions",
    "href": "lecture7/hypothesisTesting.html#t-test-assumptions",
    "title": "Introduction to Hypothesis Testing",
    "section": "\\(t\\)-test Assumptions",
    "text": "\\(t\\)-test Assumptions\n\nWe haven’t talked about the assumptions of the t-test model.\nRemember that all models have assumptions, we have to assume something to study Nature.\nThe \\(t\\)-test model assumes that the difference in means is generated by a normally distributed process.\nIt also assumes that variances are equal on both groups.\nLet’s see what happens when we assume equal variances but the data does not come from a process with equal variances:\n\n\n\nShow the code\nset.seed(1234)\n\nN1 <- 50 ## Sample size group 1\n\nN2 <- 50 ### sample size group 2\n\nMean1 <- 100 ## Mean group 1\n\nMean2 <- 20 ### Mean group 2\n\nresults <- list()\n\n\nfor(i in 1:10000){\ngroup1 <- rnorm(N1, mean = Mean1, sd = 100) ### variances or standard deviation are not equal\n\ngroup2 <-  rnorm(N2, mean = Mean2, sd = 200) ### variances or standard deviation are not equal\n\ndataSim <- data.frame(genValues = c(group1,group2), \n                      groupVar = c(rep(1,N1),rep(0,N2)))\n\nresults[[i]] <- t.test(genValues ~ groupVar, data = dataSim, var.equal = TRUE)$p.value\n}\n\n### Proportion of times we rejected the null hypothesis\n\ncat(\"Proportion of times we rejected the null hypothesis\",sum(unlist(results) <= .05)/length(results)*100)\n\n\nProportion of times we rejected the null hypothesis 70.5\n\n\n\nWe successfully rejected the null hypothesis in only 70.5% of the data sets generated. But in reality the \\(t\\)-test should reject the null hypothesis 100% of the times.\nLet’s check when we assume equal variances in our \\(t\\) - test and the model that generates is actually a process with equal variances:\n\n\n\nShow the code\nset.seed(1234)\n\nN1 <- 50\n\nN2 <- 50\n\nMean1 <- 100\n\nMean2 <- 20\n\nresults_2 <- list()\n\nfor(i in 1:10000){\n  \ngroup1 <- rnorm(N1, mean = Mean1, sd = 5) ## equal variances or standard deviation\n\ngroup2 <-  rnorm(N2, mean = Mean2, sd = 5) ## equal variances or standard deviation\n\ndataSim <- data.frame(genValues = c(group1,group2), \n                      groupVar = c(rep(1,N1),rep(0,N2)))\n\nresults_2[[i]] <- t.test(genValues ~ groupVar, data = dataSim, var.equal = TRUE)$p.value\n}\n\n### Probability of rejecting the null hypothesis\n\ncat(\"Proportion of times we rejected the null hypothesis\", sum(unlist(results_2) <= .05)/length(results_2)*100)\n\n\nProportion of times we rejected the null hypothesis 100\n\n\n\nThis time we are rejecting the null hypothesis 100% of the times. This is what we were looking for! Remember that we generated data from a process where group 1 had a mean of 100, and the group 2 had a mean of 20. The t-test should reject the null hypothesis every time I generate new data sets, but this doesn’t happen when I made the wrong assumption: I assumed equal variances when I should not do it.\nSummary: when we wrongly assume that the variances are equal between groups, we decrease the probability to reject the null hypothesis when the null should be rejected. This is bad!\nThese simulations showed the relevance of respecting the assumptions of the \\(t\\)-test."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption",
    "href": "lecture7/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption",
    "title": "Introduction to Hypothesis Testing",
    "section": "How do we know if my observed data holds the assumption ?",
    "text": "How do we know if my observed data holds the assumption ?\n\nThere are tests to evaluate the assumption of equivalence of variance between groups.\nThe most used test to evaluate the homogeneity of variance is the Levene’s Test for Homogeneity of Variance.\nWe can implement this test in R using the function leveneTest() , this function comes with the R package car. You might need to install this package in case you don’t have it installed in your computer, you can run the this line of code to install it: install.packages(\"car\")\nI’m going to test if the variance of rumination holds the assumption of equality of variance by sex:\n\n\nlibrary(car)\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nleveneTest(rumination ~ as.factor(sex), \n           data = rum_scores) \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1  0.2541 0.6148\n      203               \n\n\n\nIn this test the null hypothesis is “The variances of group 1 and group 2 are equal”, if the \\(p\\)-value is less or equal to 0.05 we reject the null hypothesis. In the output above you can see the p-value under the column Pr(>F).\nIn the case of rumination, the \\(p\\)-value = 0.61, given that the \\(p\\)-value is higher than 0.05, we don’t reject the null hypothesis. We can assume the variances of rumination by sex are equal."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption-ii",
    "href": "lecture7/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption-ii",
    "title": "Introduction to Hypothesis Testing",
    "section": "How do we know if my observed data holds the assumption ? II",
    "text": "How do we know if my observed data holds the assumption ? II\n\nWhat happens if the Levene’s Test rejects the null hypothesis of homogeneity of variance?\nCan we continue using the \\(t\\) - test to evaluate my hypothesis?\nThe answer is: Yes you can do a \\(t\\)-test but there is a correction on the degrees of freedom. We will talk more about degrees of freedom in the next sessions.\nIf you cannot assume equality of variances, all you have to do in R is to switch the argument var.equal = TRUE to var.equal = FALSE.\n\n\nt.test(rumination ~ sex, data = rum_scores, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2841, df = 149.72, p-value = 0.02377\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3702483 5.1180847\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nNow , the output says we are performing a Welch Two Sample t-test, Welch was tha mathematician who found the correction."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size",
    "href": "lecture7/hypothesisTesting.html#effect-size",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size",
    "text": "Effect size\n\nUp to this point we have studied how we test our hypothesis when we compare independent means, but we still have to answer the question, how large is a large difference between means? Or, how small is a small difference? In fact, what is considered a small difference?\nThese questions were answered by Jacob Cohen (1923-1998).\nCohen created a standardized measure to quantify the magnitude of the difference between means.\nLet’s see a pretty plot about it in this link."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#time-to-check-more-formulas",
    "href": "lecture7/hypothesisTesting.html#time-to-check-more-formulas",
    "title": "Introduction to Hypothesis Testing",
    "section": "Time to check more formulas",
    "text": "Time to check more formulas\nCohen’s \\(d\\) Effect Size:\n\\[\\begin{equation}\nES = \\frac{\\bar{X}_{1}-\\bar{X}_{2}}{\\sqrt{\\Big[\\frac{s^2_{1}+s^2_{2}}{2} \\Big ]}}\n\\end{equation}\\]\nWhere:\n\n\\(ES\\) = is effect size.\n\n\\(\\bar{X}\\) = Mean.\n\n\\(s^2\\) = variance."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example",
    "href": "lecture7/hypothesisTesting.html#effect-size-example",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example",
    "text": "Effect size example\n\nLet’s compute an effect size in R. In this example we will resume our \\(t\\)-test example of rumination by sex.\n\n\nt.test(rumination ~ sex, data = rum_scores)\n\n\n    Welch Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2841, df = 149.72, p-value = 0.02377\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3702483 5.1180847\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nIn the \\(t\\)- test output we have information we can use to compute the effect size. We have the mean for each group \\(M_{(female)} = 31.21\\), \\(M_{(male)}= 28.46\\). We already know that the difference between groups is not explained by chance alone (\\(p\\)= 0.02) because the p-value is smaller than 0.05.\nNow let’s compute the effect size,\n\n\nlibrary(dplyr)\n\n### This will create tidy table by group (sex)\nInfoByBroup <- rum_scores %>% group_by(sex) %>%\n    summarize(Mean = mean(rumination, na.rm = TRUE),  \n          Var = var(rumination, na.rm = TRUE))\n\n\nkable(InfoByBroup, \n      caption = \"Rumination mean and variance by sex\", \n      digits = 2)\n\n\n\nRumination mean and variance by sex\n \n  \n    sex \n    Mean \n    Var \n  \n \n\n  \n    0 \n    31.21 \n    71.88 \n  \n  \n    1 \n    28.46 \n    64.40 \n  \n\n\n\n\n\nEffect size computation\n\n\nShow the code\nmeanFemales <- InfoByBroup$Mean[1]\n\nmeanMales <- InfoByBroup$Mean[2]\n\nvarianceFemales <- InfoByBroup$Var[1]\n\nvarianceMales <- InfoByBroup$Var[2]\n\neffectSize <- (meanFemales-meanMales)/sqrt((varianceFemales+varianceMales)/2)\n\ncat(\"The effect size of sex on rumination is\", \"d =\", round(effectSize,2))\n\n\nThe effect size of sex on rumination is d = 0.33\n\n\n\nAccording to Cohen (2013) a small effect size is \\(d = 0.20\\), a medium effect size is \\(d = 0.50\\), a large effect size is \\(d= .80\\)"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example-ii",
    "href": "lecture7/hypothesisTesting.html#effect-size-example-ii",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example II",
    "text": "Effect size example II\n\nWe can make this calculation more fun. Let’s imagine you are a clinician, and you need a way to understand how large is the difference in rumination and depression. You need to find a way to quantify which variable you should pay more attention, which variable has larger differences between cisgender males and cisgender females.\nWhy would you care about differences between cisgender males and cisgender females? Why would you care to compare the differences between rumination and depression?\nFirstly, we need to compute the depression score. In the rumination data set you’ll find columns named CDI these are the items that correspond to the Children’s Depression Inventory. We will use this items to compute a total score.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nrum_scores <- rum_scores %>% \n  mutate(depression = rowSums(across(CDI1:CDI26))) ## Computes total score\n\nWe need to check if the variances our depression score are equal between groups before running a \\(t\\)-test:\n\nlibrary(car)\n\nleveneTest(depression ~ as.factor(sex), \n           data = rum_scores) \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(>F)\ngroup   1  0.8682 0.3526\n      205               \n\n\n\nThe Leven’s Test for Homogeneity of Variance did not reject the null hypothesis of equality of variances. Hence, we can hold the assumption of equality of variances. We don’t need to perform a correction\n\n\n## This test assumes equality of variances after performing the Levene's Test.\nt.test(depression ~ sex, data = rum_scores, var.equal = TRUE) \n\n\n    Two Sample t-test\n\ndata:  depression by sex\nt = 2.5468, df = 205, p-value = 0.01161\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.5257305 4.1298205\nsample estimates:\nmean in group 0 mean in group 1 \n      11.257353        8.929577 \n\n\n\nNice! Look at the result, we found a difference in depression by cisgender (sex), and it is not explained by chance alone.\n\n\n\n\n\n\n\nImportant\n\n\nHow do you know the difference is not explained by chance alone?"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example-iii",
    "href": "lecture7/hypothesisTesting.html#effect-size-example-iii",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example III",
    "text": "Effect size example III\n\nAfter estimating the \\(t-test\\) of depression by sex, we can now quantify the effect’s magnitude:\n\n\nInfoByBroupDepression <- rum_scores %>% group_by(sex) %>%\n    summarize(Mean = mean(depression, na.rm = TRUE),  \n          Var = var(depression, na.rm = TRUE))\n\n\nkable(InfoByBroupDepression, \n      digits= 2, \n      caption = \"Depression mean and variance by sex\")\n\n\n\nDepression mean and variance by sex\n \n  \n    sex \n    Mean \n    Var \n  \n \n\n  \n    0 \n    11.26 \n    36.86 \n  \n  \n    1 \n    8.93 \n    43.04 \n  \n\n\n\n\n\nEffect size computation\n\n\nShow the code\nmeanFemales <- InfoByBroupDepression$Mean[1]\n\nmeanMales <- InfoByBroupDepression $Mean[2]\n\nvarianceFemales <- InfoByBroupDepression$Var[1]\n\nvarianceMales <- InfoByBroupDepression$Var[2]\n\neffectSizeDepression <- (meanFemales-meanMales)/sqrt((varianceFemales+varianceMales)/2)\n\ncat(\"The effect size of sex on depression is\", \"d =\", round(effectSizeDepression,2))\n\n\nThe effect size of sex on depression is d = 0.37\n\n\n\nAs you can see the effect size of sex on depression is \\(d=\\) 0.37 whereas in rumination the effect size of sex is \\(d=\\) 0.33"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example-cont.",
    "href": "lecture7/hypothesisTesting.html#effect-size-example-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nWhat is your conclusion as a clinician? Is rumination too much different from depression?\nShould we focus on an intervention that addresses depression or/and rumination differently according to sex?\nShould you create a depression intervention accounting for the effect of sex ? Can you ignore rumination for your intervention?\nShould we focus mainly on women because the difference is large between groups?"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example-cont.-1",
    "href": "lecture7/hypothesisTesting.html#effect-size-example-cont.-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nAs always there are other ways to compute the effect size.\nThe next formula takes into account occasions when the sample size is not the same between groups.\n\n\\[\\begin{equation}\nES = \\frac{\\bar{X}_{1} - \\bar{X}_{2}}{\\sqrt{\\Big [\\frac{(n_{1}-1)s^{2}_{1}+(n_{2}-1)s^{2}_{2}}{n_{1} + n_{2}-2}\\Big ]}}\n\\end{equation}\\]\nWhere:\n\\(\\bar{X}\\) = Mean\n\n\\(s^2\\) = variance\n\n\\(n\\) = sample size\n\n\nYou can study more about effect sizes by clicking this link. It is an article with a good explanation and summary. We will talk about effect sizes again when we study Analysis of Variance (ANOVA)."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#effect-size-example-cont.-2",
    "href": "lecture7/hypothesisTesting.html#effect-size-example-cont.-2",
    "title": "Introduction to Hypothesis Testing",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nWe can also estimate the effect sizes with pacakages already programmed to be used in R. In this case we can use the package effectsize.\nYou don’t have this package installed in your R installation. You will have to install the package by running install.packages(\"effectsize\", dependencies = TRUE).\nYou only need to install a package one time. You don’t have to install packages everytime you open RStudio.\n\n\nRuminationDepression\n\n\n\nlibrary(effectsize)\n\ncohens_d(rumination ~ sex,\n         data = rum_scores) ### Equal variances assumed\n\nWarning: Missing values detected. NAs dropped.\n\n\nCohen's d |       95% CI\n------------------------\n0.33      | [0.04, 0.62]\n\n- Estimated using pooled SD.\n\ncohens_d(rumination ~ sex,\n         data = rum_scores, \n         pooled_sd = FALSE) ### does not assume equal variances\n\nWarning: Missing values detected. NAs dropped.\n\n\nCohen's d |       95% CI\n------------------------\n0.33      | [0.04, 0.62]\n\n- Estimated using un-pooled SD.\n\n\n\n\n\ncohens_d(depression ~ sex,\n         data = rum_scores) ### Equal variances assumed\n\nWarning: Missing values detected. NAs dropped.\n\n\nCohen's d |       95% CI\n------------------------\n0.37      | [0.08, 0.66]\n\n- Estimated using pooled SD.\n\ncohens_d(depression ~ sex,\n         data = rum_scores, \n         pooled_sd = FALSE) ### does not assume equal variances\n\nWarning: Missing values detected. NAs dropped.\n\n\nCohen's d |       95% CI\n------------------------\n0.37      | [0.07, 0.66]\n\n- Estimated using un-pooled SD."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#jamovi",
    "href": "lecture7/hypothesisTesting.html#jamovi",
    "title": "Introduction to Hypothesis Testing",
    "section": "JAMOVI",
    "text": "JAMOVI\n\nI hope you remember JAMOVI. I mentioned this software in the first class.\nIt is also listed in the syllabus.\nThis is a software that runs R using a interface that does not require to write the R code. JAMOVI will write the R code for you behind scenes, it also runs the code for you. Pretty awesome isn’t it?"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#central-limit-theory-clt-1",
    "href": "lecture7/hypothesisTesting.html#central-limit-theory-clt-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Central Limit Theory (CLT)",
    "text": "Central Limit Theory (CLT)\n\nThe Central Limit Theory (CLT) is a fact, it describes a rule that happens everywhere in Nature.\n\nThe CLT states: For any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means for sample size \\(n\\) will have a mean of \\(\\mu\\) and a standard deviation of \\(\\frac{\\sigma}{\\sqrt{n}}\\) and will approach a normal distribution as \\(n\\) approaches infinity.\n\nThe beauty of this theorem is due to:\nit describes the distribution of sample means for any population, no matter what shape, mean, or standard deviation.\nThe distribution of sample means “approaches” a normal distribution very rapidly.\nIt is better to explain this theorem with a simulation, as always we do in this class."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.",
    "href": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nIn this example I’ll generate 1000 “datasets” from a Gaussian process (normal distribution) with M = 60, sd = 100. Each data set will contain 10 observations.\nAfter generating 1000 data sets, I’m estimating the mean of those simulated values.\nImagine you are asking the question how many minutes do you need to take a shower? to 10 participants, you then conduct the same study the next day with a different set of 10 participants every day until you get 1000 sets of 10 participants.\n\n\nCodePlot\n\n\n\nset.seed(563)\nN <- 10\n\nM <- 60\n\nSD <- 100\n\n## Number of data sets or replications\nrep <- 1000\n\nresults <- list()\n\nfor(i in 1:rep){\n\nresults[[i]] <- mean(rnorm(n = N, mean = M, sd = SD))\n}\n\n\n\n\nplot(density(unlist(results)),\n     xlab = \"Sample means\",\n     ylab = \"p(y) or likelikood\",\n     main = \"Density plot of Sample means\")"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-1",
    "href": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nThere is more beautiful qualities about the CLT. This rule applies to all distributions. When you increase the number of observations per sample, and the number of sample approach infinity. The sample of means will look Gaussian distributed (normally distributed).\nRemember the Poisson distribution I mentioned some lectures behind?\nAs you can see the Poisson distribution after generating 1 data set of 10 observations doesn’t look like a Gaussian process at all!\nCan the sample of means from a Poisson process generate a pretty bell shape?\n\n\nCodeFigure or plot\n\n\n\nset.seed(563)\nplot(density(rpois(n = 5,lambda = 8)),\n     xlab = \"Number of times people take a shower in a week\",\n     ylab = \"p(y) or likelihood\",\n     main = \"Density plot of a Poisson distributed variable\")\n\n\n\n\nset.seed(563)\nplot(density(rpois(n = 10,lambda = 10)),\n     xlab = \"Number of times people take a shower in a week\",\n     ylab = \"p(y) or likelihood\",\n     main = \"Density plot of a Poisson distributed variable\")"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-2",
    "href": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-2",
    "title": "Introduction to Hypothesis Testing",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nLet’s find out the answer by simulating 1000 data sets:\n\n\nCodePlot\n\n\n\nset.seed(563)\nN <- 10\n\nM <- 10\n\n## Number of data sets or replications\nrep <- 1000\n\nresults <- list()\n\nfor(i in 1:rep){\n\nresults[[i]] <- mean(rpois(n = N, lambda = M))\n}\n\n\n\n\nplot(density(unlist(results)),\n     xlab = \"Sample of means (Number of times people take a shower in a week)\",\n     ylab = \"p(y) or likelihood\",\n     main = \"Density plot of sample of means from a Poisson data process\")\n\n\n\n\n\n\n\n\nThere is also something even prettier, the mean estimated with all the means, is actually close to the data generating process. It is lovely! This is a natural rule!\n\n\nmean(unlist(results))\n\n[1] 10.0076"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-3",
    "href": "lecture7/hypothesisTesting.html#central-limit-theorem-clt-cont.-3",
    "title": "Introduction to Hypothesis Testing",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nWe saw that the mean of sample means is close to the data generating process mean , and it doesn’t matter the type of distribution. However, take into account that there are probability distribution models that will require more than 800000000000 sampled means to approach the Gaussian shape. But eventually the distribution of all the means will look like a Gaussian distribution.\nIn simple terms, what does it mean? It means that the mean is an unbiased estimate on average it tends to be close to the “population” mean. This is why we like the mean as an estimate , and we use it to test hypothesis."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#standard-error",
    "href": "lecture7/hypothesisTesting.html#standard-error",
    "title": "Introduction to Hypothesis Testing",
    "section": "Standard Error",
    "text": "Standard Error\n\nThe CLT will be helpful to introduce the concept of standard error.\nThe Standard Error is a measure of how far is the mean of my sample from the true value in the data generating process.\nThe Standard Error in the context of multiple samples is the standard deviation of all sampled means.\nWe should go to another example:\n\n\n\nShow the code\nset.seed(563)\nN <- 10\n\nM <- 60\n\nSD <- 100\n\n## Number of data sets or replications\nrep <- 1000\n\nresults <- list()\n\nfor(i in 1:rep){\n\nresults[[i]] <- mean(rnorm(n = N, mean = M, sd = SD))\n}\n\ncat(\"The Standard Deviation of 1000 sampled means is\", sd(unlist(results)))\n\n\nThe Standard Deviation of 1000 sampled means is 33.25718\n\n\n\nin this example if we estimate the standard deviation of our sample means we get sd = 33.2571782."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#standard-error-cont.",
    "href": "lecture7/hypothesisTesting.html#standard-error-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Standard Error (cont.)",
    "text": "Standard Error (cont.)\n\nIn real life, you’ll never know the standard deviation of sample means, because this is a simulated example.\nIn real application you need to estimate how far your observed mean is from the true mean.\nWe can use this estimation \\(\\frac{\\sigma}{\\sqrt(n)}\\) to approximate how far we are from the mean. In this formula \\(\\sigma\\) is the standard deviation and \\(n\\) is the sample size.\nThe standard error decreases when we increase the sample size, let’s see another example:\n\n\nCodePlot\n\n\n\n\nShow the code\nset.seed(563)\n\nN <- seq(10, 10000, by = 10)\n\nM <- 60\n\nSD <- 100\n\nresults <- list()\n\nfor(i in 1:length(N)){\n\nresults[[i]] <- sd(rnorm(n = N[i], mean = M, sd = SD))/sqrt(N[i])\n}\n\n\n\n\n\nplot(x = N,\n     y = results,\n     type = \"l\",\n     xlab = \"Number of observations in each sample\",\n     ylab = \"Standard error\",\n     main = \"Line plot of Standard Error by sample size\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn simple words: The standard error (SE) measures the distance from the true value in the data generating process."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#the-689599.7-rule-for-a-normal-distribution",
    "href": "lecture7/hypothesisTesting.html#the-689599.7-rule-for-a-normal-distribution",
    "title": "Introduction to Hypothesis Testing",
    "section": "The 68–95–99.7 Rule for a Normal Distribution",
    "text": "The 68–95–99.7 Rule for a Normal Distribution\n\nThere is an important concept in frequentist statistics called “confidence interval”\nBut before getting into details I need to explain a important property of the normal distribution.\nThe following rule applies for the normal distribution:\n\n68 \\(\\%\\) of values will be between \\(\\mu\\) - \\(\\sigma\\) and \\(\\mu\\) + \\(\\sigma\\) .\n95 \\(\\%\\) of values will be between \\(\\mu\\) - \\(2 \\sigma\\) and \\(\\mu\\) + \\(2 \\sigma\\).\n99.7 \\(\\%\\) of values will be between \\(\\mu\\) - \\(3 \\sigma\\) and \\(\\mu\\) + \\(3 \\sigma\\)\n\nWe can see it by generating values from a Gaussian process:\n\n\nlibrary(ggplot2)\n\nset.seed(3632)\n\ngeneratedValues <- rnorm(200000, mean = 50, sd = 2)\n\n### +-1 standard deviation form the mean\n\nupper <- 50 + 2\n\nlower <- 50 - 2\n\npercent1sigma <- (sum(generatedValues <= upper & generatedValues >= lower)\n/length(generatedValues))*100\n\n### +-2 standard deviation form the mean\n\nupper <- 50 + 2*2\n\nlower <- 50 - 2*2\n\npercent2sigma <- (sum(generatedValues <= upper & generatedValues >= lower)/length(generatedValues))*100\n\n### +-3 standard deviation form the mean\n\nupper <- 50 + 3*2\n\nlower <- 50 - 3*2\n\npercent3sigma <- (sum(generatedValues <= upper & generatedValues >= lower)/length(generatedValues))*100\n\n\n68.35 \\(\\%\\) of observations are located at \\(\\pm 1 \\sigma\\) from the data generating process mean. That is true!\n95.45 \\(\\%\\) of observations are located at \\(\\pm 2 \\sigma\\) from the data generating process mean. That is true!\n99.74 \\(\\%\\) of observations are located at \\(\\pm 3 \\sigma\\) from the data generating process mean. That is true!"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor",
    "href": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor",
    "title": "Introduction to Hypothesis Testing",
    "section": "Allow me to use a metaphor",
    "text": "Allow me to use a metaphor\nWestfall & Henning (2013):\n\nImagine there is a lion or perhaps a coyote around your town. Every day the coyote moves around 20 km from the town, forming a circular perimeter. The town is right in the middle of the circle:"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.",
    "href": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Allow me to use a metaphor (cont.)",
    "text": "Allow me to use a metaphor (cont.)\n\nAs you can imagine the lion or coyote could move further from the town but it is always wondering around town chasing chickens or killing cows.\nWe could collect data, for instance, the distance every day from town, and see on average how far it is from town. We could also estimate the standard deviation (). This could be a good measure to see patterns, and be confident about the interval (circle) where the lion or coyote walks."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.-1",
    "href": "lecture7/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Allow me to use a metaphor (cont.)",
    "text": "Allow me to use a metaphor (cont.)\n\nLet’s imagine that the town is our data generating process mean value (). Also imagine that each time the lion or coyote walks around the town is an independent observed sample from the data generating process.\nWe need to find a way to be confident that our observed data is closed to \\(\\mu\\) (“population mean”).\nA few minutes ago we saw that according to the a Gaussian distribution, you’ll find 95 \\(\\%\\) of the values around \\(\\pm 2 \\sigma\\) from the mean. However, in our simulated data we found out that in reality you’ll find more than 95 \\(\\%\\). It was actually 95.45 \\(\\%\\).\nWe’ll use this great property of the Gaussian distribution, but we will use \\(\\pm 1.96 \\sigma\\) instead of \\(\\pm 2 \\sigma\\). Why? because we want to be closer to exactly find 95\\(%\\) of the observations. This means, we want to be 95\\(\\%\\) confident that we are close to \\(\\mu\\), or the town following our example.\nWe will estimate the confidence interval by computing:\n\n\\[\\begin{equation}\n\\bar{x} \\pm 1.96 \\frac{\\hat{\\sigma}}{ \\sqrt{n}}\n\\end{equation}\\]\nWhere,\n\\(\\bar{x}\\) is the estimated mean.\n\\(\\hat{\\sigma}\\) is the estimated standard deviation.\n\\(1.96\\) is the 97.5 percentile in the standard normal distribution (Gaussian)."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#confidence-interval",
    "href": "lecture7/hypothesisTesting.html#confidence-interval",
    "title": "Introduction to Hypothesis Testing",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nLet’s compute the confidence interval by hand, we can use again the mean of the rumination score:\n\\[\\begin{equation}\n30.25 \\pm 1.96  \\Big (\\frac{8.41}{\\sqrt{205}} \\Big )\n\\end{equation}\\]\nWe need to estimate the mean first:\n\nRumMean <- mean(na.omit(rum_scores$rumination))\nRumMean\n\n[1] 30.25854\n\n\nSecond, we need to estimate the standard deviation:\n\nsd(na.omit(rum_scores$rumination))\n\n[1] 8.406725\n\n\nNow we can estimate the 95\\(\\%\\) confidence interval:\n\nupperCI <- 30.26+1.96*(8.41/sqrt(205))\n\nlowerCI <- 30.26-1.96*(8.41/sqrt(205))\n\ncat(\"The 95% confidence interval for the mean is:\", \n        \"upper-bound=\",\n        round(upperCI,2), \n        \"lower-bound=\",\n          round(lowerCI,2))\n\nThe 95% confidence interval for the mean is: upper-bound= 31.41 lower-bound= 29.11\n\n\nWe can also estimate the confidence interval using the package rcompanion:\n\nlibrary(rcompanion)\n\n\nAttaching package: 'rcompanion'\n\n\nThe following object is masked from 'package:effectsize':\n\n    phi\n\ngroupwiseMean(rumination ~ 1,\n              data   = rum_scores,\n              conf   = 0.95,\n              digits = 3,\n              na.rm = TRUE)\n\n   .id   n Mean Conf.level Trad.lower Trad.upper\n1 <NA> 205 30.3       0.95       29.1       31.4\n\n\n\nThere is something to note here. What you are doing is just multipliying the standard error of the mean by 1.96. This value = 1.96 is called critical value.\n\n\n##Standard error multiplied by 1.96\n1.96*(8.41/sqrt(205))\n\n[1] 1.151265\n\n\n\nWith the information above we could interpret our result as follows:\n\nI am approximately 95\\(\\%\\) confident that \\(\\mu\\) is within 1.15 score points of the sample average 30.26."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#confidence-interval-intepretation",
    "href": "lecture7/hypothesisTesting.html#confidence-interval-intepretation",
    "title": "Introduction to Hypothesis Testing",
    "section": "Confidence Interval Intepretation",
    "text": "Confidence Interval Intepretation\n\nMany of the concepts studied up to this point come from a theory called “frequentist”.\nThe frequentist theory studies the probability in terms of long run events. In this approach we have to imagine that we repeat the same experiment or study several times. Only if we repeat the same study several times we’ll be able to create a confidence interval.\nThis is, of course theoretical. We are able to calculate confidence intervals with one sample. But, this interval is only an approximation.\nThe interpretation of the confidence interval demands to imagine that you repeat the same study \\(n\\) number of times. Hence the interpretation would be:\n\nSince \\(\\mu\\) will lie within the upper and lower limits of similarly constructed intervals for 95% of the repeated samples,my sample is likely to be one of those samples where \\(\\mu\\) is within the upper and lower limits, and I am therefore 95% confident that \\(\\mu\\) is between 29.1 and 31.4.\n\\[\\begin{equation}\n29.1 \\le \\mu \\le 31.4\n\\end{equation}\\]"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#confidence-interval-multiple-means",
    "href": "lecture7/hypothesisTesting.html#confidence-interval-multiple-means",
    "title": "Introduction to Hypothesis Testing",
    "section": "Confidence Interval: multiple means",
    "text": "Confidence Interval: multiple means\n\n\nLet’s imagine a ask the same question: how many minutes do you need to take a shower?\nWe could ask the same question to 50 different people 100 000 times, and then estimate the mean. The frequentist theory says that the true value will be among the 95\\(\\%\\) of the sampled means."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means",
    "href": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means",
    "title": "Introduction to Hypothesis Testing",
    "section": "Repeated measures: compare two dependent means",
    "text": "Repeated measures: compare two dependent means\n\nWe have only studied how to test the difference between independet groups.\nHowever, many times we have research designs where we do a pre-test and a post-test.\nFor example, I could create an intervantion to treat depression. If I want to evaluate if there is an effect of the intervention I could measure the depression levels before the intervention, after that my participants will receive my new depression treatment. After they finish the intervention I measure again thei levels of depression.\nIn this example, we would like to see a lower score in depression after the intervention. We would also like to test if the mean difference between time 1 and time 2 is explained by chance alone.\n\nNull hypothesis: \\[\\begin{equation}\nH_{0} = \\mu_{posttest} = \\mu_{pretest}\n\\end{equation}\\]"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.",
    "href": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.",
    "title": "Introduction to Hypothesis Testing",
    "section": "Repeated measures: compare two dependent means (cont.)",
    "text": "Repeated measures: compare two dependent means (cont.)\n\\[\\begin{equation}\nt = \\frac{\\sum D}{\\sqrt{\\frac{n \\sum D^2 - \\big (\\sum D \\big )^2}{n-1}}}\n\\end{equation}\\]\n\\(D\\) is the difference between each individual’s score from the first to the second time point.\n\\(\\sum D\\) is the sum of all the differences between groups of scores.\n\\(\\sum D^2\\) is the sum of the differences squared between groups of scores.\n\\(n\\) is the number of pairs of observations."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.-1",
    "href": "lecture7/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.-1",
    "title": "Introduction to Hypothesis Testing",
    "section": "Repeated measures: compare two dependent means (cont.)",
    "text": "Repeated measures: compare two dependent means (cont.)\n\nWe can test if the mean difference in life expectancy is explained by chance.\nLet’s compare years 2019 and 2020:\n\n\nlifeExpec <- read.csv(\"lifeExpect.csv\")\n\nt.test(lifeExpec$X2019, lifeExpec$X2020, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  lifeExpec$X2019 and lifeExpec$X2020\nt = -0.19141, df = 243, p-value = 0.8484\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.07583952  0.06240551\nsample estimates:\nmean difference \n   -0.006717005 \n\n\nWe failed to reject the null hypothesis according to the \\(t\\)-test for dependent means. How did we arrived to this conclusion?\n\n\n\n\n\n\nTip\n\n\nPay attention to the \\(p-value\\). Do you remember the null hypothesis in this case?"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html",
    "href": "lecture8/correlation&regressionPart1.html",
    "title": "Correlation and Regression Models",
    "section": "",
    "text": "To introduce correlation to estimate relationships between two variables.\nTo introduce the notion of covariance.\nTo study scatter plots to visualize correlations.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#what-is-a-correlation-coefficient",
    "href": "lecture8/correlation&regressionPart1.html#what-is-a-correlation-coefficient",
    "title": "Correlation and Regression Models",
    "section": "What is a correlation coefficient ?",
    "text": "What is a correlation coefficient ?\n\nA correlation coefficient is a numerical index that reflects the relationship between two variables. The value of this descriptive statistic ranges between -1.00 and +1.00.\nA correlation between two variables is sometimes referred to as a bivariate (for two variables) correlation"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#what-is-a-correlation-coefficient-1",
    "href": "lecture8/correlation&regressionPart1.html#what-is-a-correlation-coefficient-1",
    "title": "Correlation and Regression Models",
    "section": "What is a correlation coefficient ?",
    "text": "What is a correlation coefficient ?\n\nAt the beginning we will study the correlation named Pearson product-moment.\nThere other types of correlation estimation depending on the data generating process of each variable.\nPearson product-moment deals with continuous DATA."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features",
    "href": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features",
    "title": "Correlation and Regression Models",
    "section": "Correlation interpretation and other features",
    "text": "Correlation interpretation and other features\nSalkind & Shaw (2020):"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features-cont.",
    "href": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features-cont.",
    "title": "Correlation and Regression Models",
    "section": "Correlation interpretation and other features (cont.)",
    "text": "Correlation interpretation and other features (cont.)\nSalkind & Shaw (2020):\n\nA correlation can range in value from \\(-1.00\\) to \\(+1.00\\).\nA correlation equal to 0 means there is no relationship between the two variables.\nThe absolute value of the coefficient reflects the strength of the correlation. So, a correlation of \\(-.70\\) is stronger than a correlation of \\(+.50\\). One frequently made mistake regarding correlation coefficients occurs when students assume that a direct or positive correlation is always stronger (i.e., “better”) than an indirect or negative correlation because of the sign and nothing else.\nA negative correlation is not a “bad” correlation.\nWe will use the letter r to represent correlation. For example \\(r= .06\\)."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features-cont.-1",
    "href": "lecture8/correlation&regressionPart1.html#correlation-interpretation-and-other-features-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "Correlation interpretation and other features (cont.)",
    "text": "Correlation interpretation and other features (cont.)\n\n\\(r_{xy}\\) is the correlation coefficient.\n\\(n\\) is the sample size.\n\\(X\\) represents variable \\(X\\).\n\\(Y\\) represents variable \\(Y\\).\n\\(\\Sigma\\) means summation or addition."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#lets-take-a-look-at-positive-correlations",
    "href": "lecture8/correlation&regressionPart1.html#lets-take-a-look-at-positive-correlations",
    "title": "Correlation and Regression Models",
    "section": "Let’s take a look at positive correlations",
    "text": "Let’s take a look at positive correlations"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#lets-take-a-look-at-negative-correlations-cont.",
    "href": "lecture8/correlation&regressionPart1.html#lets-take-a-look-at-negative-correlations-cont.",
    "title": "Correlation and Regression Models",
    "section": "Let’s take a look at negative correlations (cont.)",
    "text": "Let’s take a look at negative correlations (cont.)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#correlation-matrix",
    "href": "lecture8/correlation&regressionPart1.html#correlation-matrix",
    "title": "Correlation and Regression Models",
    "section": "Correlation matrix",
    "text": "Correlation matrix\nSalkind & Shaw (2020):\n\nYou will find a correlation matrix in publications.\nIt is the best way to represent several correlations between different pairs of variables.\n\n\n\nYou will notice that a a correlation matrix has 1.00 on the diagonal and two “triangles” with the same information."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#coefficient-of-determination",
    "href": "lecture8/correlation&regressionPart1.html#coefficient-of-determination",
    "title": "Correlation and Regression Models",
    "section": "Coefficient of Determination",
    "text": "Coefficient of Determination\n\nThere is a useful trick, you could square your \\(r\\) and get a measure of correlation in terms of percentage of shared variance:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#coefficient-of-determination-cont.",
    "href": "lecture8/correlation&regressionPart1.html#coefficient-of-determination-cont.",
    "title": "Correlation and Regression Models",
    "section": "Coefficient of Determination (cont.)",
    "text": "Coefficient of Determination (cont.)\n\nWhat is the coefficient of determination in this case?\nWe just need to estimate $ r^2= -0.22^2 = -0.05\\(. Attention and depression shared only 5\\)%$ of the variability (variance)."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation",
    "text": "Scatter plots and direction of correlation\n\nI have shown you several plots, these plots are called scatter plots.\nThese plots are useful to explore visually possible correlations.\nWhen you create this plots, you only need to represent one of the variables in the x-axis and the other variable will be represented in the y-axis.\n\n\n\n\n\n\n\nNote\n\n\n\nCan you guess if the next scatter plot corresponds to a positive correlation?"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-1",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)\n\nWe can check some values and see what is happening, like case #78, in the next plot:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-2",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-2",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)\n\nMaybe if we add the line of best fit we will see it better:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-3",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-3",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)\n\nCan you spot the direction of this correlation?\nThis data come from a questionnaire that asks to rate how emotional you feel. For instance, it asks: Rate how GREAT you feel where 1 = “not feeling” to 6=“I strongly feel it”."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-4",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-4",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)\n\nLet’s add again the line of best linear fit:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-5",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-5",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-6",
    "href": "lecture8/correlation&regressionPart1.html#scatter-plots-and-direction-of-correlation-cont.-6",
    "title": "Correlation and Regression Models",
    "section": "Scatter plots and direction of correlation (cont.)",
    "text": "Scatter plots and direction of correlation (cont.)\n\nLet’s add the line of linear fit:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#important-remarks",
    "href": "lecture8/correlation&regressionPart1.html#important-remarks",
    "title": "Correlation and Regression Models",
    "section": "Important remarks",
    "text": "Important remarks\n\nWhen the correlation is high, it means there is a large portion of shared variance between \\(x\\) and \\(y\\).\nWhen the correlation is high all the values will converge towards the line of best linear fit.\nWhen the correlation is low, the values will be sparse and far from the line of best fit.\nA flat linear line means that there is not correlation between \\(x\\) or \\(y\\) or the correlation is remarkably low. This means \\(r=0\\) or closer to zero."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#section",
    "href": "lecture8/correlation&regressionPart1.html#section",
    "title": "Correlation and Regression Models",
    "section": "",
    "text": "In R you can estimate Pearson correlations using the function cor() as showed here:\n\n\n### pos is the name of the object representing my data set\ncor(pos$down, pos$great)\n\n[1] -0.359944\n\n\nIn this estimation, I’m calculating the correlation between the emotion DOWN and the emotion GREAT. The Pearson correlation was \\(r= -0.36\\). Is this a strong correlation?\n\nWe could follow an ugly rule of thumb, but be careful, these are not rules cast in stone (Salkind & Shaw, 2020):"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#jamovi",
    "href": "lecture8/correlation&regressionPart1.html#jamovi",
    "title": "Correlation and Regression Models",
    "section": "JAMOVI",
    "text": "JAMOVI"
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#jamovi-1",
    "href": "lecture8/correlation&regressionPart1.html#jamovi-1",
    "title": "Correlation and Regression Models",
    "section": "JAMOVI",
    "text": "JAMOVI"
  },
  {
    "objectID": "assignment4/assigment4.html",
    "href": "assignment4/assigment4.html",
    "title": "Assignment #4",
    "section": "",
    "text": "This assignment has multiple aims:\n\nTo assign tasks in a lab format where you are able to use the statistical models studied in class.\nTo show the type of questions you will encounter in the upcoming exam.\n\nYou will perform some calculations using R language, you may also use the software JAMOVI to solve several questions in this assignment.\nPlease, remember to submit your answers in a Word document where you write the question along with the answer as I show in the following example:\n\n1) What is a parameter?\n\n\nAnswer: A parameter is an unknown value in a model, but it will be estimated using the data\n\nIn this assignment you may need to add your R code to your document. Please, keep a tidy style when you answer the question, after you answer the question add the code in a text box. Please watch this video where I explain how to report your results when using R."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#todays-aims",
    "href": "lecture8/correlation&regressionPart1.html#todays-aims",
    "title": "Correlation and Regression Models",
    "section": "Today’s aims",
    "text": "Today’s aims\n\nTo introduce correlation to estimate relationships between two variables.\nTo introduce the notion of covariance.\nTo study scatter plots to visualize correlations."
  },
  {
    "objectID": "lecture8/correlation&regressionPart1.html#references",
    "href": "lecture8/correlation&regressionPart1.html#references",
    "title": "Correlation and Regression Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nSalkind, N. J., & Shaw, L. A. (2020). Statistics for people who (think they) hate statistics: Using r. Sage publications."
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#todays-aims",
    "href": "lecture7/hypothesisTesting.html#todays-aims",
    "title": "Introduction to Hypothesis Testing",
    "section": "Today’s aims",
    "text": "Today’s aims\n\nWe will study the concept of p-value a.k.a significance test.\nI will introduce the concept of hypothesis testing.\nI will also introduce the mean comparison for independent groups.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "lecture7/hypothesisTesting.html#references",
    "href": "lecture7/hypothesisTesting.html#references",
    "title": "Introduction to Hypothesis Testing",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nCohen, J. (2013). Statistical Power Analysis for the Behavioral Sciences. Academic Press.\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html",
    "href": "lecture8/correlation&regressionPart2.html",
    "title": "Correlation and Regression Models",
    "section": "",
    "text": "To introduce basic notions of the regression model\nTo start thinking about statistical models with more variables (exciting times!)\nTo encourage you to think about more realistic research questions (this might take several sessions!)\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(ISLR)\nlibrary(DT)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model",
    "href": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model",
    "title": "Correlation and Regression Models",
    "section": "What is a regression model ?",
    "text": "What is a regression model ?\n\nYou might be thinking, is regression something related that happened in the past?\nIs related to study something that might happen in the future?\nRegression is a weird word!"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model-cont.",
    "href": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model-cont.",
    "title": "Correlation and Regression Models",
    "section": "What is a regression model ? (cont.)",
    "text": "What is a regression model ? (cont.)\nIn this lecture we will follow ideas from Westfall & Arias (2020):\n\nRegression models are used to relate a variable \\(Y\\) with a single variable \\(X\\) or multiple variables \\(X_{1}, X_{2}, ..., X_{k}\\)\nSome questions that you might answer with a regression model:\nHow does a person’s choice of toothpaste (\\(Y\\)) relate to the person’s age (\\(X_{1}\\)) and income (\\(X_{2}\\))?\nHow does a person’s cancer remission status (\\(Y\\)) relate to their chemotherapy regimen (\\(X\\))?\nHow does a person’s self-esteem (\\(Y\\)) relates to time in social media (\\(X_{1}\\)) and number of followers (\\(X_{2}\\))"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model-cont.-1",
    "href": "lecture8/correlation&regressionPart2.html#what-is-a-regression-model-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "What is a regression model ? (cont.)",
    "text": "What is a regression model ? (cont.)\n\nA regression model can help you to predict what an unknown \\(Y\\) will be for a given fixed value of \\(X\\). Prediction is more like a “what-if” analysis. It is a prediction about the future but also , you can answer what might have happened in the past.\nA regression model can be represented as:\n\n\\[\\begin{equation}\nY|X = x \\sim p(y|x)\n\\end{equation}\\]\n\nThis means the expression \\(p(y|X=x)\\) is the distribution of potentially observable \\(Y\\) values when \\(X = x\\), and is called the conditional distribution of \\(Y\\), given that \\(X = x\\).\nIn simple words, in regression we are estimating a conditional distribution. I know that we are going fast in this class, but the examples and the exercises in this class my help later.\nWe can also represent the regression model as \\(p(y|x)\\). It is shorter!"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#we-can-talk-more-about-conditional-distributions",
    "href": "lecture8/correlation&regressionPart2.html#we-can-talk-more-about-conditional-distributions",
    "title": "Correlation and Regression Models",
    "section": "We can talk more about conditional distributions",
    "text": "We can talk more about conditional distributions\n\nLet’s imagine you need to predict the development of episodic memory in humans. You will need to use age of the individual to predict their memory score. Let’s imagine you want to predict the probability of the memory score (\\(X\\)) when age = 27 years old. But also, you need predict the memory score when individuals are 5 years old.\nThe model \\(p(y|x)\\) does not assume any distribution, it doesn’t assume whether the distribution is discrete or continuous. But we will use a Gaussian process to represent the conditional statements \\(p(y|X=27)\\) and \\(p(y|X = 5)\\)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#we-can-talk-more-about-conditional-distributions-cont.",
    "href": "lecture8/correlation&regressionPart2.html#we-can-talk-more-about-conditional-distributions-cont.",
    "title": "Correlation and Regression Models",
    "section": "We can talk more about conditional distributions (cont.)",
    "text": "We can talk more about conditional distributions (cont.)\n\nThe model only assumes that there is a distribution of possible values in Nature when age = 27 and age = 5"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#but-soon-we-will-assume-a-distribution",
    "href": "lecture8/correlation&regressionPart2.html#but-soon-we-will-assume-a-distribution",
    "title": "Correlation and Regression Models",
    "section": "But soon we will assume a distribution",
    "text": "But soon we will assume a distribution\nWestfall & Arias (2020):\n\nFamilies of conditional distributions\n\n\nWhen p(y|x) are Assumed to be:\nThen you have…\n\n\n\n\nPoisson\nPoisson regression\n\n\nNegative binomial\nNegative binomial regression\n\n\nBernoulli\nLogistic regression\n\n\nNormal\nClassical regression\n\n\nMultinomial\nMultinomial logistic regression\n\n\nBeta\nBeta regression\n\n\nLognormal\nLognormal regression\n\n\n\n\nThis is not a final list of possible regression models, but at least the most popular ones.\nWe won’t have time to study all of them, in the best scenario we will study three different regression models: Poisson regression, Logistic regression, and Classical regression."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#important-to-remember",
    "href": "lecture8/correlation&regressionPart2.html#important-to-remember",
    "title": "Correlation and Regression Models",
    "section": "Important to remember!",
    "text": "Important to remember!\nWestfall & Arias (2020):\n\n\n\n\n\n\nImportant\n\n\nThe regression model \\(p(y|x)\\) does not come from the data. Rather, the regression model \\(p(y|x)\\) is assumed to produce the data.\n\n\n\n\nModels produce data, data DOES NOT produce models!"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#variable-x-and-variable-y-beasts-of-many-names",
    "href": "lecture8/correlation&regressionPart2.html#variable-x-and-variable-y-beasts-of-many-names",
    "title": "Correlation and Regression Models",
    "section": "Variable \\(X\\) and Variable \\(Y\\): Beasts of many names",
    "text": "Variable \\(X\\) and Variable \\(Y\\): Beasts of many names\n\nIn regression you’ll find several names for \\(X\\):\n\npredictor variable(s)\ndescriptor variable(s)\nfeature variable(s)\nindependent variable(s)\nexogenous variable(s)\nresponse/predictor\n\nAlso several names for \\(Y\\):\n\nresponse variable\ntarget variable\ncriterion variable\ndependent variable\nendogenous variable"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#variable-x-and-variable-y-beasts-of-many-names-1",
    "href": "lecture8/correlation&regressionPart2.html#variable-x-and-variable-y-beasts-of-many-names-1",
    "title": "Correlation and Regression Models",
    "section": "Variable \\(X\\) and Variable \\(Y\\): Beasts of many names",
    "text": "Variable \\(X\\) and Variable \\(Y\\): Beasts of many names\n\nThe DATA in regression models looks like this:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#example-time",
    "href": "lecture8/correlation&regressionPart2.html#example-time",
    "title": "Correlation and Regression Models",
    "section": "Example Time",
    "text": "Example Time\n\nLet’s do preliminary model:\n\nThis is the data named \"Wage\" it comes from the package ISLR. It has information of variables related to wages for a group of males from the Atlantic region of the United States.\n\n\n\nlibrary(ISLR)\nlibrary(DT)\n\ndatatable(Wage)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#example-time-cont.",
    "href": "lecture8/correlation&regressionPart2.html#example-time-cont.",
    "title": "Correlation and Regression Models",
    "section": "Example Time (cont.)",
    "text": "Example Time (cont.)\nLet’s fit a regression model were age will be a predictor of wage. This means \\(X = age\\) and \\(Y= wage\\) :\n\nfitModel <- lm(wage ~ age, data = Wage)\nsummary(fitModel)\n\n\nCall:\nlm(formula = wage ~ age, data = Wage)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-100.265  -25.115   -6.063   16.601  205.748 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 81.70474    2.84624   28.71   <2e-16 ***\nage          0.70728    0.06475   10.92   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.93 on 2998 degrees of freedom\nMultiple R-squared:  0.03827,   Adjusted R-squared:  0.03795 \nF-statistic: 119.3 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\n\nThe estimate column shows two important rows, the first one is the intercept this is the starting point of a line (more about it later). The second row is an estimate for age this is called the slope. It tells you the rate of change.\nIn this case it is telling us that, for each year of age, these males earned \\(0.70728*1000=\\$707.28\\) more. We’ll talk about it later."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#example-time-cont.-1",
    "href": "lecture8/correlation&regressionPart2.html#example-time-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "Example Time (cont.)",
    "text": "Example Time (cont.)\n\nggplot(data = Wage, aes(x=age, y=wage)) +\n  geom_point()+ geom_smooth(method = \"lm\")+\n  theme_classic()\n\n`geom_smooth()` using formula 'y ~ x'"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#my-aims-in-this-lecture",
    "href": "lecture8/correlation&regressionPart2.html#my-aims-in-this-lecture",
    "title": "Correlation and Regression Models",
    "section": "My aims in this lecture",
    "text": "My aims in this lecture\n\nTo introduce basic notions of the regression model\nTo start thinking about statistical models with more variables (exciting times!)\nTo encourage you to think about more realistic research questions (this might take several sessions!)\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(gghighlight)\n\nLoading required package: ggplot2\n\nlibrary(ISLR)\nlibrary(DT)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#references",
    "href": "lecture8/correlation&regressionPart2.html#references",
    "title": "Correlation and Regression Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using r. Sage publications.\n\n\nWestfall, P. H., & Arias, A. L. (2020). Understanding regression analysis: A conditional distribution approach. Chapman; Hall/CRC."
  },
  {
    "objectID": "assignment4/assigment4.html#description",
    "href": "assignment4/assigment4.html#description",
    "title": "Assignment #4",
    "section": "Description",
    "text": "Description\nThis assignment has multiple aims:\n\nTo assign tasks in a lab format where you are able to use the statistical models studied in class.\nTo show the type of questions you will encounter in the upcoming exam.\n\nYou will perform some calculations using R language, you may also use the software JAMOVI to solve several questions in this assignment.\nPlease, remember to submit your answers in a Word document where you write the question along with the answer as I show in the following example:\n\n1) What is a parameter?\n\n\nAnswer: A parameter is an unknown value in a model, but it will be estimated using the data\n\nIn this assignment you may need to add your R code to your document. Please, keep a tidy style when you answer the question, after you answer the question add the code in a text box. Please watch this video where I explain how to report your results when using R."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Esteban Montenegro",
    "section": "Education",
    "text": "Education\nTexas Tech University| Lubbock, TX| PhD in Educational Psychology | May 2020\nUniversity of Costa Rica | San Jose, Costa Rica| B.S. in Psychology | March 2012"
  },
  {
    "objectID": "lecture2/Sampling.html#why-do-we-care-about-sampling",
    "href": "lecture2/Sampling.html#why-do-we-care-about-sampling",
    "title": "Sampling designs",
    "section": "Why do we care about sampling ?",
    "text": "Why do we care about sampling ?\n\nLet’s imagine you want to evaluate the levels of burnout in all companies in USA. Is it feasible?\nIf not, what should we do? How do we get enough information to evaluate burnout nationally?"
  },
  {
    "objectID": "lecture2/Sampling.html#references",
    "href": "lecture2/Sampling.html#references",
    "title": "Sampling designs",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nLohr, S. L. (2021). Sampling: Design and analysis. Chapman; Hall/CRC."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#reality-nature-science-and-models",
    "href": "lecture3/introProbAndStats.html#reality-nature-science-and-models",
    "title": "Introduction to probability and statistics",
    "section": "Reality, Nature, Science, and Models",
    "text": "Reality, Nature, Science, and Models\n\nIt might seem trivial to talk about nature and how science related to nature. However, we will study nature when we study statistics.\nAs Westfall & Henning (2013) mentioned in their book: “Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation—that is, it encompasses those things that we can agree we are observing” (p.1)\nAs psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study constructs, I will talk about constructs frequently.\nStatistics is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or Nature."
  },
  {
    "objectID": "lecture3/introProbAndStats.html#references",
    "href": "lecture3/introProbAndStats.html#references",
    "title": "Introduction to probability and statistics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "lecture4/Distributions.html#previous-class",
    "href": "lecture4/Distributions.html#previous-class",
    "title": "Probability distributions and random variables",
    "section": "Previous class",
    "text": "Previous class\n\nIn our last session we talked about DATA (uppercase) and data (lowercase).\nWe studied that DATA means all the possible values in Nature for instance, all the college students in US versus a single fix observation called data (lowercase)\nIn this session is important to remember this convention.\nDATA means random variable whereas data means fixed quantities.\nFor instance, in a dice you have 6 possible values, when roll a dice ; let’s imagine you get 5 , then your data is y = 5 while you DATA is any possible value from 1 to 6.\nRandom variables have a distribution, this means DATA is random."
  },
  {
    "objectID": "lecture4/Distributions.html#references",
    "href": "lecture4/Distributions.html#references",
    "title": "Probability distributions and random variables",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model",
    "text": "Classical Regression Model\n\nThe model that we just ran is named Classical Regression Model, many people call it linear regression, other people call it Gaussian regression.\nI prefer to call it Classical Regression Model (Westfall & Arias, 2020).\nThis is how the model looks like in many books:\n\n\\[\\begin{equation}\nY = \\beta_{0}+ \\beta_{1}X + \\epsilon\n\\end{equation}\\]\n\nWe will talk about this model very often, but before going straight to bussiness we need to talk about assumptions."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions",
    "text": "Classical Regression Model: Assumptions\n\nRemember: Models produce data! Then, models are explanations of what happens in Nature.\nThe first assumption is randomness, many books only assume this concept but it doesn’t hurt to make it explicit.\nRandomness in this context means that:\n\n\n\n\n\n\n\nNote\n\n\nThe value of \\(Y\\) is variable, coming randomly from a distribution \\(p(y|x)\\) of potentially observable \\(Y\\) values that is specific to the particular values (\\(X = x\\)) of the \\(X\\) variables (Westfall & Arias, 2020).\n\n\n\n\nIn simple words, we assume \\(Y\\) is variable produced randomly by a conditional distribution \\(p(y|X)\\).\nBut of course, we test if there is a dependency between \\(y\\) and \\(x\\). That’s part of the job."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions (cont.)",
    "text": "Classical Regression Model: Assumptions (cont.)\n\nRemember when I showed you that \\(p(y|x)\\) is assuming a distribution for each \\(x\\)?\nThere is a conditional mean function in the Classical Regression Model, the expression looks like this:\n\n\\[\\begin{equation}\nf(x) = E(Y|X=x)\n\\end{equation}\\]\nIn this formula \\(E\\) means expectation or expected value. In this case the expected value is the collection of means of the conditional distributions."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-1",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions (cont.)",
    "text": "Classical Regression Model: Assumptions (cont.)\n\nThere is an assumption of constant variance (homoscedasticity).\nGiven that we are assuming there are several distributions, each \\(x\\) has a distribution, we also assume that the variance in this conditional distributions is the same or constant.\n\n\n\n\n\n\n\nNote\n\n\nThe variances of the conditional distributions \\(p(y|x)\\) are constant (i.e., they are all the same number, \\(\\sigma^2\\)) for all specific values \\(X = x\\) (Westfall & Arias, 2020)."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-2",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-2",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions (cont.)",
    "text": "Classical Regression Model: Assumptions (cont.)\n\nThere an important assumption which is normality.\nThe Classical Regression Model or linear model, assumes all the conditional distributions are produced by a normal distribution or Gaussian distribution.\n\n\n\n\n\n\n\nNote\n\n\nThe conditional probability distribution functions \\(p(y|x)\\) are normal distributions (as opposed to Bernoulli, Poisson, or other) for every \\(X = x\\).(Westfall & Arias, 2020)."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-3",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-3",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions (cont.)",
    "text": "Classical Regression Model: Assumptions (cont.)\nWestfall & Arias (2020):"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-4",
    "href": "lecture8/correlation&regressionPart2.html#classical-regression-model-assumptions-cont.-4",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: Assumptions (cont.)",
    "text": "Classical Regression Model: Assumptions (cont.)\n\n\n\nThere is also an assumption of linearity.\nThis assumption states that the means of the conditional distributions \\(p(y|x)\\) fall precisely on a straight line of the form \\(\\beta_{0} + \\beta_{1}x\\)\nWhen we assume linearity the parameter \\(\\beta_{O}\\) is the mean of \\(Y\\) when the parameter \\(\\beta_{1} = 0\\).\nTo make \\(\\beta_{1} = 0\\) we would just need to multiply it by zero: \\(\\beta_{0} + \\beta_{1}(0)\\).\nBut, what happens if \\(x\\) does not have zero? Well, in that case, \\(\\beta_{0}\\) would not be exactly the mean of \\(Y\\) , instead \\(\\beta_{0}\\) would be the vertical height or distance from zero.\nIn this case, \\(\\beta_{1}\\) is a parameter that indicates the difference between conditional means.\n\n\nWestfall & Arias (2020):"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#ordinary-least-squares",
    "href": "lecture8/correlation&regressionPart2.html#ordinary-least-squares",
    "title": "Correlation and Regression Models",
    "section": "Ordinary Least Squares",
    "text": "Ordinary Least Squares\n\nI have mentioned before in class, the existence of a line of best fit\nThis is the time to study what is this line.\nOne fo the estimation methods in the Classical Regression Model is Ordinary Least Squares estimation.\nIn this method, we draw a line in every possible direction, and calculate the distance of each value from the line. After calculating the distance form the line, we square those distances and add them together. This is what we call a Sum of Squares. The line that produces the lowest Sum of Squares is the line of best fit.\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Field et al. (2012) and Westfall & Arias (2020)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#wage-data-example",
    "href": "lecture8/correlation&regressionPart2.html#wage-data-example",
    "title": "Correlation and Regression Models",
    "section": "Wage data example",
    "text": "Wage data example\n\nBy this point we should talk more about our applied example.\nLet’s see what happens when we assume \\(\\beta_{1} = 0\\) by just removing age from the regression model.\n\n\nfitModel <- lm(wage ~ 1, data = Wage)\nsummary(fitModel)\n\n\nCall:\nlm(formula = wage ~ 1, data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-91.618 -26.320  -6.782  16.977 206.639 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 111.7036     0.7619   146.6   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.73 on 2999 degrees of freedom\n\n\n\nThis is what we call the intercept only model, it can also be considered a null model. Remember those words? In this model, we are not modeling any relationship. The regression model returns exactly the mean of wage. If you don’t believe me you can see it:\n\n\nmean(Wage$wage)\n\n[1] 111.7036\n\n\n\nIt is the same value as the intercept!\nLet’s add again age as a predictor:\n\n\nfitModel <- lm(wage ~ age, data = Wage)\nsummary(fitModel)\n\n\nCall:\nlm(formula = wage ~ age, data = Wage)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-100.265  -25.115   -6.063   16.601  205.748 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 81.70474    2.84624   28.71   <2e-16 ***\nage          0.70728    0.06475   10.92   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.93 on 2998 degrees of freedom\nMultiple R-squared:  0.03827,   Adjusted R-squared:  0.03795 \nF-statistic: 119.3 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\n\nNow the coefficient for the estimated intercept changed. This happened because now the intercept represents the average wage when age is zero. But wait, you cannot have a zero age and get a salary? In this case, the intercept misses the interpretation, then it represents the height from zero."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#wage-data-example-1",
    "href": "lecture8/correlation&regressionPart2.html#wage-data-example-1",
    "title": "Correlation and Regression Models",
    "section": "Wage data example",
    "text": "Wage data example\n\n\nShow the code\nplot(Wage$age, Wage$wage, \n     pch = 16, \n     cex = 0.7, \n     col = \"blue\",\n     xlab = \"Age\",\n     ylab = \"Wage\",\n     main = \"Line of Best Fit Linear Regression\")\nabline(lm(wage ~ age, data = Wage), col = \"red\")"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#wage-data-example-2",
    "href": "lecture8/correlation&regressionPart2.html#wage-data-example-2",
    "title": "Correlation and Regression Models",
    "section": "Wage data example",
    "text": "Wage data example\n\nIf we want to interpret the intercept in the way the linear model specifies. We can transform age to get a meaningful zero.\nThere is something call \\(z\\)-values, others call it \\(z\\)-scores. This tranformation is based on the normal distribution.\nLet’s see the formula to transform any continuous distribution into \\(z\\) values:\n\n\\[\\begin{equation}\nz= \\frac{x_{i}-\\bar{x}}{\\sigma}\n\\end{equation}\\]\nWhere: - \\(x_{i}\\) is each observed value. - \\(\\bar{x}\\) is the mean of the observed values. - \\(\\sigma\\) is the standard deviation of the observed values.\n\nAs you can see, what we do is to tranform the distribution, now the observed values will be centered at zero. It means the mean is going to be zero, and the values will be negative and positive values.\nIn R you can transform any continiuos distributin into \\(z\\)-values using the function scale().\n\n\nWage$age_Z <- scale(Wage$age)\nsummary(Wage[, c(\"age\", \"age_Z\")])\n\n      age             age_Z.V1      \n Min.   :18.00   Min.   :-2.115215  \n 1st Qu.:33.75   1st Qu.:-0.750681  \n Median :42.00   Median :-0.035925  \n Mean   :42.41   Mean   : 0.000000  \n 3rd Qu.:51.00   3rd Qu.: 0.743808  \n Max.   :80.00   Max.   : 3.256282  \n\n\n\nIn the example above, you can see that the mean of age is now zero after transforming the distribution. Also, notice that age_Z ranges from -2.12 to 3.26.\nGiven that now we have \\(z\\)-values we can interpret the values in terms of standard deviations from the mean. For instance, we can say that 18 years old is a value located -2.12 standard deviation below the mean (because the value is negative), we can also say that 80 years old is a value located 3.26 standard deviations above the mean (because the value is positive)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#wage-data-example-3",
    "href": "lecture8/correlation&regressionPart2.html#wage-data-example-3",
    "title": "Correlation and Regression Models",
    "section": "Wage data example",
    "text": "Wage data example\nLet’s add age_Z variable to our regression model:\n\n\nShow the code\nfitModel <- lm(wage ~ age_Z, data = Wage)\nsummary(fitModel)\n\n\n\nCall:\nlm(formula = wage ~ age_Z, data = Wage)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-100.265  -25.115   -6.063   16.601  205.748 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 111.7036     0.7473  149.48   <2e-16 ***\nage_Z         8.1637     0.7474   10.92   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.93 on 2998 degrees of freedom\nMultiple R-squared:  0.03827,   Adjusted R-squared:  0.03795 \nF-statistic: 119.3 on 1 and 2998 DF,  p-value: < 2.2e-16\n\n\n\nDo you see what happened? After transforming age into values with a real zero, the intercept now shows the mean of wage. If you don’t believe it, let’s estimate again the mean of wage:\n\n\nmean(Wage$wage)\n\n[1] 111.7036\n\n\n\nIt is the same number! Then it is true, the intercept is the mean of \\(Y\\) when \\(\\beta_{1}\\) has a real zero among the possible values.\nThen we can interpret the result as: for 1-unit increment in age_Z, wage increases $816.37. Also you can say: for 1-standard deviation increment, wage increases $816.37."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#wage-data-example-4",
    "href": "lecture8/correlation&regressionPart2.html#wage-data-example-4",
    "title": "Correlation and Regression Models",
    "section": "Wage data example",
    "text": "Wage data example\n\nWe can also transform wage into \\(z\\)-values, this will make the interpretation fully standardized:\n\n\n\nShow the code\noptions(scipen = 999)\n\nWage$wage_Z <- scale(Wage$wage)\n\nsummary(lm(wage_Z ~ age_Z, data = Wage))\n\n\n\nCall:\nlm(formula = wage_Z ~ age_Z, data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4028 -0.6019 -0.1453  0.3978  4.9306 \n\nCoefficients:\n                         Estimate            Std. Error t value\n(Intercept) 0.0000000000000001415 0.0179076042933873671    0.00\nage_Z       0.1956372015635888528 0.0179105896404604427   10.92\n                       Pr(>|t|)    \n(Intercept)                   1    \nage_Z       <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9808 on 2998 degrees of freedom\nMultiple R-squared:  0.03827,   Adjusted R-squared:  0.03795 \nF-statistic: 119.3 on 1 and 2998 DF,  p-value: < 0.00000000000000022\n\n\n\nAfter standardizing wage we have an intercept that is almost zero, this makes sense because the mean of wage is now zero. It is actually zero, but models are not perfect, they are approximations, and there is randomness, that’s why the intercept is not exactly zero.\nIn this model, we could interpret the estimate of age_Z like it was a correlation, because it is standardized. We can also say: for 1-standard deviation increase in age, wage increases 0.20 standard deviations."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension",
    "href": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension",
    "title": "Correlation and Regression Models",
    "section": "Fun fact: \\(t\\)-test is a linear regression extension",
    "text": "Fun fact: \\(t\\)-test is a linear regression extension\n\nUp to this point I have talked only about the case where the predictor has a continuous distribution. But, what happens when our predictor has discrete values (e.g gender, academic level)?\nThe Classical Regression Models does not assume anything about the predictors. We can actually use any discrete variable.\nAs always, we can understand what happens with a binary predictor by running an example:\n\n\n\nShow the code\n### I'm going to transform the values, so I can get a variable where 1 = have insurance, 0 = does not have insurance.\n\nWage$dummyHealth <- ifelse(Wage$health_ins == \"2. No\", 0, 1)  ### This is a \"if else statement\".\n\n## Let's test if having an insurance predicts differences in wage\n\nfitWage <- lm(wage ~ dummyHealth, data = Wage)\n\nsummary(fitWage)\n\n\n\nCall:\nlm(formula = wage ~ dummyHealth, data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-87.872 -25.355  -5.763  15.919 217.255 \n\nCoefficients:\n            Estimate Std. Error t value            Pr(>|t|)    \n(Intercept)   92.317      1.311   70.41 <0.0000000000000002 ***\ndummyHealth   27.922      1.573   17.75 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.7 on 2998 degrees of freedom\nMultiple R-squared:  0.09505,   Adjusted R-squared:  0.09475 \nF-statistic: 314.9 on 1 and 2998 DF,  p-value: < 0.00000000000000022\n\n\n\nLet’s digest the information from this example:\n\nThe model that we are fitting looks like this: \\(y_{wage} = \\beta_{0} + \\beta_{1}insuranceYES + \\epsilon\\).\nWhen the variable is discrete there is a reference group. Normally, the reference group is coded as \\(0\\) and the target group is coded as \\(1\\). This is tipically known as dummy coding.\n\nThe above table gave a value for the intercept. That intercept, in simple words; is the mean when \\(dummyHealth = 0\\). The slope in the table is 27.922, that number is in simple terms the mean difference in wage between groups.\nDon’t you believe it? Let’s see it next:"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension-1",
    "href": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension-1",
    "title": "Correlation and Regression Models",
    "section": "Fun fact: \\(t\\)-test is a linear regression extension",
    "text": "Fun fact: \\(t\\)-test is a linear regression extension\nFirst, let’s see the mean of each group:\n\n\nShow the code\nWage %>% select(dummyHealth, wage) %>% \n  group_by(dummyHealth) %>%\n  summarise(mean = mean(wage),\n            sd = sd(wage)) %>%\n  kbl(caption = \"Wage's mean and standard deviation by insurance status\") %>%\nkable_classic_2(\"hover\", full_width = F,bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\")) %>%\n  footnote(general = \"Not insurance = 0, Insurance = 1\")\n\n\n\n\nWage's mean and standard deviation by insurance status\n \n  \n    dummyHealth \n    mean \n    sd \n  \n \n\n  \n    0 \n    92.3167 \n    35.97190 \n  \n  \n    1 \n    120.2383 \n    41.23698 \n  \n\n\nNote: \n\n Not insurance = 0, Insurance = 1\n\n\n\n\n\n\nIn the table you can see that the mean of \\(Insurance = 0\\) is 92.32! Just like we expected!\nNow let’s compute the mean difference:\n\n\n120.2383-92.3167\n\n[1] 27.9216\n\n\n\nAgain! If you check our regression result the slope was 27.922, that’s exactly the mean difference!\nWhat is the message here? When add dummy coded variables in a Classical Regression Model, you are measuring how far is the mean of \\(group = 0\\) from \\(group = 1\\), it simple words: it is a \\(t\\)-test."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension-2",
    "href": "lecture8/correlation&regressionPart2.html#fun-fact-t-test-is-a-linear-regression-extension-2",
    "title": "Correlation and Regression Models",
    "section": "Fun fact: \\(t\\)-test is a linear regression extension",
    "text": "Fun fact: \\(t\\)-test is a linear regression extension\n-Let’s see it with a plot\n\n\nShow the code\nplot(Wage$dummyHealth, Wage$wage,\n     xlab = \"Insurance Status\",\n     ylab = \"Wage\",\n     main = \"A scatterplot of wage by insurance status\",\n     xaxt = \"n\")\n\n# X-axis\naxis(1, at = c(0, 1))\nabline(lm(wage ~ dummyHealth, data = Wage), col = \"red\")"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#what-if-my-predictor-has-multiple-categories",
    "href": "lecture8/correlation&regressionPart2.html#what-if-my-predictor-has-multiple-categories",
    "title": "Correlation and Regression Models",
    "section": "What if my predictor has multiple categories?",
    "text": "What if my predictor has multiple categories?\n\nYou might wonder, what if my predictor has multiple groups?\nWe can also transform multiple categories into several dummy coded variables, that means, the categories will become variables coded with \\(0\\) or \\(1\\).\nWe can answer the following question:\nIs race related to wage? If so, how different is the average wage compared to White Americans?\nIn the data set Wage there is a variable named race. We can check its values:\n\n\n\nShow the code\n Wage %>% count(race) %>%\n   kbl(caption = \"Number of participants in each race group\")%>%\n  kable_classic_2(\"hover\", \n                  full_width = F,\n                  bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nNumber of participants in each race group\n \n  \n    race \n    n \n  \n \n\n  \n    1. White \n    2480 \n  \n  \n    2. Black \n    293 \n  \n  \n    3. Asian \n    190 \n  \n  \n    4. Other \n    37 \n  \n\n\n\n\n\n\nAs you can see, we have 4 groups in this variable, and we need to create dummy variables.\nWe should create \\(k-1\\) new dummy variables, that means if you have 4 categories or groups, such as our case, we should have \\(4-1= 3\\) new dummy variables:\n\n\n\n\nBlack\nAsian\nOther\n\n\n\n\n0\n0\n0\n\n\n1\n1\n1\n\n\n0\n1\n1\n\n\n…\n…\n….\n\n\n\n\nNow we will create 3 new variables named black, asian, and other. Each variable will have only zeros and ones. Where \\(1 = YES\\), and \\(0 = NO\\).\n\n\n\nShow the code\nWage <- Wage %>% mutate(black = ifelse(race == \"2. Black\", 1, 0),\n                        asian = ifelse(race == \"3. Asian\", 1, 0),\n                        other = ifelse(race == \"4. Other\", 1,0))\n\ndatatable(Wage %>% select(race, black,asian,other))\n\n\n\n\n\n\n\n\nWe are ready to estimate the regression model:\n\n\n\nShow the code\nsummary(lm(wage ~ black + asian + other, data = Wage))\n\n\n\nCall:\nlm(formula = wage ~ black + asian + other, data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-92.478 -24.708  -6.251  17.283 216.741 \n\nCoefficients:\n            Estimate Std. Error t value             Pr(>|t|)    \n(Intercept) 112.5637     0.8333 135.088 < 0.0000000000000002 ***\nblack       -10.9625     2.5634  -4.276            0.0000196 ***\nasian         7.7246     3.1236   2.473              0.01345 *  \nother       -22.5903     6.8726  -3.287              0.00102 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 41.5 on 2996 degrees of freedom\nMultiple R-squared:  0.0121,    Adjusted R-squared:  0.01112 \nF-statistic: 12.24 on 3 and 2996 DF,  p-value: 0.0000000589\n\n\n\nDid you notice that white is not in the model? That happens because we don’t need it!\nIf you do \\(k-1\\) new dummy variables, you are already estimating the mean of white in the intercept.\nYou don’t believe me, right? Let’s estimate the mean of white:\n\n\n\nShow the code\nWage %>% filter(race == \"1. White\" ) %>%\n  summarise(mean = mean(wage),\n            sd = sd(wage)) %>%\n  kbl(caption = \"Mean of wage when participant is White\") %>%\n  kable_classic_2(\"hover\", \n                  full_width = F,\n                  bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nMean of wage when participant is White\n \n  \n    mean \n    sd \n  \n \n\n  \n    112.5637 \n    41.73383 \n  \n\n\n\n\n\n\nYes! I was right! The mean of white is exactly our intercept!\nEach slope estimated in our regression model is the mean difference compare to white group.\nLet’s see if it true:\n\n\n\nShow the code\nWage %>% group_by(race) %>% summarise(mean = mean(wage)) %>%\n  mutate(DifferenceVsWhite = c(0, \n                               101.60118    - 112.56367 , \n                               120.28829- 112.56367 , \n                               89.97333 -112.56367)) %>% \n  kbl(caption = \"Mean Differences Compare to White\") %>%\n    kable_classic_2(\"hover\", \n                  full_width = F,\n                  bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\n\nMean Differences Compare to White\n \n  \n    race \n    mean \n    DifferenceVsWhite \n  \n \n\n  \n    1. White \n    112.56367 \n    0.00000 \n  \n  \n    2. Black \n    101.60118 \n    -10.96249 \n  \n  \n    3. Asian \n    120.28829 \n    7.72462 \n  \n  \n    4. Other \n    89.97333 \n    -22.59034 \n  \n\n\n\n\n\n\nGreat! I was right again!"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar…",
    "text": "Multiple predictors went to a bar…\n\nIn the last example we added multiple predictors after creating dummy coded variables.\nWe estimated a model named multiple regression. Many authors prefered the word multiple to explicitly mention that there are more than one predictor.\nWhen you add more predictors, the Classical Regression Model looks like this:\n\n\\[\\begin{equation}\nY = \\beta_{0}+ \\beta_{1}X_{1} + \\beta_{2}X_{2} + ...+ \\epsilon\n\\end{equation}\\]\n\nNow, this model will account for more possible variables related to \\(Y\\)."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-1",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-1",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar…",
    "text": "Multiple predictors went to a bar…\n\n\nShow the code\nlibrary(scatterplot3d)\nrum <- read.csv(\"ruminationClean.csv\")\nfit <- lm(depreZ ~ rumZ + worryZ, data = rum)\n\nplot1 <- scatterplot3d(rum[c(\"rumZ\",\"worryZ\", \"depreZ\"  )], \n              angle = 60, \n              pch = 16,\n              color = \"steelblue\",\n              box = FALSE)\nplot1$plane3d(fit)"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-2",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-2",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar…",
    "text": "Multiple predictors went to a bar…\n\nIn the last example, I standardized the my outcome variable depression, I also standardized the predictors rumination and worry.\nI showed you that transforming the predictors and outcome unto \\(z\\)-values will help you to get stadardized estimates.\nThis is not always necessary, may software will do something like this:\n\n\\[\\begin{equation}\nr_{XY} = \\beta_{1}\\Big(\\frac{\\sigma_{X}}{\\sigma_{Y}}\\Big)\n\\end{equation}\\]\n-Let’s see if it works, I’ll run a regression with the raw data and compare versus a regression where I standardized the variables before fitting the model:\n\nsummary(lm(anx ~ worry, data = rum))\n\n\nCall:\nlm(formula = anx ~ worry, data = rum)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.202 -1.791 -0.462  1.538  9.935 \n\nCoefficients:\n            Estimate Std. Error t value          Pr(>|t|)    \n(Intercept)  0.01671    0.56609   0.030             0.976    \nworry        0.14384    0.01871   7.688 0.000000000000729 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.856 on 194 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.2296 \nF-statistic: 59.11 on 1 and 194 DF,  p-value: 0.0000000000007291\n\n\n\nNext, let’s estimate the standarized slope:\n\n\n0.14384*(sd(rum$worry, na.rm = TRUE)/sd(rum$anx, na.rm = TRUE))\n\n[1] 0.4836261\n\n\n\nLet’s run again the same regression with standardized trasformed variables:\n\n\nsummary(lm(scale(anx) ~ worryZ, data = rum))\n\n\nCall:\nlm(formula = scale(anx) ~ worryZ, data = rum)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6002 -0.5509 -0.1421  0.4731  3.0563 \n\nCoefficients:\n             Estimate Std. Error t value          Pr(>|t|)    \n(Intercept) -0.001125   0.062746  -0.018             0.986    \nworryZ       0.483627   0.062906   7.688 0.000000000000729 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8784 on 194 degrees of freedom\n  (16 observations deleted due to missingness)\nMultiple R-squared:  0.2335,    Adjusted R-squared:  0.2296 \nF-statistic: 59.11 on 1 and 194 DF,  p-value: 0.0000000000007291\n\n\n\nYes, they are the same. The software by default will use the formula\\(\\beta_{1}\\Big(\\frac{\\sigma_{X}}{\\sigma_{Y}}\\Big)\\) to standardize your regression slopes, when you analyze your data using SPSS, SAS, STATA or other statistical packages outside R."
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar: non-linear models",
    "text": "Multiple predictors went to a bar: non-linear models\n\nWe have discussed that the assumption of a straight line explaining all the conditional means is many times not realistic.\nWe can actually fit a model where we give more freedom to the line.\nHow do we do this? We just have to add a polynomial term to the equation. For instance we could fit a quadractic regression model by doing something like this:\n\n\\[\\begin{equation}\nY = \\beta_{0}+ \\beta_{1}X + \\beta_{2}X^2 + \\epsilon\n\\end{equation}\\]\n\nSimply, we square the predictor. Let’s do that with our wage data example:\n\n\nfitSq <- lm(wage ~ age + I(age^2), data = Wage)\nsummary(fitSq)\n\n\nCall:\nlm(formula = wage ~ age + I(age^2), data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-99.126 -24.309  -5.017  15.494 205.621 \n\nCoefficients:\n              Estimate Std. Error t value            Pr(>|t|)    \n(Intercept) -10.425224   8.189780  -1.273               0.203    \nage           5.294030   0.388689  13.620 <0.0000000000000002 ***\nI(age^2)     -0.053005   0.004432 -11.960 <0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 39.99 on 2997 degrees of freedom\nMultiple R-squared:  0.08209,   Adjusted R-squared:  0.08147 \nF-statistic:   134 on 2 and 2997 DF,  p-value: < 0.00000000000000022"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-1",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-1",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar: non-linear models",
    "text": "Multiple predictors went to a bar: non-linear models\n\n\nShow the code\nggplot(Wage, aes(age, wage)) +\n      geom_point() +\n      stat_smooth(method = \"lm\", \n                  formula = y ~ x + I(x^2), \n                  size = 1) + \n  theme_classic()"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-2",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-2",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar: non-linear models",
    "text": "Multiple predictors went to a bar: non-linear models\n\nWe could fit a model with a cubic term:\n\n\\[\\begin{equation}\nY = \\beta_{0}+ \\beta_{1}X + \\beta_{2}X^2 + \\beta_{2}X^3 + \\epsilon\n\\end{equation}\\]\n\n\nShow the code\nggplot(Wage, aes(age, wage)) +\n      geom_point() +\n      stat_smooth(method = \"lm\", \n                  formula = y ~ x + I(x^2) + I(x^3), \n                  size = 1) + \n  theme_classic()"
  },
  {
    "objectID": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-3",
    "href": "lecture8/correlation&regressionPart2.html#multiple-predictors-went-to-a-bar-non-linear-models-3",
    "title": "Correlation and Regression Models",
    "section": "Multiple predictors went to a bar: non-linear models",
    "text": "Multiple predictors went to a bar: non-linear models\n\nLet’s add more flexibility:\n\n\n\nShow the code\nggplot(Wage, aes(age, wage)) +\n      geom_point() +\n      stat_smooth(method = \"lm\", \n                  formula = y ~ x + I(x^2) + I(x^3) + I(x^4)+ I(x^5), \n                  size = 1) + \n  theme_classic()"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html",
    "href": "introLongitudinal/introLongitudinalSEM.html",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "",
    "text": "Introduce the concept of latent variable.\nExplain important concepts to jump into Structural Equation Modeling (SEM)\nExpose applied examples in Longitudinal SEM."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#what-is-a-latent-variable",
    "href": "introLongitudinal/introLongitudinalSEM.html#what-is-a-latent-variable",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "What is a latent variable?",
    "text": "What is a latent variable?\n\n\nBollen & Hoyle (2012):\n\nHypothetical variables.\nTraits.\nData reduction strategy?\nClassification strategy?\nA variable in regression analysis which is, in principle\nunmeasurable.\n\n\n\nHave you thought how to measure intelligence?\nWhat about the concept of “good performance”?\nWhat does good health look like ? Is it a concept?"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#lets-see-the-graphics-convention",
    "href": "introLongitudinal/introLongitudinalSEM.html#lets-see-the-graphics-convention",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Let’s see the graphics convention",
    "text": "Let’s see the graphics convention"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#what-is-a-latent-variable-1",
    "href": "introLongitudinal/introLongitudinalSEM.html#what-is-a-latent-variable-1",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "What is a latent variable?",
    "text": "What is a latent variable?"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#but-wait-what-is-statistical-model",
    "href": "introLongitudinal/introLongitudinalSEM.html#but-wait-what-is-statistical-model",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "But wait… what is statistical model?",
    "text": "But wait… what is statistical model?\n\n\n\n\n\n\n\nImportant\n\n\nModels are theoretical reductions, they intend to explain how the DATA is produced by Nature."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#what-is-the-name-of-this-type-of-models",
    "href": "introLongitudinal/introLongitudinalSEM.html#what-is-the-name-of-this-type-of-models",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "What is the name of this type of models?",
    "text": "What is the name of this type of models?\n\n\n\nLatent variables like the ones mentioned today belong to a family of models named Structural Equation Modeling (SEM) .\nThe word equation is there because we will estimate several equations at the same type.\nStructural implies a set of equations we have to solve to unveil the relationship between variables.\nModeling means we will create models based on theory.\nSEM was created to “confirm” theory. It is thought as a confirmatory approach."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#the-familys-foundation",
    "href": "introLongitudinal/introLongitudinalSEM.html#the-familys-foundation",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "The family’s foundation",
    "text": "The family’s foundation\n\nSEM is known as a “covariance-based approach” this means we will use the concept of variance:\n\n\\[\\begin{equation}\n\\sigma^2 = \\frac{\\sum(x_{i}-\\bar{x})}{n-1}\n\\end{equation}\\]\n\nAlso the concept of covariance is relevant here:\n\n\\[\\begin{equation}\ncov(X,Y) = \\frac{\\sum(x_{i}-\\bar{x})(y_{i} -\\bar{y})}{n-1}\n\\end{equation}\\]\n\nAnd why not, let’s throw correlation in here:\n\n\\[\\begin{equation}\ncor(X,Y) = \\frac{cov(X,Y)}{\\sigma x \\sigma y}\n\\end{equation}\\]"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#the-familys-foundation-1",
    "href": "introLongitudinal/introLongitudinalSEM.html#the-familys-foundation-1",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "The family’s foundation",
    "text": "The family’s foundation"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#sem-relates-to-theories-and-ideas",
    "href": "introLongitudinal/introLongitudinalSEM.html#sem-relates-to-theories-and-ideas",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "SEM relates to theories and ideas",
    "text": "SEM relates to theories and ideas\nHenseler (2020):"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#sem-relates-to-theories-and-ideas-ii",
    "href": "introLongitudinal/introLongitudinalSEM.html#sem-relates-to-theories-and-ideas-ii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "SEM relates to theories and ideas II",
    "text": "SEM relates to theories and ideas II"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#reflective-factors-formative-factors",
    "href": "introLongitudinal/introLongitudinalSEM.html#reflective-factors-formative-factors",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Reflective factors // Formative factors",
    "text": "Reflective factors // Formative factors"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#assumptions",
    "href": "introLongitudinal/introLongitudinalSEM.html#assumptions",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Assumptions",
    "text": "Assumptions\n\nDepends on how we estimate the model.\nMultivariate normal data generating process.\nLocal independence: only the latent factor explains the indicators.\nSample size should be large enough to estimate the model.\nMulticollinearity is a problem."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#lets-formally-define-the-sem-model",
    "href": "introLongitudinal/introLongitudinalSEM.html#lets-formally-define-the-sem-model",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Let’s formally define the SEM model",
    "text": "Let’s formally define the SEM model\n\\[\\begin{equation}\n\\Sigma = \\Lambda \\Psi \\Lambda' + \\Theta\n\\end{equation}\\]\nWhere:\n\n\\(\\Sigma\\) = The estimated covariance matrix.\n\\(\\Psi\\) = Matrix with covariance between latent factors.\n\\(\\Lambda\\) = Matrix with factor loadings.\n\\(\\Theta\\) = Matrix with unique observed variances.\nIn SEM we also have another model for the implied means:\n\n\\[\\begin{equation}\ny = \\tau + \\Lambda\\eta + \\epsilon\n\\end{equation}\\]\n\nDoes it look familiar to you?"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#lets-formally-define-the-sem-model-ii",
    "href": "introLongitudinal/introLongitudinalSEM.html#lets-formally-define-the-sem-model-ii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Let’s formally define the SEM model II",
    "text": "Let’s formally define the SEM model II"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#just-a-remark-about-model-fit",
    "href": "introLongitudinal/introLongitudinalSEM.html#just-a-remark-about-model-fit",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Just a remark about model fit",
    "text": "Just a remark about model fit"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#positive-and-negative-example",
    "href": "introLongitudinal/introLongitudinalSEM.html#positive-and-negative-example",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Positive and Negative example",
    "text": "Positive and Negative example"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#longitudinal-sem",
    "href": "introLongitudinal/introLongitudinalSEM.html#longitudinal-sem",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Longitudinal SEM",
    "text": "Longitudinal SEM\n\nIn Longitudinal SEM we can model changes overtime in different ways.\nIt depends on the question the researcher wants to address.\n\nLongitudinal Confirmatory Factor Analysis (LCFA)\nPanel models.\nLatent growth models.\nSpline models.\nRandom Intercept Panel Model.\nLatent Change Score Model."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-checking-assumptions",
    "href": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-checking-assumptions",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Longitudinal CFA: Checking assumptions",
    "text": "Longitudinal CFA: Checking assumptions\nLittle (2013)\n\nConfigural Invariance Model"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-ii",
    "href": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-ii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Longitudinal CFA II",
    "text": "Longitudinal CFA II\n\nWeak Invariance Model"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-iii",
    "href": "introLongitudinal/introLongitudinalSEM.html#longitudinal-cfa-iii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Longitudinal CFA III",
    "text": "Longitudinal CFA III\n\nStrong Invariance Model"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#panel-models",
    "href": "introLongitudinal/introLongitudinalSEM.html#panel-models",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Panel Models",
    "text": "Panel Models\nLittle (2013)\n\nPanel Model Example"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#random-intercept-panel-models",
    "href": "introLongitudinal/introLongitudinalSEM.html#random-intercept-panel-models",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Random Intercept Panel Models",
    "text": "Random Intercept Panel Models\nMulder & Hamaker (2020):\n\nRI Model"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#random-intercept-panel-models-ii",
    "href": "introLongitudinal/introLongitudinalSEM.html#random-intercept-panel-models-ii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Random Intercept Panel Models II",
    "text": "Random Intercept Panel Models II\nAsebedo et al. (2022)\n\nRI Model Example"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models",
    "href": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Latent Growth Models",
    "text": "Latent Growth Models\n\n\n\nLatent Growth Models are an extension of Multilevel Growth Models.\nLGM is useful to predict trajectories, and add more paths that can explain the growth, or the growth or change might be a predictor.\n\nMean structure model:\n\\[\\begin{equation}\ny_{ti} = \\lambda_{1t} \\eta_{1i} + \\lambda_{2t} \\eta_{2i}+ \\epsilon\n\\end{equation}\\]\nThe most relevant part is the latent mean:\n\\[\\begin{align}\n\\eta_{1t} &= \\alpha_{1} + \\zeta_{1t}\\\\\n\\eta_{2t} &= \\alpha_{2} + \\zeta_{2t}\\\\\n\\end{align}\\]\n\n\n\n\nLatent Growth Model"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models-ii",
    "href": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models-ii",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Latent Growth Models II",
    "text": "Latent Growth Models II\n\nIn Growth Models we’ll get a trajectory for each participant or observation. They are nested in time.\n\n\nEstimated Trajectories"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models-another-example",
    "href": "introLongitudinal/introLongitudinalSEM.html#latent-growth-models-another-example",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Latent Growth Models: Another Example",
    "text": "Latent Growth Models: Another Example\n\n\n\n\n\nMath Score Growth Model\n\n\n\n\n\n\nMath Score Growth Model Trajectory"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#why-should-we-use-sem",
    "href": "introLongitudinal/introLongitudinalSEM.html#why-should-we-use-sem",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Why should we use SEM ?",
    "text": "Why should we use SEM ?\n\nSEM has good properties:\n\nIt helps to model several hypothesis at the same time.\nIt has flexibility to estimate multigroup models.\nYou have several estimators to choose.\nThe model accounts for the error variance in your items.\nYou can model non-linearity (we didn’t talk about it).\nYou can estimate multilevel models and divide the variance (RI panel model).\nIt is fun!"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#why-should-we-use-sem-1",
    "href": "introLongitudinal/introLongitudinalSEM.html#why-should-we-use-sem-1",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Why should we use SEM ?",
    "text": "Why should we use SEM ?\n\nSEM has also some problems:\n\nRequires a fair number of observations in most cases (frequentist approach).\nPeople are afraid of using it! I don’t know why?\nThere is a chance of misfit in the model. Models with multiple parameters tend to be prone to misspecification.\nLarge models might be far from the true data generating process, at least there is more chance."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#when-should-we-use-sem",
    "href": "introLongitudinal/introLongitudinalSEM.html#when-should-we-use-sem",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "When should we use SEM?",
    "text": "When should we use SEM?\n\nWe should use SEM when the data generating process is truly latent. This applies to many variables in psychology, sociology and social sciences. But also in health sciences.\nSEM is good at treating missing data under the right assumptions.\nIn fact, the missing values can be imputed when treated as a latent variable."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#what-is-left-for-a-next-talk",
    "href": "introLongitudinal/introLongitudinalSEM.html#what-is-left-for-a-next-talk",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "What is left for a next talk ?",
    "text": "What is left for a next talk ?\n\nHow the estimation of SEM takes place.\nBayesian Inference vs. Frequentist Inference.\nMore about different estimators.\nAdvanced modeling approaches.\nMissing data modeling.\nLatent Class Analysis.\nItem Response Theory Models (IRT).\nMore on Dynamic Modeling.\n\nAnd…more memes for sure."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#thanks-for-not-falling-asleep",
    "href": "introLongitudinal/introLongitudinalSEM.html#thanks-for-not-falling-asleep",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "Thanks for not falling asleep!",
    "text": "Thanks for not falling asleep!"
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#my-aims-today",
    "href": "introLongitudinal/introLongitudinalSEM.html#my-aims-today",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "My aims today",
    "text": "My aims today\n\n\nIntroduce the concept of latent variable.\nExplain important concepts to jump into Structural Equation Modeling (SEM)\nExpose applied examples in Longitudinal SEM."
  },
  {
    "objectID": "introLongitudinal/introLongitudinalSEM.html#references",
    "href": "introLongitudinal/introLongitudinalSEM.html#references",
    "title": "A friendly Introduction to Longitudinal Analysis with Latent Variables",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nAsebedo, S. D., Quadria, T. H., Chen, Y., & Montenegro-Montenegro, E. (2022). Individual differences in personality and positive emotion for wealth creation. Personality and Individual Differences, 199, 111854.\n\n\nBollen, K. A., & Hoyle, R. H. (2012). Latent variables in structural equation modeling. In Handbook of structural equation modeling (pp. 56–67). The Guilford Press.\n\n\nHenseler, J. (2020). Composite-based structural equation modeling: Analyzing latent and emergent variables. Guilford Publications.\n\n\nLittle, T. D. (2013). Longitudinal structural equation modeling. Guilford press.\n\n\nMulder, J. D., & Hamaker, E. L. (2020). Three Extensions of the Random Intercept Cross-Lagged Panel Model. Structural Equation Modeling: A Multidisciplinary Journal, 1–11. https://doi.org/10.1080/10705511.2020.1784738"
  },
  {
    "objectID": "PSYC3000.html#test-iframe",
    "href": "PSYC3000.html#test-iframe",
    "title": "PSYC 3000 Lab",
    "section": "Test iframe",
    "text": "Test iframe"
  },
  {
    "objectID": "agingCourse.html",
    "href": "agingCourse.html",
    "title": "Adulthood & Aging",
    "section": "",
    "text": "Life span and aging\nAs a researcher I’m interested in studying developmental changes in aging adults. I focus on healthy aging, and sometimes I study dementia. I combine these topics with my passion with statistical methods.\nGiven my goals, I decided to teach the course Adulthood and Aging at CSU Stan. This section is dedicated to share my slides with any person who finds my materials useful. But, I aim primarily to create lectures for students at CSU Stan.\nYou may see the slides in the next section.\n\n\nLectures\nLecture 1: Themes in adult development\nLecture 2: “Models of Development: Nature and Nurture in Adulthood\nLecture 3: Physical changes\nLecture 4: The Study of Adult Development and Aging: Research Methods"
  },
  {
    "objectID": "Lecture1aging/lecture1.html",
    "href": "Lecture1aging/lecture1.html",
    "title": "Themes in adult development",
    "section": "",
    "text": "As you may have read in the first chapter, the authors introduced some misconceptions related to aging. My aim to guide the discussion related to being an aging adult from a biopsychosocial model."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#the-biopsychosocial-model-1",
    "href": "Lecture1aging/lecture1.html#the-biopsychosocial-model-1",
    "title": "Themes in adult development",
    "section": "The biopsychosocial model",
    "text": "The biopsychosocial model\nThe idea of the biopsychosocial model is to study human development from different fields and areas. We cannot isolate the human being an ignore the following main dimensions:\n\nBiological dimension: Physical changes and genetics.\nSociocultural dimension: social context, history, culture.\nPsychological dimension: cognition, personality, emotions."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#gerontological-perspective",
    "href": "Lecture1aging/lecture1.html#gerontological-perspective",
    "title": "Themes in adult development",
    "section": "Gerontological perspective",
    "text": "Gerontological perspective\n\nGerontology: is the scientific study of the aging process, is an interdisciplinary field. People who devote their professional lives to the study of gerontology come from many different academic and applied areas—biology, medicine, nursing, sociology, history, and even the arts and literature.\nDon’t confuse gerontology with geriatrics. The latter is a medical field."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#four-principles-of-adult-development",
    "href": "Lecture1aging/lecture1.html#four-principles-of-adult-development",
    "title": "Themes in adult development",
    "section": "Four principles of adult development",
    "text": "Four principles of adult development\n\nChanges Are Continuous Over the Life Span: Changing is a cumulative process, all the changes you experienced in the adolescence will affect your changes in adulthood. In science, we separate the human’s life span into stages to make easier to study all the changes, but we need to see the context of those changes as a continuum interconnected.\nOnly the Survivors Grow Old: The survivor principle states that the people who live to old age are the ones who managed to outlive the many threats that could have caused their deaths at earlier ages. In human development, we aim to study how people survive, we also aim to describe the characteristic of the healthy survivor. Some survivors are individuals with better physical and cognitive skills, they might have also made changes in their lifestyle to live longer. But, there are a portion of individuals who never change their habits and still they live longer. Why do you think this happens?\nIndividuality matter: As Whitbourne & Whitbourne (2020) mentioned in their book, people who are not scientist, assume that all older adults are “similar”. This is a big misconception, as we grow older, we change a lot. Our life path starts becoming very different as we grow compare to our peers. You might find a lot of similar characteristics in teenagers, but aging adults will have a larger “baggage” where life events changed their development in different ways. This is always a challenge when you try to describe and predict changes in aging adults.\n\n\n\n\n\n\n\nImportant concepts\n\n\nInterindividual differences: differences between people.\nIntraindividual differences: differences within the same individual.\n\n\n\n\n“Normal” Aging Is Different From Disease: The principle that normal aging is different from disease means that growing older doesn’t necessarily mean growing sicker."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#is-age-a-good-measure-to-define-adult",
    "href": "Lecture1aging/lecture1.html#is-age-a-good-measure-to-define-adult",
    "title": "Themes in adult development",
    "section": "Is age a good measure to define “Adult”?",
    "text": "Is age a good measure to define “Adult”?\n\nIn many cultures there are laws or traditions based on age to define who is an adult.\nFor instance, in USA an adult could be considered a person who is older than 21 years of age. But, in many countries in the world 18 years of age is the threshold. In fact, are there real differences between a person who is 18 years old compare to a 21 years old person? What marks the difference?\nLegally, and culturally, the age of consent, voting or driving a car defines this rule in several countries."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#functional-age-instead-of-chronological-age",
    "href": "Lecture1aging/lecture1.html#functional-age-instead-of-chronological-age",
    "title": "Themes in adult development",
    "section": "Functional age instead of chronological age",
    "text": "Functional age instead of chronological age\n\nIn gerontology is frequent to see research focused on functional age given that chronological can be confusing and misleading.\nThere are three measures of functional age:\n\nBiological age: cardiovascular fitness, muscle and bone strength, cellular aging.\nPsychological age: reaction time, memory, learning ability.\nSocial age: work roles, family status, position in community."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#life-is-shaped-by-many-influences",
    "href": "Lecture1aging/lecture1.html#life-is-shaped-by-many-influences",
    "title": "Themes in adult development",
    "section": "Life is shaped by many influences",
    "text": "Life is shaped by many influences\n\nPaul Baltes was a German psychologist who created a vast number of information regarding human development. According to Baltes, there are normative age-graded influences that shape our life.\nFor example, there a cultural norms that say when we should get married, when should we get our degrees, and when should we have children. If a person doesn’t follow these norms, upsetting feelings emerge as a consequence of not meeting the social expectations.\nThere are normative history-graded influences, these are events that happened to everyone or a cohort of individuals. Wars or economic trends are a good example. The best most recent example is the pandemic. The COVID-19 pandemic is a clear example of a historical influence in our development. These events will change the culture, politics, and your health if you got the virus.\nNonnormative influcences are random idiosyncratic events that occur to an individual, they might occur with no regular pattern. It could be an accident, or a surgery."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#key-social-factors",
    "href": "Lecture1aging/lecture1.html#key-social-factors",
    "title": "Themes in adult development",
    "section": "Key social factors",
    "text": "Key social factors\n\nThere are important social factors that many times are moderators of our development as adults:\n\nSex at birth and gender\nRace\nSocioeconomic status (SES)\nReligion"
  },
  {
    "objectID": "Lecture1aging/lecture1.html#the-demographic-pyramid",
    "href": "Lecture1aging/lecture1.html#the-demographic-pyramid",
    "title": "Themes in adult development",
    "section": "The demographic pyramid",
    "text": "The demographic pyramid"
  },
  {
    "objectID": "Lecture3aging/lecture3.html",
    "href": "Lecture3aging/lecture3.html",
    "title": "Physical changes",
    "section": "",
    "text": "Physical changes are perhaps the most evident changes in any human being.\nSociety also marks what changes are desirable or not.\nThere’s no doubt that aging implies many times changing habits, routines, and expectations.\nLet’s study the most important physical changes.\n\n\n\n\n\n\n\nNote\n\n\n\nI know aging is tough, but as scientist and practitioner scientists we should understand that there are ways to live a healthy adulthood and age with energy and empowered."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-skin",
    "href": "Lecture3aging/lecture3.html#changes-in-skin",
    "title": "Physical changes",
    "section": "Changes in skin",
    "text": "Changes in skin\n\nThis is the most obvious change in life. You start noticing wrinkles, you perhaps start finding small dark dots in your skin. And your skin might be dryer overtime. These are the reasons:\n\nThe skin adapts to the bone mass reduction in the skull.\nFace muscles have less elasticity.\nElastin, a molecule that is supposed to provide flexibility, becomes less able to return to its original shape after it is stretched during a person’s movements.\nExposure to sunlight increases wrinkles.\nProblems in toenails are frequent as we age."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-hair",
    "href": "Lecture3aging/lecture3.html#changes-in-hair",
    "title": "Physical changes",
    "section": "Changes in hair",
    "text": "Changes in hair\n\nIt is easy to spot when somebody has grey hair.\nThis happens because the pigmented hair falls and new grey hair starts to grow. The melanin is sparse when we get old causing growing hair less pigmented.\nAndrogenic alopecia causes hair loss."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-muscles-bmi-and-height",
    "href": "Lecture3aging/lecture3.html#changes-in-muscles-bmi-and-height",
    "title": "Physical changes",
    "section": "Changes in muscles, BMI, and height",
    "text": "Changes in muscles, BMI, and height\n\nWe get shorter when we age, this is part of the process.\nAging implies to loss muscle and strength. The process where we loss muscle is named sarcopenia.\nTendons are stiffer.\nIncrease risk of falling.\nthe Body Mass Index (BMI) decreases when you reduce your muscle mass. It is even more dangerous to increase fat while reducing muscle. This is correlated with diabetes.\nWhat should we do to slow down losing muscle? Excercises with weights, changes in diet, changes in cardiovascular activity."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#important-muscles-are-related-to-cognition",
    "href": "Lecture3aging/lecture3.html#important-muscles-are-related-to-cognition",
    "title": "Physical changes",
    "section": "Important: muscles are related to cognition!",
    "text": "Important: muscles are related to cognition!\n\nYou might be thinking, why should I care about physical changes if I’m a social scientist or behavioral scientist?\nChanges such as muscle loss are strongly related to cognition. There is string evidence of the relationship of muscle mass with improvements in cognitive skills in older adults. You may click the article showed below or click here.\n\nEffects of Resistance Training Program on Muscle Mass and Muscle Strength and the Relationship with Cognition in Older WomenThe aim of this study was to study the effects of a resistance training programme on Maximal Dynamic Strength (MDS) and muscle morphology of the upper limbs (UL) and lower limbs (LL), as well as to analyse their association with cognition, in a population of older women."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-bone-mass",
    "href": "Lecture3aging/lecture3.html#changes-in-bone-mass",
    "title": "Physical changes",
    "section": "Changes in bone mass",
    "text": "Changes in bone mass\n\nOur bone tissue has the capability to regenerate, but this ability decreases overtime.\nThere is a strong correlation between bone mass and a longer life."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-the-cardiovascular-system",
    "href": "Lecture3aging/lecture3.html#changes-in-the-cardiovascular-system",
    "title": "Physical changes",
    "section": "Changes in the cardiovascular system",
    "text": "Changes in the cardiovascular system\n\nOne of the most important systems when talking about healthy aging.\nIt is heavily impacted by life style and diet, and of course genetic factors.\nThe ratio of High-density lipoproteins (HDLs) and High-density lipoproteins (HDLs) is a good indicator of a healthy cardiovascular system. When the proportion is unbalanced between these lipoproteins, a plaque starts growing in the arteries.\nIn research we use the VO2 max and Vo2 Peak to determine how healthy is your cardiovascular fitness. This is a measure of the maximum amount of oxygen intake. VO2 max is correlated with cognition."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-the-respiratory-system-and-urinary-system",
    "href": "Lecture3aging/lecture3.html#changes-the-respiratory-system-and-urinary-system",
    "title": "Physical changes",
    "section": "Changes the respiratory system and urinary system",
    "text": "Changes the respiratory system and urinary system\n\nLungs are also affected by the muscle loss, along with less elastic lungs.\nThe urinary system also suffers some changes. As we age our bladder won’t hold urine efficiently. In males, the prostate gets larger (hypertrophy) and it might cause urinary problems.\nThere is distress in older adults when these changes start happening. Urinary changes might cause sedentarism, therefore the overall health decreases."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-the-endocrine-system",
    "href": "Lecture3aging/lecture3.html#changes-in-the-endocrine-system",
    "title": "Physical changes",
    "section": "Changes in the endocrine system",
    "text": "Changes in the endocrine system\n\nGrowth Hormone.\nCortisol: effect on hippocampus.\nThyroid Hormones: basal metabolic rate (BMR). The BMR slows overtime.\nMelatonin: Circadian rhythm.\nMenopause.\nAndropause.\nErectile dysfunction."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#changes-in-nervous-system",
    "href": "Lecture3aging/lecture3.html#changes-in-nervous-system",
    "title": "Physical changes",
    "section": "Changes in nervous system",
    "text": "Changes in nervous system\n\nNeural fallout model: the central nervous system is not able to regenerate neurons or replace neurons with new cells.\nThis was seen like an important factor to claim that aging adults are not able to learn.\nNowadays, theories related to neural plasticity are changing this idea.\nThe Compensation-Related Utilization of Neural Circuits Hypothesis (CRUNCH) model proposes that the demands of cognitively challenging tasks cause an overall excitation of brain activity in older adults leading to overall patterns of compensation not limited to one particular area.\n\n\n\n\n\n\n\nNote\n\n\nNeuroplasticity can be conceptualized as an intrinsic property of the brain that enables modification of function and structure in response to environmental demands, via the strengthening, weakening, pruning, or addition of synaptic connections, and by promoting neurogenesis (Gobbi Porto et al., 2015)."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#the-biopsychosocial-model",
    "href": "Lecture1aging/lecture1.html#the-biopsychosocial-model",
    "title": "Themes in adult development",
    "section": "The biopsychosocial model",
    "text": "The biopsychosocial model\n\nAs you may have read in the first chapter, the authors introduced some misconceptions related to aging. My aim to guide the discussion related to being an aging adult from a biopsychosocial model."
  },
  {
    "objectID": "Lecture1aging/lecture1.html#references",
    "href": "Lecture1aging/lecture1.html#references",
    "title": "Themes in adult development",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWhitbourne, S. K., & Whitbourne, S. B. (2020). Adult development and aging: Biopsychosocial perspectives. John Wiley & Sons."
  },
  {
    "objectID": "Lecture2aging/lecture2.html",
    "href": "Lecture2aging/lecture2.html",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "",
    "text": "It is likely that the most fundamental question in human development is: were we born determine to be like we are right now? Can the environment change what is already programmed in our genome?\nAs gerontologist, psychologist, sociologist, or educators, we care about the answer to these questions. You might be thinking: Come on! It is obvious that both nature and nurture are connected. But, this is not clear in reality.\nWe will discuss in this lecture some important theories that are looking to answer elemental questions. I will develop a separate lecture to address genetics."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#developmental-science",
    "href": "Lecture2aging/lecture2.html#developmental-science",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Developmental science",
    "text": "Developmental science\n\nWhitbourne & Whitbourne (2020) emphasized the distinction between using the word “psychology” and “science” along with the word “development”.\nA more integrative approach takes place when we convey a definition where all other sciences can come to the table to discuss aging and human devolopment.\nIf you Google centers such as the Alzheimer’s Disease Center at UC Davis, you will see a lot of scientist from different fields working to understand healthy aging and pathological aging. We represent only one of the fields.\nThis is why we need to study more than only behavior, genes, or social forces. As professionals doing research, and interventions with aging adults,we need to study more than we imagine."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-1",
    "href": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-1",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Models of individual-environment interactions",
    "text": "Models of individual-environment interactions\n\nIn the 20th century, psychology and other fields created theories focusing mostly on nature. They saw human development as something happening at natures pace, it was a more deterministic approach.\nJohn Watson and B.F. Skinner did not like this approach.\nThey were the foundation fathers of behaviorism.\nBehaviorist, pay a lot of attention to the context. In fact, Watson was the first to demonstrate that the environment can modify your development and learning process. After that, Skinner created a vast scientific literature. Skinner changed how we approach learning, and the changes impacted by the environment.\n\n\n\n\n\n\n\nWatson’s Experiment with Albert\n\n\nWatson conducted an influential experiment where he conditioned a child to be afraid of rabbits. You may see this video to learn more: CLICK HERE)"
  },
  {
    "objectID": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-2",
    "href": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-2",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Models of individual-environment interactions",
    "text": "Models of individual-environment interactions\n\nScientist also conduct research with identical twins (monozygotic).\nIn this studies, we want to study the impact of environment on human development.\nFor example, we can address the question: How is possible that two identical human beings develop in different directions? Under the same context (same household, hopefully).\nYou may check this wonderful twins study: CLICK HERE)"
  },
  {
    "objectID": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-3",
    "href": "Lecture2aging/lecture2.html#models-of-individual-environment-interactions-3",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Models of individual-environment interactions",
    "text": "Models of individual-environment interactions"
  },
  {
    "objectID": "Lecture2aging/lecture2.html#ecological-perspective",
    "href": "Lecture2aging/lecture2.html#ecological-perspective",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Ecological Perspective",
    "text": "Ecological Perspective\n\nThis perspective is dominated by Urie Bronfenbrenner\nBronfenbrenner was an influential psychologist who advocated to pay attention to the complexity of the environment."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#lets-jump-into-something-negative",
    "href": "Lecture2aging/lecture2.html#lets-jump-into-something-negative",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Let’s jump into something negative",
    "text": "Let’s jump into something negative\n\nAgeism: “a set of beliefs, attitudes, social institutions, and acts that denigrate individuals or groups based on their chronological age. Similar to other”isms” such as racism and sexism, ageism occurs when an individual is assumed to possess a set of stereotyped traits.” (Whitbourne & Whitbourne, 2020, p. 29)\nIn fact, Madonna recently claimed that media and general public where criticizing her look because she looks old, she is 65 years old. You may read more on the link below:\n\nMadonna hits back at ageist criticism after Grammy Awards appearanceWritten by Leah Dolan, CNN Madonna has spoken out against criticism of her appearance after presenting at the Grammy Awards Sunday night. In her latest Instagram post, the multi-award-winning singer lamented being \"caught in the glare of ageism and misogyny that permeates the world we live in,\" after a close-up photo of her face went viral online and sparked a torrent of negative comments."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#why-ageism-exist",
    "href": "Lecture2aging/lecture2.html#why-ageism-exist",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Why ageism exist?",
    "text": "Why ageism exist?\n\nSome possible explanations:\n\nterror management theory: people regard with panic and dread the thought that their lives will someday come to an end.\nmodernization hypothesis: the increasing urbanization and industrialization of Western society is what causes older adults to be devalued. They can no longer produce, so they become irrelevant and even a drain on the younger population."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#eriksons-psychosocial-theory",
    "href": "Lecture2aging/lecture2.html#eriksons-psychosocial-theory",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Erikson’s Psychosocial Theory",
    "text": "Erikson’s Psychosocial Theory\n\nErikson was a prominent psychologist. He was trained as a psychoanalyst."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#schemas-assimilation-and-acommodation",
    "href": "Lecture2aging/lecture2.html#schemas-assimilation-and-acommodation",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Schemas, assimilation, and acommodation",
    "text": "Schemas, assimilation, and acommodation\nYou may learn more about Piaget’s life by CLICKING HERE. Piaget was a French researcher, he was more interested in epistemology. His interest helped to question: how do we learn? how our cognition changes?\n\nSchemas: the mental structures we use to understand the world.\nAssimilation: it refers to the situation in which individuals change their interpretation of reality to fit the schemas they already hold.\nAccomodation: it refers to the situation in which individuals change their interpretation of reality to fit the schemas they already hold. In simple words, you are the one who changes. You change your schema.\nThe aim is to find an equilibrium between these two processes."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#nature-or-nurture",
    "href": "Lecture2aging/lecture2.html#nature-or-nurture",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "Nature or nurture",
    "text": "Nature or nurture\n\n\n\n\n\nIt is likely that the most fundamental question in human development is: were we born determine to be like we are right now? Can the environment change what is already programmed in our genome?\nAs gerontologist, psychologist, sociologist, or educators, we care about the answer to these questions. You might be thinking: Come on! It is obvious that both nature and nurture are connected. But, this is not clear in reality.\nWe will discuss in this lecture some important theories that are looking to answer elemental questions. I will develop a separate lecture to address genetics."
  },
  {
    "objectID": "Lecture2aging/lecture2.html#references",
    "href": "Lecture2aging/lecture2.html#references",
    "title": "Models of Development: Nature and Nurture in Adulthood",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nWhitbourne, S. K., & Whitbourne, S. B. (2020). Adult development and aging: Biopsychosocial perspectives. John Wiley & Sons."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#appearence",
    "href": "Lecture3aging/lecture3.html#appearence",
    "title": "Physical changes",
    "section": "Appearence",
    "text": "Appearence\n\nPhysical changes are perhaps the most evident changes in any human being.\nSociety also marks what changes are desirable or not.\nThere’s no doubt that aging implies many times changing habits, routines, and expectations.\nLet’s study the most important physical changes.\n\n\n\n\n\n\n\nNote\n\n\nI know aging is tough, but as scientist and practitioner scientists we should understand that there are ways to live a healthy adulthood and age with energy and empowered."
  },
  {
    "objectID": "Lecture3aging/lecture3.html#references",
    "href": "Lecture3aging/lecture3.html#references",
    "title": "Physical changes",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nGobbi Porto, F. H. de, Fox, A. M., Tusch, E. S., Sorond, F., Mohammed, A. H., & Daffner, K. R. (2015). In vivo evidence for neuroplasticity in older adults. Brain Research Bulletin, 114, 56–61."
  },
  {
    "objectID": "Hw1Sp2023Psyc3000/assigment1.html",
    "href": "Hw1Sp2023Psyc3000/assigment1.html",
    "title": "Assignment #1",
    "section": "",
    "text": "In this assignment you’ll have to answer questions based on the presentation Introduction to Probability and Statistics and the lecture Probability distributions and random variables. Also, you may need to check the examples provided by Westfall & Henning (2013) (Chapter 1).\nPlease submit your answers in a Word document or Libre Office document with a cover page. Copy the question and then answer the question in the following paragraph, similar to this example\n\n1) What is a parameter?\n\n\nAnswer: A parameter is an unknown value in a model, but it will be estimated using the data\n\nIn addition, please watch the next video where I explain how to add your R code to your answers CLICK HERE."
  },
  {
    "objectID": "Hw1Sp2023Psyc3000/assigment1.html#relevant-questions-related-to-nature",
    "href": "Hw1Sp2023Psyc3000/assigment1.html#relevant-questions-related-to-nature",
    "title": "Assignment #1",
    "section": "Relevant questions related to NATURE",
    "text": "Relevant questions related to NATURE\nPlease answer the following questions, if you find yourself not understanding the question please send me an email or schedule an appointment. My aim is to evaluate if I did a good job communicating the main points, in addition; I aim to evaluate if you understood the content.\n\nLet’s imagine that you need a model relating crime rate in a “typical” city to the poverty, then you came up with the next model:\n\n\\[\\begin{equation}\nCrimeRate = 60+5*Poverty\n\\end{equation}\\]\n1.1) What kind of model is this? Provide the right term from the presentation or from page 8 in Westfall & Henning (2013). (10 points)\n1.2) Is this model good to study crime? Answer this question based on the lecture or based on page 9 in Westfall & Henning (2013). (10 points)\n\nIn page 13, Westfall & Henning (2013) define a Purely Probabilistic Statistical Model as:\n\n\n\n\n\n\n\nImportant\n\n\n\nA purely probabilistic statistical model states that a variable \\(Y\\) is produced by a pdf having unknown parameters. In symbolic shorthand, the model is given as \\(Y \\sim p(y|\\theta)\\)\n\n\nBased on this definition, please write the name of three probability models we studied in class. (Hint: Check the lecture Probability distributions and random variables CLICK HERE ) (10 points)\n\nWe studied in class that models generate data, what is the distribution of the following model?, and what information is assumed in the “population model” or data generating process? (10 points)\n\n\n## Average or mean\nMean <- 50\n## Standard deviation\nSD <- 20\nN <- 2000\n\ngeneratedValues <- rnorm(n = N, mean = Mean, sd = SD )\n\nplot(density(generatedValues),\n     xlab = \"generated values\",\n     ylab = \"p(y) or likelihood\",\n     main = \"What is this distribution?\")\n\n\n\n\n\nI explained in class that simulations are good for learning complicated topics in statistics. I also mentioned that simulations help to explain the concept of DATA (uppercase) versus data (lowercase). Explain the concept of DATA and data using the example of tossing a coin (10 points). When we simulate one data set using the Bernoulli distribution, is this data set* DATA* or data? (10 points)\nR contains data sets available for practicing. You can see the list of data sets by running data(). Run the data() function, after that select one data set, then run the function summary(). What information issummary() giving you? (10 points)\n\n\n### Example: I'm running summary() with the data set named 'mtcars'. Select\n### another data set from data()\ndata()\nsummary(mtcars)\n\n\nRead the chapter The most important question: how do I import data? from the lab book. You may read it by clicking HERE.\n\nAfter reading the chapter, please set the working directory in your local computer. Then import the following data set into R: CLICK HERE . Finally estimate the mean life expectancy of year 2019. Your code should look similar to this example:\n\nsetwd(\"write/yourpath/toyour/targetfolder\")\n\nlifexp <- read.csv(\"lifeExpect.csv\")\n\nmean(lifexp$X2019)\n\nInclude your code, and the output after running the code (15 points).\n\nIn this exercise, I will introduce a new function in R. The name of this function is pipe. A pipe is a R function that helps to do several steps in one single line of code. It is like creating a chain of multiple steps.\n\nIn R, pipes are created using this sign: |>. Let’s see the following example:\n\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\n  penguins |>\n    filter(species == \"Adelie\") |>\n    select(bill_depth_mm) |>\n    with(mean(bill_depth_mm, na.rm = TRUE))\n\n[1] 18.34636\n\n\nIn the code above, I’m performing several steps on the data set named penguins, from the package palmerpenguis. You will see the function filter() from the package dplyr. The function filter() is selecting the penguins classified as “Adelie”, then I’m selecting the column “bill_depth_mm”, and finally I’m estimating the mean of “bill_depth_mm”. As you can see, you can read the pipe as “then do this”. In English you could read: filter penguins by species = “Adelie”, then select the column “bill_depth_mm”, then estimate the mean of “bill_depth_mm”.\nFollowing this explanation, select in the life expectancy data the column “X2019”, after that estimate the mean the the column “X2019”. Include your code, and the output. (15 points)\n\n\n\n\n\n\nPreviuos step\n\n\n\nTo answer this question you have to install the package dplyr, by running this code: install.packages(\"dplyr\"). To use the package you have to run first: library(dplyr). You may read this information first: https://blackhill86.github.io/StatsBook/functionsPack.html"
  },
  {
    "objectID": "Hw1Sp2023Psyc3000/assigment1.html#extra-points-5-points",
    "href": "Hw1Sp2023Psyc3000/assigment1.html#extra-points-5-points",
    "title": "Assignment #1",
    "section": "Extra points (5 points)",
    "text": "Extra points (5 points)\nCreate a good meme to express how you feel in this class, include your meme in your Word document."
  },
  {
    "objectID": "Lecture4aging/lecture4.html",
    "href": "Lecture4aging/lecture4.html",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "",
    "text": "A variable is any observation that varies, that means it is observed information that has variability. A variable can be age, memory, gender, and many more.\nThe aim is to evaluate how one variable varies as a function of another variable. For example, we can study how memory varies (changes) as a function of age."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#variables-in-developmental-research-1",
    "href": "Lecture4aging/lecture4.html#variables-in-developmental-research-1",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Variables in Developmental Research",
    "text": "Variables in Developmental Research\n\nYou may already know the concept of independent and dependent variable. If not that’s ok. I’ll refresh your memory.\nIndependent variables: are those that influence, or affect outcomes in experimental studies. They are described as “independent” because they are variables that are manipulated in an experiment and thus independent of all other influences.\nHowever, we will use this concept more vaguely, we won’t use it only when talking about experiments. It will be used also for correlational relationships, formally its name in survey designs is predictor."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#variables-in-developmental-research-2",
    "href": "Lecture4aging/lecture4.html#variables-in-developmental-research-2",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Variables in Developmental Research",
    "text": "Variables in Developmental Research\n\nDependent variables: are those that depend on the independent variables; they are the outcomes or results of the influence of the independent variables. It is also called outcome in survey designs or correlation designs."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#cause-and-effect-conslusions-in-aging-studies",
    "href": "Lecture4aging/lecture4.html#cause-and-effect-conslusions-in-aging-studies",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Cause-and-Effect conslusions in aging studies",
    "text": "Cause-and-Effect conslusions in aging studies\n\nIt is impossible to manipulate age as a experimental variable. In essence, the aging studies are quasi-experiments. Because you cannot assign participants randomly to different conditions.\nThat’s why we normally conduct longitudinal studies.\nIn longitudinal studies we can add different cohorts. A cohort is marked by the the date of birth. Let’s imagine you want evaluate the effect of age in social media use. You can add a cohort of participants that were born in 1980, and another cohort that was born in 2001. This two cohorts will help you to understand the impact of social media, or probably the opposite. The impact of age in social use. You might find that older adults look for a different type of content, you may observe differences on how frequent both cohorts use social media."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#longitudinal-designs",
    "href": "Lecture4aging/lecture4.html#longitudinal-designs",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Longitudinal Designs",
    "text": "Longitudinal Designs\nIn a study using a longitudinal design, people are followed repeatedly from one test occasion to another. By observing and studying people as they age, researchers aim to determine whether participants have changed over time as a result of the aging process.(Whitbourne & Whitbourne, 2020, p. 50)"
  },
  {
    "objectID": "Lecture4aging/lecture4.html#longitudinal-designs-1",
    "href": "Lecture4aging/lecture4.html#longitudinal-designs-1",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Longitudinal Designs",
    "text": "Longitudinal Designs\n\nNot always is possible to conduct a longitudinal design. This type of study is expensive.\nThis is why is common to conduct a cross-sectional study. A cross-sectional study does not follow the individual overtime. In this studies, we collect data only one time at a specific moment. For instance, if you go to a mall tomorrow at 6 p.m. and ask questions to possible participants, you are conducting a crosssectional study. You cannot do a follow-up of the people at the mall. You have information collected at that particular moment.\nIn aging studies, we try to collect information from different age groups to draw conclusions about the effect of age. For instance, you could perform an study where you aim to study muscular flexibility. You would recruit young people, and also not so young participants for your study."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#truly-longitudinal-designs",
    "href": "Lecture4aging/lecture4.html#truly-longitudinal-designs",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Truly Longitudinal Designs",
    "text": "Truly Longitudinal Designs\n\nSingle-cohort longitudinal design: in this studies we do a follow up of a single cohort. For instance, you could select individuals that were born in 2003, then you do a follow up to measure muscle strength the next year, and then the year after the next and so on. You follow the same individuals overtime, that’s the main point."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#truly-longitudinal-designs-1",
    "href": "Lecture4aging/lecture4.html#truly-longitudinal-designs-1",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Truly Longitudinal Designs",
    "text": "Truly Longitudinal Designs\n-Cross-sequential study: starts with a traditional cross-sectional study and then follows all participants longitudinally. In this scenario, you could recruit, for instance, teenagers to measure their social skills overtime. You could have collected the data in 2015, then you come back to measure the same group of students in 2016. But, this time you recruit a new group of teenagers. So, you now have participants that were measure in 2015, and 2016, and you have to continue with the follow up every year.\n\nIn this study, we don’t control the age of the participants. We only aim to recruit new participants every year and do follow ups every year.\nThis is a weak design because there might be cohort effects affecting the study and we don’t control those effects. For instance we might have students that were born in 2000, 1999, 2001 or any other year."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#truly-longitudinal-designs-2",
    "href": "Lecture4aging/lecture4.html#truly-longitudinal-designs-2",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Truly Longitudinal Designs",
    "text": "Truly Longitudinal Designs\nCohort-Sequential Design: The cohort-sequential design is like starting a longitudinal study at the same age over and over again. That is, each year, a new sample of participants of a certain age are selected and enrolled in a longitudinal study. Here, each new “cohort” is enrolled in a longitudinal sequence that covers the same age span. This design is particularly well suited to identifying age differences while controlling for cohort differences (Little, 2013)."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#far-from-causal-relationships-correlational-designs",
    "href": "Lecture4aging/lecture4.html#far-from-causal-relationships-correlational-designs",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Far from causal relationships: correlational designs",
    "text": "Far from causal relationships: correlational designs\n\nCorrelational designs aim to establish relationships between two variable or more.\nThis type of designs are not robust to detect a causal relationship.\nWe use statistics to unveil the relationship between variables, and how they affect each other. It is common to find statistical models such as:\n\nRegression models.\nPath analysis.\nStructural Equation Modeling.\nLatent Class Analysis."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#types-of-research-methods",
    "href": "Lecture4aging/lecture4.html#types-of-research-methods",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Types of research methods",
    "text": "Types of research methods\n\nLaboratory Studies\nQualitative Studies\nArchival Research\nSurveys\nEpidemiological Studies\nCase Reports\nFocus Groups\nDaily Diaries\nObservational Methods\nMeta-Analysis"
  },
  {
    "objectID": "Lecture4aging/lecture4.html#reliability-and-validity",
    "href": "Lecture4aging/lecture4.html#reliability-and-validity",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Reliability and validity",
    "text": "Reliability and validity\n\nA measure is reliable if it yields consistent results every time it is used.\nThe concept of validity varies depending on the intended use of the measure. Content validity provides an indication."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#variables-in-developmental-research",
    "href": "Lecture4aging/lecture4.html#variables-in-developmental-research",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "Variables in Developmental Research",
    "text": "Variables in Developmental Research\n\nA variable is any observation that varies, that means it is observed information that has variability. A variable can be age, memory, gender, and many more.\nThe aim is to evaluate how one variable varies as a function of another variable. For example, we can study how memory varies (changes) as a function of age."
  },
  {
    "objectID": "Lecture4aging/lecture4.html#references",
    "href": "Lecture4aging/lecture4.html#references",
    "title": "The Study of Adult Development and Aging: Research Methods",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nLittle, T. D. (2013). Longitudinal structural equation modeling. Guilford press.\n\n\nWhitbourne, S. K., & Whitbourne, S. B. (2020). Adult development and aging: Biopsychosocial perspectives. John Wiley & Sons."
  },
  {
    "objectID": "PSYC4310.html",
    "href": "PSYC4310.html",
    "title": "PSYC4310: Seminar in Social Psychology",
    "section": "",
    "text": "The Seminar in Social Psychology is in reality an excuse to conduct research in a wide number of topics. The main goal in simple words is to provide a space where the students can take their first steps to prepare a research proposal, and create a scientific poster.\n\nIn this course I usually try to add some R code along with stats, because why not? statistics are everywhere and they are fun!\nYou may check a sample of the syllabus: Syllabus\n\n\nLecture 1: The journey to the begining\nLecture 2: Designing your research project\n\n\n\nHow to use SONA?"
  },
  {
    "objectID": "syllabusPSYC4310.html",
    "href": "syllabusPSYC4310.html",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "",
    "text": "Important\n\n\n\nCourse time: at student’s pace.\nProfessor: Esteban Montenegro-Montenegro, PhD.\nOffice hours online: Schedule an appointment on Warrior Connect or send me an email.\nOffice phone: (209) 513-9457\nEmail: emontenegro1@csustan.edu\nSchedule an appointment: Warrior Connect"
  },
  {
    "objectID": "syllabusPSYC4310.html#course-description",
    "href": "syllabusPSYC4310.html#course-description",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Course description",
    "text": "Course description\nCourse description (from course catalog): This course examines research methods used in Social Psychology. Students design and conduct an original study. Satisfies upper-division writing proficiency requirement. (3 Units). Prerequisites: Completion of the Writing Proficiency Screening Test with a passing score and PSYC 3000.\nCourse Introduction: In this course, you will develop a better understanding of psychological research methods and their application. You will be part of a group of students and collaborate to conduct a research study from start to finish, including an APA Style poster presentation and manuscript. Your research team will review past literature, design a study, collect data, analyze the results of that study, and present a research poster to the class. You will complete an entire APA Style formatted manuscript.\nCourse Structure: This course will be asynchronous online. The students will have read, work on assignments, and execute a research project after watching videos or reading materials assigned on Canvas. *WE WON’T HAVE A CLASS SESSION EACH WEEK”. Instead, students will work on their own pace without forgetting important deadlines."
  },
  {
    "objectID": "syllabusPSYC4310.html#course-learning-objectives",
    "href": "syllabusPSYC4310.html#course-learning-objectives",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nThis course is designed to give you experience designing and administering an empirical study in social psychology, analyzing the data, drawing conclusions based on the analyses, and presenting the results to others. The course is structured around a research project that you will conduct with a small group of classmates. Upon completing this course, you should be able to:\n\nConduct a scientific literature search and review.\nUnderstand basic research methods in social psychology.\nDesign and conduct scientific experiments in psychology using appropriate methods that adhere to the APA code of ethics.\nUse technology to analyze scientific data and present research findings.\nDemonstrate APA Style writing skills, including the ability to paraphrase and correctly cite references.\nOrally communicate research methodologies and findings to others.\nCollaborate with group members to complete a research project.\n\n\n\n\n\n\n\n\nProgram learning outcomes\nCovered?\n\n\n\n\n\nDemonstrate psychological literacy\n\nX\n\n\n\nBe able to identify strengths and weakness in psychological studies\n\nX\n\n\n\nApply psychology concepts to address real-world problems\n\nX\n\n\n\nCommunicate effectively in formal and informal written and oral modes\n\nX\n\n\n\nBe able to identify the commonalities and differences between different theoretical approaches\n\n\n\n\n\nDescribe and act in accordance with the scientist-practitioner model\n\nX\n\n\n\nAct according to the ethical principles adopted by the profession\n\nX"
  },
  {
    "objectID": "syllabusPSYC4310.html#course-materials",
    "href": "syllabusPSYC4310.html#course-materials",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Course Materials:",
    "text": "Course Materials:\nIn this class, you are allowed to use any or a combination of the following statistical software to analyze your data:\n\nR is a programming language. It is an open source language, free and very famous around the world. We will learn how we can use it to analyze diverse type of statistical models. You can download the installer and read more about it from: https://www.r-project.org/\n\nRStudio is an Integrated Development Environment (IDE) it works as a friendly interface to use R language. You can read and download RStudio from here: https://www.rstudio.com/products/rstudio/download/\n\nJamovi is a friendly software based on R language. It runs R behind scenes, many users believe that Jamovi is a friendly approach to learn R. You can download JAMOVI from here: https://www.jamovi.org/\nYou may also use software such as SPSS, or Excel. It is up to you what statistical software is best according to your knowledge and skills. I will give more emphasis to R in your projects. I will facilitate tutorials and office hours to help you develop stronger skills in R if you see it feasible for your projects. However, it is not mandatory to use R to conduct data analysis.\nComputer/Laptop: Your device should be good enough to get access to internet, office and conduct basic data analysis models. Most of the modern computer will work for this class, however depending on your final projects you might need a computer or tablet with at least 8 GB of ram memory.\nMicrosoft Office programs (PowerPoint, Word, Excel): Lessons, assignments, and data sets will need to be accessed and/or completed using these programs. It is your responsibility to have access to these programs (and not online or Google Drive options) downloaded on your computer, laptop. Microsoft Office is available for download to currently enrolled Stan State students. I also accept documents in pdf created using Latex, R markdown or markdown.\nRequired Text: Publication Manual of the American Psychological Association, 7th edition This book is the official manual for writing and properly formatting your research and paper in APA Style. It describes all you’ll need to know on how to write an effective paper from beginning to end. Also, it includes, what may be a more difficult part of writing in APA Style, how to use and format citations and references. All of your manuscripts will be graded using APA writing style. Therefore, you ARE required to buy this book for this class. If you are a Psychology major, you will be using this regularly throughout the course of your academic career.\nI also suggest the book: Galvan, J. L., & Galvan, M. C. (2017). Writing literature reviews: A guide for students of the social and behavioral sciences. Routledge. We’ll be reading several chapters but not the entire book."
  },
  {
    "objectID": "syllabusPSYC4310.html#evaluation",
    "href": "syllabusPSYC4310.html#evaluation",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Evaluation",
    "text": "Evaluation\nAssignments (50%): Below is a description of the final research assignments. In addition to the assignments described below, you will complete additional individual and group tasks. Specific instructions and due dates to assignments will be announced in class and in the assignment link on Canvas. You might use Microsoft Word, Libre Office, or pdf documents created with Latex or Rmarkdown. Just make sure your document is tidy and meets all the requirements of APA style when applicable.\nResearch Project (25%): With a set of classmates (5 maximum), you will work on a research project throughout the semester. Each of you will be joining other classmates as part of a group in order to complete a research project. Together you will decide on a topic relevant to social psychology, conduct a literature review on that topic, and design and execute your own research study regarding an aspect of that topic that interests you and your group. There are several activities and assignments designed to help you work and complete this research project. Your group and you have limited time to design and execute a research project, for this reason, keep your research topic simple.\nIn order for your group to function at its ultimate capacity, you will be taking on specific leadership roles, which are described below. It is vital that you all carefully consider who you select for these roles, as their ability to perform adequately will have an impact on your group’s performance overall.\n\n\n\n\n\n\nRoles\n\n\n\n\nTeam Leader: The individual that holds this position will be responsible for managing the general group activities. This person will also be submitting the majority of the initial group assignments. This individual needs to be responsible, conscientious, and organized. When necessary, this individual will also schedule Zoom meetings for group members to meet outside of the scheduled class days and times. This person will also serve as the group’s liaison and will contact me via email outside of class for any general group inquiries. This will eliminate a potentially considerable amount of unnecessary emails between myself and other group members if there is a problem with an assignment. This person needs to have Microsoft Word installed on their personal computer or laptop.\n\nThe Team Leader is not the boss of the group or person responsible for completing the group work—ONLY the individual responsible for managing the group.\n\nIRB Coordinator: This individual will be responsible for managing the Institutional Review Board (IRB) Application Packet. As a group, you will ALL work together to complete the packet, but this individual will be responsible for ensuring that any requested revisions to the IRB application packet get done in a timely manner. This person is responsible for contacting me via email with any IRB Application Packet-related group questions. You will receive group points for this application packet and any revisions. The person that holds this position needs to have Microsoft Word installed on their personal computer or laptop.\nQualtrics Coordinator: The individual that holds this position will be responsible for managing the development of your study on Qualtrics. As a group, you will ALL work together to design the initial set-up and revisions. This person is responsible for contacting me via email with any Qualtrics-related group questions. In this class, your group will pilot test your study, the Qualtrics Coordinator will be responsible for completing the logistics of this task. You will still receive group points for the set-up, as long as your Qualtrics Coordinator has made any requested revisions.\nData Manager: The individual that holds this position will be responsible for managing the data for your study. At the end of data collection, this person is responsible for exporting the data from the survey management system, preparing the data for data analysis, and sharing the “final” data set with group members. This person is responsible for contacting me via email with any data management-related group questions. This person needs to have Microsoft Excel installed on their personal computer or laptop.\nPoster Coordinator: The individual that holds this position will be responsible for managing the group conference poster, as well as all of its revisions. As a group, you will all work together to design your poster, but the Poster Coordinator will submit assigned poster drafts and ensure that any requested revisions are made. You will receive group points for the poster and the revisions. This person is responsible for contacting me via email with any poster-related group questions. This person needs to have Microsoft PowerPoint installed on their personal computer or laptop.\n\n\n\nAPA Style Manuscript: You will write your own complete APA Style manuscript about your research project. This manuscript consists of the following elements: Title Page, Introduction, Method, Results (with at least one figure), Discussion, References, and Appendix. There are several activities and assignments designed to help you work and complete this manuscript. Relevant assignments will be graded for content, writing style, and APA Style formatting.\nAPA Style Conference Poster (15%): With your group, you will develop an APA Style conference poster. This poster will be developed using Microsoft PowerPoint and contain the following elements: Title, Authorship, Abstract, Introduction, Method, Results (with at least one figure), Discussion, and References. Other poster elements are encouraged, such as table(s), images, and acknowledgments. You may present your poster at the Undergraduate Symposium organized by the Psychology and Child Development Department.\nGroup Presentation (10%): For your final, with your group, you will give a 10-15 minute presentation of your project. In this presentation, you will provide a summary of the purpose of the study, the previous research, the hypotheses, method, results, discussion of what the results mean, limitations, and future directions for research. This presentation will also include a Q&A session to allow your classmates and me to ask questions and make comments on your research project. You may record your presentation using Zoom and then upload the video to Panopto on Canvas. I will prepare an user guide to show you how to do it.\nSONA. One point will be given for each credit earned via the sona-systems website, http://csustan.sona-systems.com/. Make sure that you sign up using a participant (not a researcher) account, and that you assign the credits to this course. FRIDAY, DECEMBER 11th is the last day to participate in studies, and December 11th is the deadline for assigning credits to this course. I will get a report of the credits assigned to this course on December 11th, which is when they will be added to your grade on Canvas\nLate work: Late assignments will be accepted up to 48 hours after the deadline. Points will be deducted as follows: one day later will deduct 15% of your grade, two days late represents 25% less in your grade, three days delayed is 0% of your grade.\nDisputing a score: Students are welcome to dispute a score if they believe they were incorrectly deducted for their work. For this, students must provide a written explanation, specifying why they were incorrectly graded. For non-final assignments or exams, this must be done within seven (7) days from the date the score was posted on Canvas. For final assignments or exams, this must be done before final course grades are submitted; your instructor will post an announcement on Canvas with this date.\nFinal Grades: Grades are based on all weighted evaluation categories (participation activities, graded assignments, and exams) total points. Letter grades will be assigned using the following percentages (rounded to the second decimal point):\n\n\n\n\n\n\n\nGrading Criteria\n\n\n\n\n\nA = 93 - 100\n\\(A^-\\)= 90-93\n\\(B^+\\) = 87-90\nB = 83-87\n\\(B^-\\) = 80-83\nC = 73-77\n\\(C^-\\) = 70-73\nD = 63-67\n\\(D^-\\) = 60 - 63"
  },
  {
    "objectID": "syllabusPSYC4310.html#expectations-and-policies",
    "href": "syllabusPSYC4310.html#expectations-and-policies",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Expectations and Policies",
    "text": "Expectations and Policies\nCommunication skills: You are expected to exercise strong academic verbal and writing skills for expressing yourself to complete class assignments. Failing to clearly communicate your ideas on class assignments may result in you losing points on that assignment. You also need strong analytical and critical thinking skills for completing weekly tasks. I will do my best to respond to emails within 1 - 2 business days (Monday through Friday). If you do not hear back from me within this time interval, send me a follow-up email that includes your original email message. Please keep in mind that if your email question is sent at the last minute it may not be possible to send you a response right before the submission of an assignment or exam. Before emailing me, please see if the answer to your question can be found on the course syllabus, schedule, or Canvas webpage.\nAttendance: If you want a good grade in this class, you need to keep up with the online course material. However, given the format of this course attendance is not contemplated in the final grade.\nDiversity: I will always embrace diversity as the most important human value. It is expected that students understand the importance of creating diverse and safe places free of discrimination by gender, age, race, ethnicity, nationality, sexual orientation, gender identity or disability. I am an ally to the lesbian, gay, bisexual, transgender, queer, intersex, and asexual (LGBTQIA) community, and I am available to listen and support you in an affirming manner. I can assist in connecting you with resources on campus to address problems you may face pertaining to sexual orientation and/or gender identity that could interfere with your success at Stan State.\nAcademic misconduct: Academic dishonesty will not be tolerated. Instances of academic misconduct (e.g., plagiarism, cheating) will result in a grade of zero on the exam/assignment in question. Additionally, you may also receive a lower letter grade or “F” in the class, be reported to Judicial Affairs for academic misconduct activity tracking or disciplinary action, suspended or expelled from the university. It is your responsibility to know the rules. Always paraphrase and cite the source properly according to APA style, avoid copying sentences unless they are necessary, and you cite the author in APA style. Always cite your source! In detail, pay attention to the California Code of Regulations:\n\n\n\n\n\n\nWarning\n\n\n\n“Title 5, California Code of Regulations, Section 41301 notes that students may be”expelled, suspended, placed on probation, or given a lesser sanction for one or more of the following causes which must be campus related: 1. Cheating or plagiarism in connection with an academic program at a campus. . . .” (see “Student Rights & Responsibilities” section of the current Stanislaus State catalog).”\n\n\nAPA Style: Unfortunately, I have to enforce the use of APA style, this is important to generate clean and tidy documentation while you follow scientific formatting. You have to follow the APA style 7th edition, I would recommend to buy the manual or just use this website, it has plenty of information about it, it also provides tools to generate references and citations: https://owl.purdue.edu/owl/research_and_citation/apa_style/apa_style_introduction.html\nStudents with disabilities: If you are a student with a documented disability at Stan State, please see me immediately to discuss appropriate accommodations. You must email me your letter of accommodation from Stan State’s DRS department as soon as possible. For exam accommodations, you must email me your accommodation letter at least seven days (7) before a scheduled exam to receive your accommodation (see schedule for exam dates). Contact me via email if you wish to discuss your accommodation or if you are in the process of registering for DRS services. Note, that accommodations are not provided retroactively."
  },
  {
    "objectID": "syllabusPSYC4310.html#student-resources",
    "href": "syllabusPSYC4310.html#student-resources",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Student Resources",
    "text": "Student Resources\nHere are some of the resources available to you here at Stan State. All these services are available to you, as a Stan State student, free of charge (except certain medical appointments and procedures). Please visit their web pages to learn more about the services they provide.\nBasic Need Support: 209-667-3108. Resources are available to help with securing food and emergency finances.\nStudent Health Center: 209-667-3396. Medical care, health education, disease prevention, laboratory testing, physicals, women’s and reproductive health, flu shots, immunizations.\nDisability Resource Services: 209-667-3159. Supports students and arranges accommodations for students with disabilities, including disabilities related to learning, vision, mobility, hearing, autism, or chronic or temporary health factors.\nPsychological Counseling Services: 209-667-3381. Confidential individual personal counseling and group/wellness workshops to help students deal with stress, anxiety, depression, grief, relationships.\nDiversity Resources: Workshops, student space, reading nook, complimentary coffee and tea, social justice library, conference room space.\nUndocumented Student Services: 209-667-3519. Walk-in advising, workshops, legal services, DACA renewal, scholarships, peer support, family and community engagement.\nAcademic Success Center: 209-667-3700. Drop-in advising for general education, university requirements, undeclared majors, academic probation, and California Promise.\nLearning Commons: 209-667-3642, Tutoring (walk-in and regular appointments), supplemental instruction, WPST, writing center.\nCareer and Professional Development: 209-667-3661. Career coaching, workshops, resume building, business attire.\nWarrior Food Pantry: 209-667-3561. Non-perishable food items and toiletries, at no cost. Collect up to 10 items per week.\nStudent Affairs: 209-667-3177. General hub for all student academic and support services on campus."
  },
  {
    "objectID": "syllabusPSYC4310.html#class-schedule",
    "href": "syllabusPSYC4310.html#class-schedule",
    "title": "RESEARCH SEMINAR IN SOCIAL PSYCHOLOGY (WP)",
    "section": "Class Schedule",
    "text": "Class Schedule\nThe following class schedule is always under construction, which might change every week. You will receive a notification if there are changes. If not, you will not be penalized because of any unannounced change.\n\n\n\n\n\n\nWarning\n\n\n\nYou should always check Canvas, I might add additional readings such as scientific articles, press articles or videos.\n\n\n\n\n\n\n\n\n  \n    \n      Schedule Fall 2023\n    \n    \n      This schedule may change during the semester\n    \n    \n      Date\n      Topic\n      Assignment\n    \n  \n  \n    08/21-08/25\nCourse introduction. Complete survey\n1. Survey\n\n    08/28-09/01\nConducting a literature search.\n2. Research Study Ideas\n\n    09/04-09/08\nComplete basic research study idea assignment for approval.\n3. Article Summary I\n\n    09/11-09/15\nReview ethics, avoiding plagiarism\nRead Galvan Chapters 7, 13. \n     09/18-09/22\nPeer grade avoiding plagiarism assignment\nIRB application\nIntroduction Outline\n    09/25-09/29\nReview article summary II and study proposal assignments\n\nReferences Section\n\n    10/02-10/06\nPlan research study and work on study proposal\n\nRead Galvan Chapter 8\nArticle Summary II\n\n    10/09-10/13\nRecruiting participants- access Sona\nStudy Proposal\n    10/16-10/20\nReview paper 1 assignment, Qualtrics,\nIRB application.\nPIRB Application\n    10/23-10/27\nReview editing papers,\nANOVA and regression Review\nPaper #1\n\n    10/30-11/03\nDiscussion section.\nTBD, Check Canvas\n\n    11/06-11/10\nData analysis in R and/or Jamovi\n\n    11/13-11/17\nData analysis in R and/or Jamovi\n\n    11/20-11/24\nData analysis in R and/or Jamovi\n\n    11/27-12/01\nData analysis in R and/or Jamovi\n\n    12/04-12/08\nGroup presentations\nGroup Presentation\nDue to Canvas\n    Dec 15th\nPSYCHOLOGY RESEARCH SYMPOSIUM"
  },
  {
    "objectID": "PSYC4310.html#you-may-also-check-some-of-the-presentations-i-prepared-for-this-course",
    "href": "PSYC4310.html#you-may-also-check-some-of-the-presentations-i-prepared-for-this-course",
    "title": "PSYC4310: Seminar in Social Psychology",
    "section": "",
    "text": "Lecture 1: The journey to the begining\nLecture 2: Designing your research project"
  },
  {
    "objectID": "PSYC3000.html#life-is-crazy-psychology-is-also-numbers",
    "href": "PSYC3000.html#life-is-crazy-psychology-is-also-numbers",
    "title": "PSYC3000",
    "section": "",
    "text": "At this point you might be thinking: “I decided to study psychology to avoid numbers!”\nBut numbers are not avoidable if you want to be scientist or scientist practitioner. It might be boring sometimes, but other times you might have fun answering questions related to NATURE. Yes, NATURE!\nThis is a psychology course, and my aim is to study statistics as a science that helps other sciences to study NATURE. Psychological processes are also part of NATURE, of course!",
    "crumbs": [
      "Home",
      "Content",
      "PSYC3000"
    ]
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html",
    "href": "lecture9/AssumptionsAndMore.html",
    "title": "Correlation and Regression Models",
    "section": "",
    "text": "library(ISLR)\nlibrary(DT)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(broom)\nlibrary(viridis)\n\nLoading required package: viridisLite\n\nlibrary(ggplot2)\n\n\nRegression is only one member of the family of General Linear Model (GLM).\nThe GLM covers models such as Analysis of Variance (ANOVA), Multivariate Analysis of Variance (MANOVA), Classical Regression Model (linear regression) and \\(t\\)-test.\nMy aim was to introduce these topics as a generalization of GLM, or better said, as a family.\nThere is a large theory related to these GLM models, but we don’t have time to discuss every detail. More advanced classes are necessary to explain particular details.\n\n\n\n\n\n  graph TD\n    A[GLM] --&gt; B(ANOVA)\n    A[GLM] --&gt; C(MANOVA)\n    A[GLM] --&gt; D(t-test)\n    A[GLM] --&gt; E(Linear Regression)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#why-are-we-studying-the-regression-model-and-other-demons",
    "href": "lecture9/AssumptionsAndMore.html#why-are-we-studying-the-regression-model-and-other-demons",
    "title": "Correlation and Regression Models",
    "section": "Why are we studying the regression model and other demons?",
    "text": "Why are we studying the regression model and other demons?\n\nRegression is only one member of the family of General Linear Model (GLM).\nThe GLM covers models such as Analysis of Variance (ANOVA), Multivariate Analysis of Variance (MANOVA), Classical Regression Model (linear regression) and \\(t\\)-test.\nMy aim was to introduce these topics as a generalization of GLM, or better said, as a family.\nThere is a large theory related to these GLM models, but we don’t have time to discuss every detail. More advanced classes are necessary to explain particular details.\n\n\n\n\n\n  graph TD\n    A[GLM] --&gt; B(ANOVA)\n    A[GLM] --&gt; C(MANOVA)\n    A[GLM] --&gt; D(t-test)\n    A[GLM] --&gt; E(Linear Regression)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova",
    "href": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova",
    "title": "Correlation and Regression Models",
    "section": "Let’s make up time for ANOVA",
    "text": "Let’s make up time for ANOVA\n\nAnalysis of Variance a.k.a ANOVA is a model that helps to analyze mean differences in multiple groups. It is actually very similar to the Classical Regression Model, that’s why I included ANOVA in the Classical Regression Model lecture.\nThe ANOVA model tests the null hypothesis that all group means are equal. It could be represented like this:\n\n\\[\\begin{equation}\nH_{0}: \\mu_{1} = \\mu_{2} = \\mu_{3}\n\\end{equation}\\]\n\nThis would be what we call an omnibus test, which means we are testing the overall effect of our independent variable on the dependent variable.\nAs always, let’s understand this with an example:\n\n\n\n\nWage's mean, sd, and variance by Marital Status\n\n\nmaritl\nM\nSD\nvariance\n\n\n\n\n1. Never Married\n92.73\n32.92\n1083.73\n\n\n2. Married\n118.86\n43.12\n1859.38\n\n\n3. Widowed\n99.54\n23.74\n563.64\n\n\n4. Divorced\n103.16\n33.80\n1142.51\n\n\n5. Separated\n101.22\n33.66\n1133.22\n\n\n\n\n\n\n\n\nIn the table above we can see the mean of wage by marital status, there are males who never married, married, widowed, divorced or separated. ANOVA will help us to answer the question:\n\n\nAre the mean differences in wage by marital status explainable by chance alone?\n\n\nIn this case ANOVA could tell us: Yes! there is a difference, but where?\nIn this scenario, ANOVA will allow us to do pairwise comparisons , this means; we could compare the mean of divorced males versus the mean of married males, but also test all the combinations later on. This is called a post hoc analysis."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.",
    "href": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.",
    "title": "Correlation and Regression Models",
    "section": "Let’s make up time for ANOVA (cont.)",
    "text": "Let’s make up time for ANOVA (cont.)\n\nThe ANOVA follows the logic of variance decomposition:\n\n\\[\\begin{equation}\noutcome_{i} = (model) + error_{i}\n\\end{equation}\\]\n\nThis means, that ANOVA accounts for the variances within your groups, and also calculates the variance that is explained by your MODEL. In fact, the Classical Regression Model does exactly the same thing.\nRemember: The line of best fit in regression is estimated adding the sum of square distances from the line. The line with the lowest sum of distances is the best line in regression.\nWe are going to do something similar in ANOVA, it is called the Total Sum of Squares (\\(SS_{T}\\)), this calculation will give you the distance from the grand mean:\n\n\\[\\begin{equation}\nSS_{T} = \\sum^N_{i=1}(x_{i}-\\bar{x}_{(grandMean)})\n\\end{equation}\\]\n\nThe term \\(x_{i}\\) is the score or value for each observation, and \\(\\bar{x}_{(grandMean)}\\) is the overall mean."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.-1",
    "href": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.-1",
    "title": "Correlation and Regression Models",
    "section": "Let’s make up time for ANOVA (cont.)",
    "text": "Let’s make up time for ANOVA (cont.)\n\nWe can continue with our example with wage \\(\\sim\\) maritl.\nWe can calculate the grand mean of wage:\n\n\nWageGrandMean &lt;- mean(Wage$wage)\nWageGrandMean \n\n[1] 111.7036\n\n\n\nNow we could estimate \\(SS_{t}\\) in R:\n\n\nsstData &lt;- Wage |&gt; select(wage, maritl) |&gt; \n  mutate(SSt = (wage - mean(wage))^2)\n  \nsum(sstData$SSt)\n\n[1] 5222086\n\n\nThe total \\(SS\\) is 5222086. This is the TOTAL variation within the data.\n\nWe also need to estimate the Model Sum of Squares:\n\n\\[\\begin{equation}\n\nSS_{M} = \\sum^k_{n=1}n_{k}(\\bar{x}_{k}-\\bar{x}_{grandMean})^2\n\\end{equation}\\]\n\n\\(k\\) represents each group, in simple words we are estimating the mean difference of each group from the overall mean. That’s the distance from the grand mean.\nWe will need to estimate the residuals of the our model, the residuals is variance not explained by our ANOVA model. The estimation is:"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.-2",
    "href": "lecture9/AssumptionsAndMore.html#lets-make-up-time-for-anova-cont.-2",
    "title": "Correlation and Regression Models",
    "section": "Let’s make up time for ANOVA (cont.)",
    "text": "Let’s make up time for ANOVA (cont.)\n\n\nIn this estimation we calculate the difference of each observation from the group mean where the observation is located. So, for instance if Katie was in the divorced group, then we could compute: \\(wage_{katie} - mean(wage_{divorced})\\). This is an indicator of variation not explained by the model. This means, ANOVA does not explain what happens within each group’s variation, it accounts for the between group variation."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#time-for-examples",
    "href": "lecture9/AssumptionsAndMore.html#time-for-examples",
    "title": "Correlation and Regression Models",
    "section": "Time for examples !",
    "text": "Time for examples !\n\n\nShow the code\nmodelAnova &lt;- aov(wage ~ maritl, data = Wage)\nsummary(modelAnova)\n\n\n              Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nmaritl         4  363144   90786   55.96 &lt;2e-16 ***\nResiduals   2995 4858941    1622                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe result showed that the difference is not explained by chance alone. But wait, this is an omnibus test. It just tell me that at least one mean is different."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#time-for-examples-1",
    "href": "lecture9/AssumptionsAndMore.html#time-for-examples-1",
    "title": "Correlation and Regression Models",
    "section": "Time for examples !",
    "text": "Time for examples !\n\n\nShow the code\n  ggplot(data = Wage,aes(x=maritl, y=wage, fill=maritl)) +\n    geom_boxplot() +\n   stat_summary(fun=\"mean\", color=\"red\")+\n    scale_fill_viridis(discrete = TRUE, alpha=0.6, option=\"A\") +\n    theme_classic()+\n   theme(legend.position = \"none\")+\n  labs(x= \"Marital Status\", y= \"Wage\", title = \"Boxplot of Wage by Marital Status\")"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#time-for-examples-2",
    "href": "lecture9/AssumptionsAndMore.html#time-for-examples-2",
    "title": "Correlation and Regression Models",
    "section": "Time for examples !",
    "text": "Time for examples !\n\nLet’s see the pairwise comparisons:\n\n\n\nShow the code\npairTest &lt;-TukeyHSD(modelAnova)\n\n\n\n\nas.data.frame(lapply(pairTest, function(x) round(x,2))) %&gt;%\n  rename(meanDiff = maritl.diff,\n          lower = maritl.lwr ,\n         upper = maritl.upr ,\n         pvalueAdj = maritl.p.adj\n         ) %&gt;%\n kbl(caption = \"Pairwise Contrast\") %&gt;%\nkable_classic_2(\"hover\", full_width = T, \n                bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\n\nPairwise Contrast\n\n\n\nmeanDiff\nlower\nupper\npvalueAdj\n\n\n\n\n2. Married-1. Never Married\n26.13\n21.18\n31.07\n0.00\n\n\n3. Widowed-1. Never Married\n6.80\n-18.78\n32.39\n0.95\n\n\n4. Divorced-1. Never Married\n10.42\n1.60\n19.25\n0.01\n\n\n5. Separated-1. Never Married\n8.48\n-6.96\n23.92\n0.56\n\n\n3. Widowed-2. Married\n-19.32\n-44.66\n6.02\n0.23\n\n\n4. Divorced-2. Married\n-15.70\n-23.77\n-7.63\n0.00\n\n\n5. Separated-2. Married\n-17.64\n-32.66\n-2.63\n0.01\n\n\n4. Divorced-3. Widowed\n3.62\n-22.75\n29.99\n1.00\n\n\n5. Separated-3. Widowed\n1.68\n-27.58\n30.93\n1.00\n\n\n5. Separated-4. Divorced\n-1.94\n-18.65\n14.76\n1.00"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation",
    "text": "F ratio estimation\n\nIn previous classes we studied the \\(t\\)-test. Hopefully, you remember the \\(t\\)-distribution:\n\n\n\nShow the code\nset.seed(234)\n\nplot(density(rt(30000,5)),\n     xlab = \"Simulated Values\",\n     ylab = \"p(y)/likelihood\",\n     main = \"Probability density plot generated from the t-distribution\")"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-1",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-1",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation",
    "text": "F ratio estimation\n\nThe t-distribution is used to find the probability of the estimated mean difference.\nIn ANOVA we will use the F-distribution to find the probability of the estimated difference. In \\(t\\)-test we find only one mean difference, in ANOVA we will have several mean differences. The F-distribution will help us to test all the mean differences at the same time. That’s way we call omnibus test the first step in ANOVA.\nThis distribution is a little bit peculiar. Because it requires degrees of freedom in the numerator and the denominator, we will see later how are we going to use these degrees of freedom in the context of the ANOVA."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-2",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-2",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation",
    "text": "F ratio estimation\nJust another F distribution\n\n\nShow the code\nset.seed(234)\nplot(density(rf(30000,5,10)),\n     xlab = \"Simulated Values\",\n     ylab = \"Likelihood\",\n     main = \"Probability density plot generated from the F-distribution\")"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation: let’s go to the point",
    "text": "F ratio estimation: let’s go to the point\n\nTo estimate the F ratio we need first to estimate the Mean Squares based on the Sum of Squares.\nWe previously saw the formula to estimate the \\(SS_{R}\\), which in simple words, the total variation due to external factors not controlled by the variables in the model. While \\(SS_{M}\\) is the total variation accounted by the ANOVA model. \\[\\begin{align}\nMS_{M} &= \\frac{SS_{M}}{df_{M}}\\\\\nMS_{R} &= \\frac{SS_{R}}{df_{R}}\\\\\n\\end{align}\\]\n\n\\(MS_{m}\\) is the average amount of variation explained by the model (e.g., the systematic variation) , whereas \\(MS_{R}\\) is a gauge of the average amount of variation explained by extraneous variables (the unsystematic variation) (Field et al., 2012).\n\nThe \\(df\\) in \\(MS_{M}\\) are estimated by subtracting 1 to the number of groups. In our example we have 5 groups, then we should do \\(df = 5-1\\). The degrees of freedom in \\(MS_{R}\\) are estimated by computing \\(N-k\\), where \\(N\\) is the sample size and \\(k\\) the number of groups. In our example would be \\(df = 3000-5\\).\nLet’s take a loot again at the ANOVA taht we estimated before:\n\n\n\nShow the code\nmodelAnova &lt;- aov(wage ~ maritl, data = Wage)\nsummary(modelAnova)\n\n\n              Df  Sum Sq Mean Sq F value Pr(&gt;F)    \nmaritl         4  363144   90786   55.96 &lt;2e-16 ***\nResiduals   2995 4858941    1622                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nRemember that we had 5 marital status groups, and the total sample is 3000 males. If you check the Df column in the table above you will see two numbers, the first one is 4 (\\(5-1=4\\)) , and the second number is 2995 (\\(3000-5= 2995\\))."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point-1",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point-1",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation: let’s go to the point",
    "text": "F ratio estimation: let’s go to the point\n\nAfter computing the Mean Squares and degrees of freedom, you’ll be able estimate the F ratio:\n\n\\[\\begin{align}\nF &= \\frac{MS_{M}}{MS_{R}}\\\\\n  &= \\frac{90786}{1622}\\\\\n  &= 55.96\\\\\n\\end{align}\\]\n\nThe F in this case is a ratio of the variation explained by the model. If we conducted an experiment, we want a large F ratio because it would mean that we maximize the systematic variance explained by the experiment. Ideally, \\(MS_{R}\\) should be small."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point-2",
    "href": "lecture9/AssumptionsAndMore.html#f-ratio-estimation-lets-go-to-the-point-2",
    "title": "Correlation and Regression Models",
    "section": "F ratio estimation: let’s go to the point",
    "text": "F ratio estimation: let’s go to the point\n\nThe Cumulative Density Function of the F distribution will help us to estimate the probability of seeing a number as extreme as: 55.96. The red line corresponds to the location of 55.96.\n\n\n\nShow the code\nsimValues &lt;- seq(0,60,0.1)\n\nvalues &lt;-  pf(simValues, \n              df1 = 4, \n              df2 = 2995,\n              lower.tail = FALSE)\n\nplot(simValues, values, type = \"l\",\n     xlab = \"F values\",\n     ylab = \"Probability of F\",\n     main = \"Probability of observing F values\")\nabline(v = 55.96, col=\"red\", lwd=3, lty=2)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model",
    "href": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model",
    "title": "Correlation and Regression Models",
    "section": "ANOVA and Classical Regression Model",
    "text": "ANOVA and Classical Regression Model\n\nOne of the most important assumptions in ANOVA and Classical Regression Model is constant variance.\nWhen you estimate an ANOVA you could test this assumption using the Levene’s Test, the same test that we used to test the constant variance in \\(t\\)-test.\nThe null hypothesis in the Levene’s test states: The variances of all groups are equal. If the test shows a \\(p\\)-value lower than 0.05, we reject the null hypothesis.\n\n\n\nShow the code\nlibrary(car)\nleveneTest(wage ~ maritl, data = Wage)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n        Df F value    Pr(&gt;F)    \ngroup    4  10.258 3.006e-08 ***\n      2995                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nThe result shows a \\(p\\)-value lower than 0.05, therefore we should reject the null hypothesis. The variances are not equal between groups.\n\n\n\n\n\n\n\nAlways remember!\n\n\nALL OF THE ASSUMPTIONS REFER TO THE DATA-GENERATING PROCESS. NONE OF THE ASSUMPTIONS REFER TO THE ACTUAL OBSERVED DATA (Westfall & Arias, 2020)\n\n\n\n\nIn this case, we have a large sample size, this might cause a small p-value in the case of the Levene’s Test."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-1",
    "href": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-1",
    "title": "Correlation and Regression Models",
    "section": "ANOVA and Classical Regression Model",
    "text": "ANOVA and Classical Regression Model\n\nIn the context of regression we can plot the predicted values called \\(\\hat{y}\\) (“yhat”), versus the residuals\n\n\n\nShow the code\nnewWage &lt;- Wage %&gt;%  mutate(married = ifelse(maritl == \"2. Married\", 1,0),\n                          widowed = ifelse(maritl== \"3. Widowed\", 1,0),\n                          divorced = ifelse(maritl== \"4. Divorced\", 1,0),\n                          separate = ifelse(maritl== \"5. Separated\", 1,0))\n\n\n\nfitModel &lt;- lm(wage ~ married  + widowed + divorced + separate, data = newWage)\n\ny.hat = fitModel$fitted.values\nresid = fitModel$residuals\nplot(y.hat, resid,\n     xlab = \"Predicted wage\",\n     ylab = \"residuals\")\nabline(h=0)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-2",
    "href": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-2",
    "title": "Correlation and Regression Models",
    "section": "ANOVA and Classical Regression Model",
    "text": "ANOVA and Classical Regression Model\n\nIn the previous plot we can see more variability when the predicted wage is high. We can also see differences in variability in the lower income groups.\nWe can plot similar information, but this time let’s use the absolute residuals:\n\n\n\nShow the code\nabs.resid = abs(resid)\nplot(y.hat, abs.resid,\n     xlab = \"Predicted wage\",\n     ylab = \"Absolute residuals\",\n     main = \"Predicted wage by absolute residuals\") \n\n\n\n\nSimilar pattern, there is more variability in the groups where men earn more money. But is this bad for the linear model?\nIt depends. We need to think if this is just part of the data generating process."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-3",
    "href": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-3",
    "title": "Correlation and Regression Models",
    "section": "ANOVA and Classical Regression Model",
    "text": "ANOVA and Classical Regression Model\n\nWe can check the constant variance assumption in the context of a continuous predictor. This example only applies to Linear Regression, it won’t work with ANOVA because ANOVA only supports nominal predictors (groups).\n\n\n\nShow the code\nrum &lt;- read.csv(\"ruminationClean.csv\")\n\nfitModel &lt;- lm(anx ~ worry, data = rum)\n\ny.hat = fitModel$fitted.values\nresid = fitModel$residuals\nplot(y.hat, resid,\n     xlab = \"Predicted Anxiety\",\n     ylab = \"residuals\")\nabline(h=0)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-4",
    "href": "lecture9/AssumptionsAndMore.html#anova-and-classical-regression-model-4",
    "title": "Correlation and Regression Models",
    "section": "ANOVA and Classical Regression Model",
    "text": "ANOVA and Classical Regression Model\n\nGiven that plots are sometimes difficult to interpret in this case, you might conduct another test called the Glejser test (Westfall & Arias, 2020).\nIn this test we regress the absolute residuals on the predicted values. So, we are going to estimate a regression model to predict the residual, weird right?\nThe short story is that we will get an slope, and if this slope is not explained by chance alone we will reject the constant variance assumption.\nLet’s test the constant variance assumption when we estimate anxiety \\(\\sim\\) worry.\n\n\n\nShow the code\n### The target model\nfitAnx &lt;- lm(anx ~ worry, data = rum)\n### get the residuals and predicted values\nyhat &lt;-  fitAnx$fitted.values ## predicted values\nresi &lt;- abs(fitAnx$residuals) ## residuals \n\nsummary(lm(resi ~ yhat))\n\n\n\nCall:\nlm(formula = resi ~ yhat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.4731 -1.1677 -0.3061  0.8137  7.6189 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.71106    0.35353   2.011   0.0457 *  \nyhat         0.35385    0.08094   4.372 2.01e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.777 on 194 degrees of freedom\nMultiple R-squared:  0.08968,   Adjusted R-squared:  0.08499 \nF-statistic: 19.11 on 1 and 194 DF,  p-value: 2.009e-05\n\n\n\nIn this case we cannot hold the assumption of homocedasticity (constant variance), because the estimated slope is not explained by chance alone (\\(p &lt; .001\\))."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption",
    "href": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: normality assumption",
    "text": "Classical Regression Model: normality assumption\n\nYou can graphically evaluate if your model holds the assumption of normality. Remember that this assumption states that each conditional distribution \\(p(y|x)\\) is normally distributed.\nThe best option is to plot the residuals of your model versus the quantiles of the normal distribution, this is called a QQ-Plot (quantile to quantile plot)."
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption-1",
    "href": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption-1",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: normality assumption",
    "text": "Classical Regression Model: normality assumption\n\n\nShow the code\n### The target model\nfitAnx &lt;- lm(anx ~ worry, data = rum)\n### get the residuals and predicted values\nres &lt;- abs(fitAnx$residuals) ## residuals \nqqnorm(res)\nqqline(res)"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption-2",
    "href": "lecture9/AssumptionsAndMore.html#classical-regression-model-normality-assumption-2",
    "title": "Correlation and Regression Models",
    "section": "Classical Regression Model: normality assumption",
    "text": "Classical Regression Model: normality assumption\n\nThe qqplot can be sometimes difficult to read. We could perform a test called the Shapiro-Wilk test. In this test, the null hypothesis states “There is not deviation from the normal distribution”. A \\(p\\)-value less than .05 means that the model does not hold the assumption of conditional normality.\n\n\n\nShow the code\nshapiro.test(res)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  res\nW = 0.85861, p-value = 1.612e-12\n\n\n\nThe Shapiro-Wilk test showed a p-value lower than .001, this means that the deviations of the residuals from the normal distribution are not explained by chance.\n\n\n\n\n\n\n\nIMPORTANT!\n\n\nThe regression model and the General Linear Model does not assume that the outcome should come from a normally distributed process. That’s why the normality test is performed on the residuals of the model. The residuals are the product of the conditional relationship \\(p(y|x)\\). Don’t test the normality assumption on your dependent or outcome variable."
  },
  {
    "objectID": "lecture10/ModelComparasion.html",
    "href": "lecture10/ModelComparasion.html",
    "title": "Compare multiple models",
    "section": "",
    "text": "Introduce the concept of nested model.\nExplain why we need to compare different statistical models?\nExplain the concept of over-fitting a model.\nAnd probably some memes…"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#aims-in-this-lecture",
    "href": "lecture10/ModelComparasion.html#aims-in-this-lecture",
    "title": "Compare multiple models",
    "section": "",
    "text": "Introduce the concept of nested model.\nExplain why we need to compare different statistical models?\nExplain the concept of over-fitting a model.\nAnd probably some memes…"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#why-do-we-care-about-model-comparasion",
    "href": "lecture10/ModelComparasion.html#why-do-we-care-about-model-comparasion",
    "title": "Compare multiple models",
    "section": "Why do we care about model comparasion ?",
    "text": "Why do we care about model comparasion ?\n\nAs you might remember, probabilistic models are our possible explanations on how DATA is produced in Nature.\nYou may also recall that statistics is concerned with reducing uncertainty, specially because we always have unknown parameters in our models.\nThere could be multiple possible models that potentially explain how DATA is produced. We need to make a decision and select the model that best fit the DATA."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#but-wait-what-is-the-best-fit",
    "href": "lecture10/ModelComparasion.html#but-wait-what-is-the-best-fit",
    "title": "Compare multiple models",
    "section": "But wait… What is the best fit ?",
    "text": "But wait… What is the best fit ?\n\n\n\nYou might be asking, what is a “fit” or “best fit” model?\nWhen I’m using the word “fit”, I’m referring to how appropriate is the statistical model to explain the relationships of the outcome with all possible independent variables.\n“Fit” also means that our model is an estimated model on observed data, therefore, our model could be a “good fit model” or “bad fit model”.\nA bad fit model will not reproduce the most reliable estimates, and your standard errors will be large.\n\n::: callout-important Remember: All models are wrong…but some are useful! (Box, 1976) :::"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#lets-think-about-possible-scenarios",
    "href": "lecture10/ModelComparasion.html#lets-think-about-possible-scenarios",
    "title": "Compare multiple models",
    "section": "Let’s think about possible scenarios",
    "text": "Let’s think about possible scenarios\n\nImagine that you have three independent variables that could be related to depression, let’s see the model:\n\n\\[depression \\sim \\beta_{0} + \\beta_{1}selfEsteem_{1} + \\beta_{2}likePancake_{2} + \\beta_{3}rumination_{3} + \\epsilon\\]\n\nIn this model we have three predictors: 1) self-esteem score, 2) Do you like pancakes?, and a rumination score.\nHowever, is this the best model? Would a model without \\(\\beta_{2}likePancake\\) be a better fit model?"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring",
    "href": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring",
    "title": "Compare multiple models",
    "section": "\\(R^2\\) R-Square is not so boring",
    "text": "\\(R^2\\) R-Square is not so boring\n\nCommonly, we evaluate how good-fitted is hour regression model using the \\(R^2\\):\n\n\\[R^2 = \\frac{SS_{M}}{SS_{t}}\\] Where:\n\\(SS_{M}\\) = Sum of Squares of the Model.\n\\(SS_{T}\\) = Total Sum of Squares.\n\nThis should look similar to the estimation that we did when calculating an ANOVA model. Because, linear regression and ANOVA belong to the same family of the General Linear Model."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring-1",
    "href": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring-1",
    "title": "Compare multiple models",
    "section": "\\(R^2\\) R-Square is not so boring",
    "text": "\\(R^2\\) R-Square is not so boring\n\nYou can see that \\(R^2\\) is a proportion of variance explained by the model. Why? Because, we divide the variability of the model by the total variability in the data. The result is an estimate that quantifies the percentage of variance explained by the model. As always, we can study an example:\n\n\\[depression \\sim \\beta_{0} + \\beta_{1}selfEsteem_{1} + \\beta_{2}numberCandies_{2} + \\beta_{3}rumination_{3} + \\epsilon\\]\n\n\nShow the code\n### Simulation time!!\nlibrary(lavaan) ## &lt;--- package to generate the values.\n\n\n\n1\n\nI’m specifying the “population model”.\n\n2\n\nThis line creates the simulated values. I’m generating 100 random values.\n\n3\n\nEstimating the model using function lm().\n\n\n\n\nThis is lavaan 0.6-16.1856\nlavaan is FREE software! Please report any bugs.\n\n\nShow the code\nlibrary(broom)\n\n1mod1 &lt;- 'depression ~ 0.5*selfSteem + 0.8*rumination + 0*numberCandies'\n\n\n\ngenerated &lt;- simulateData(mod1,\n                          meanstructure = TRUE, \n                          sample.nobs = 100,\n2                          seed = 12)\n\n\nfitModel &lt;- lm(depression ~ selfSteem + \n                 rumination + \n                 numberCandies, \n3               data = generated)\n\ngt(tidy(fitModel),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−0.04\n0.09\n−0.41\n0.68\n    selfSteem\n0.37\n0.10\n3.80\n0.00\n    rumination\n0.75\n0.10\n7.81\n0.00\n    numberCandies\n0.03\n0.09\n0.28\n0.78\n  \n  \n  \n\n\n\n\n\nNow let’s check the \\(R^2\\):\n\n\n\nShow the code\ngt(glance(fitModel) |&gt;\n     dplyr::select(r.squared, AIC, BIC)) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  data_color(\n    columns = c(r.squared),\n    palette = \"red\")\n\n\n\n\n\n\n  \n    \n    \n      r.squared\n      AIC\n      BIC\n    \n  \n  \n    0.42\n272.60\n285.63\n  \n  \n  \n\n\n\n\n\nThe \\(R^2\\) is 0.42, this can be read as: the linear model explains 42 % of the total variance."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring-2",
    "href": "lecture10/ModelComparasion.html#r2-r-square-is-not-so-boring-2",
    "title": "Compare multiple models",
    "section": "\\(R^2\\) R-Square is not so boring",
    "text": "\\(R^2\\) R-Square is not so boring\n\nBut, let’s see what happens when I delete the independent variable number of candies from the model:\n\n\n\nShow the code\nfitModel &lt;- lm(depression ~ selfSteem + \n                 rumination, \n1               data = generated)\n\ngt(tidy(fitModel),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2)\n\n\n\n1\n\nIn this example number of candies is omitted.\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−0.04\n0.09\n−0.41\n0.68\n    selfSteem\n0.38\n0.10\n3.84\n0.00\n    rumination\n0.75\n0.10\n7.84\n0.00\n  \n  \n  \n\n\n\n\n\nWe can see again the \\(R^2\\):\n\n\n\nShow the code\ngt(glance(fitModel) |&gt;\n     dplyr::select(r.squared, AIC, BIC)) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  data_color(\n    columns = c(r.squared),\n    palette = \"red\") |&gt;\n  tab_header(title = \"R-squared after deleting number of candies\")\n\n\n\n\n\n\n  \n    \n      R-squared after deleting number of candies\n    \n    \n    \n      r.squared\n      AIC\n      BIC\n    \n  \n  \n    0.42\n270.68\n281.10\n  \n  \n  \n\n\n\n\n\nSo far, the \\(R^2\\) remained the same. It is an expected result, because number of candies was not explaining a portion of variance beyond chance.\nBut, if we delete a meaningful variable such as rumination the R^2 will decrease:\n\n\nfitModel &lt;- lm(depression ~ selfSteem + \n                 numberCandies, \n1               data = generated)\n\ngt(tidy(fitModel),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"Estimated values after deleting rumination\")\n\n\n1\n\nRumination was deleted in this model.\n\n\n\n\n\n\n\n\n  \n    \n      Estimated values after deleting rumination\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−0.06\n0.12\n−0.48\n0.63\n    selfSteem\n0.28\n0.12\n2.22\n0.03\n    numberCandies\n0.00\n0.12\n0.04\n0.97\n  \n  \n  \n\n\n\n\n\nWe can check again the \\(R^2\\) after deleting rumination:\n\n\n\nShow the code\ngt(glance(fitModel) |&gt;\n     dplyr::select(r.squared, AIC, BIC)) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  data_color(\n    columns = c(r.squared),\n    palette = \"red\") |&gt;\n  tab_header(title = \"R-squared after deleting rumination\")\n\n\n\n\n\n\n  \n    \n      R-squared after deleting rumination\n    \n    \n    \n      r.squared\n      AIC\n      BIC\n    \n  \n  \n    0.05\n319.76\n330.18\n  \n  \n  \n\n\n\n\n\nYou will see that the \\(R^2\\) value decreased after removing rumination. Why? Rumination is an important variable related to depression, by omitting rumination, your model explains less variance and therefore it affects the model fit."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#low-vs.-high-r2",
    "href": "lecture10/ModelComparasion.html#low-vs.-high-r2",
    "title": "Compare multiple models",
    "section": "Low vs. High \\(R^2\\)",
    "text": "Low vs. High \\(R^2\\)\n\nHigh R-squaredLow R-squared\n\n\n\nfitModelHigh &lt;- lm(depression ~ selfSteem + rumination +\n                     numberCandies, \n                   data = generated)\n\n\nplot(x=predict(fitModelHigh), y=generated$depression,\n     xlab='Predicted Values',\n     ylab='Actual Values',\n     main='Predicted vs. Actual Values with high R-Squared')\n\nabline(a=0, b=1)\n\n\n\n\n\n\n\n\nShow the code\nfitModelLow &lt;- lm(depression ~ selfSteem + \n                    numberCandies, \n                  data = generated)\n\n\nplot(x=predict(fitModelLow), y=generated$depression,\n     xlab='Predicted Values',\n     ylab='Actual Values',\n     main='Predicted vs. Actual Values with low R-Squared')\n\nabline(a=0, b=1)"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#more-options-to-compare-models",
    "href": "lecture10/ModelComparasion.html#more-options-to-compare-models",
    "title": "Compare multiple models",
    "section": "More options to compare models",
    "text": "More options to compare models\n\nThe \\(R^2\\) is an estimate that helps to select the best model. You will see that researchers select the model with a larger \\(R^2\\). But this is misleading.\nThe \\(R^2\\) could be inflated by adding non-meaningful predictors. If you throw multiple predictors into a regression you will get a high R-Squared. This is like doing a soup with random ingredients, you will probably get your soup but, is it the best soup?"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#more-options-to-compare-models-1",
    "href": "lecture10/ModelComparasion.html#more-options-to-compare-models-1",
    "title": "Compare multiple models",
    "section": "More options to compare models",
    "text": "More options to compare models\n\nAkaike Information Criterion (AIC) is one of the alternatives to select variables, therefore we can arrive to the best fitted model possible.\nThe AIC was created by Hirotugu Akaike.\n\n-Let’s see how this AIC fit measure looks like:\n\\[AIC = −2(\\text{Log Likelihood}) + 2 \\cdot ( \\text{number of estimated parameters})\\] - In the AIC estimation, you can see something call “Log Likelihood”. This is the information of the model. We haven’t had time to cover what is likelihood or something call “Maximum Likelihood”. But, at this point you are good if you understand “likelihood” as model information.\n\nNumber of estimated parameters is also part of the AIC estimation. This term in the estimation corresponds to how many parameter you are estimating in your model.\n\n\n\n\n\n\n\nTip\n\n\n\nParameter: unknown information in the model, we collect data to have information to reduce the uncertainty of the parameter or set of parameters.\n\n\n\nWe can understand the concept of number of parameters via an example:\n\n\\[depression \\sim \\beta_{0} + \\beta_{1}selfEsteem_{1} + \\beta_{2}numberCandies_{2} + \\beta_{3}rumination_{3} + \\epsilon\\]\n\nA few slides before, we studied the above model. If you count how many Greek letters it has, you will arrive to the total number of parameters:\nWe have three slopes represented by \\(\\beta_{i}\\) (Beta).\nWe also have a residual error represented by \\(\\epsilon\\).\nWe also have an intercept represented by \\(\\beta_{0}\\).\nHence, we have in total 5 parameters!"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#more-options-to-compare-models-2",
    "href": "lecture10/ModelComparasion.html#more-options-to-compare-models-2",
    "title": "Compare multiple models",
    "section": "More options to compare models",
    "text": "More options to compare models\n\nThe AIC is considered a “penalized” information criterion. It will penalize models with a large number of parameters.\nLower values indicate a better fit model. While large values indicate poorly fitted models.\nAs always, take a look at the following example. I’m estimating a regression model where only the intercept of depression is estimated. In simple words, it is a model that is estimating only the mean of depression.\n\n\n\nShow the code\nruminData &lt;- read.csv(\"ruminationComplete.csv\", \n                       na.strings = \"99\") |&gt;\n  mutate(ruminTotal = rowMeans(across(CRQS1:CRSQ13)),\n         depreTotal = rowSums(across(CDI1:CDI26), na.rm = TRUE),\n         anxTotal = rowMeans(across(DASS1:DASS7)),\n         negativeBeliefs = rowMeans(across(NBRSdv1:NBRSdv13)),\n         positiveBeliefs = rowMeans(across(PBRSdv1:PBRSdv9)),\n         attention = ATENCION,\n1         activation = ACTIVACION)\n\n#### We could estimate the most simple model.\n#### This is a model that estimates only the intercept.\n\ninterceptOnly &lt;- lm(depreTotal ~ 1, data = ruminData)\n\ngt(tidy(interceptOnly),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"Intercept-only model for the Depression score\")\n\n\n\n1\n\nI’m reading in the data, then I’m computing the total score with mutate() function.\n\n\n\n\n\n\n\n\n  \n    \n      Intercept-only model for the Depression score\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n10.44\n0.43\n24.12\n0.00\n  \n  \n  \n\n\n\n\n\nThe mean of depression according to the intercept-only model is 10.4433962. This model is so simple, because we didn’t add any predictor. This is what it makes this model a null model."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#we-can-compare-the-null-model",
    "href": "lecture10/ModelComparasion.html#we-can-compare-the-null-model",
    "title": "Compare multiple models",
    "section": "We can compare the null model",
    "text": "We can compare the null model\n\nNow we are in good position to compare the intercept-only model versus alternative models. And guess what? We will use the AIC to compare alternative models.\nWe can start by adding one predictor, let’s add rumination:\n\n\n\nShow the code\n### Let's add rumination as a predictor.\n\nfitWithRumination &lt;- lm(depreTotal ~ ruminTotal, data = ruminData)\n\ngt(tidy(fitWithRumination),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"Rumination as predictor of the Depression score\")\n\n\n\n\n\n\n  \n    \n      Rumination as predictor of the Depression score\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−2.81\n1.35\n−2.08\n0.04\n    ruminTotal\n5.69\n0.56\n10.16\n0.00\n  \n  \n  \n\n\n\n\n\nThe result is not surprising, if you pay attention to the slope, we can say: For 1-point increment in rumination, depression increments 5.69 points. But, is this model a good fit for the data compare with the intercept-only model?\n\n\n\nShow the code\ncompareAIC &lt;- data.frame(Model = c(\"Intercept Model\", \n                                   \"Model adding Rumination\"),\n                         AICvalue = c(AIC(interceptOnly)[1], \n                                      AIC(fitWithRumination)[1]))\n\ngt(compareAIC,\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"AIC values for the Intercept Model and the model with Rumination as predictor\")\n\n\n\n\n\n\n  \n    \n      AIC values for the Intercept Model and the model with Rumination as predictor\n    \n    \n    \n      Model\n      AICvalue\n    \n  \n  \n    Intercept Model\n1,385.27\n    Model adding Rumination\n1,259.84\n  \n  \n  \n\n\n\n\n\nThe AIC value for the intercept model was higher compare to the model with one predictor. We can conclude that adding rumination helps to estimate a model that fits the data better."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models",
    "href": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models",
    "title": "Compare multiple models",
    "section": "Backward, and Forward variable selection: Building models",
    "text": "Backward, and Forward variable selection: Building models\n\nWe just saw that AIC helps to select the best model, but what happens if you have multiple variables? Is there a way to select the best model faster? The answers is YES!\nYou can start adding one variable at the time, starting from the intercept-only model. In this approach you go “forward” by adding one variable at the time, then you compare each model versus the previous model.\nYou may also estimate a big model with multiple predictors. AIC can help you to trim the model until you keep the best variables related to your outcome. This is called “backwards” selection."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-1",
    "href": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-1",
    "title": "Compare multiple models",
    "section": "Backward, and Forward variable selection: Building models",
    "text": "Backward, and Forward variable selection: Building models\n\nIn R you can use the function stepAIC() to select the best model with the best set of variables.\nYou can follow a forward selection process or a backwards selection process.\nIn the next example, I’ll start building all models using the forward method. The first model will be a model estimating only the intercept of depression, and at each step, the stepAIC() function will add one variable at the time, while comparing all the possible combination of models.\nI’ll select the best model out of the following predictors:\n\nRumination.\nAnxiety.\nNegative metacognitive beliefs about rumination.\nPositive metacognitive beliefs about rumination.\nActivation.\nAttention.\n\n\n\n\nShow the code\nlibrary(MASS)\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nShow the code\n### Omit missing values\n\nruminDataNoMissing &lt;- ruminData |&gt;\n  dplyr::select(depreTotal, \n         ruminTotal,\n         anxTotal,\n         negativeBeliefs,\n         positiveBeliefs,\n         attention,\n         activation) |&gt;\n drop_na()\n\ninterceptModel &lt;- lm(depreTotal ~ 1, \n                     data = ruminDataNoMissing )\n\nmodelSelection &lt;- stepAIC(interceptModel,\n                          scope = list(upper = ~ruminTotal+anxTotal+negativeBeliefs+positiveBeliefs+attention+activation, \n                                       lower = ~1), \n                          direction = \"forward\",\n                          trace = FALSE)\n\n\ngt(tidy(modelSelection),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"Final model selected by stepAIC\")\n\n\n\n\n\n\n  \n    \n      Final model selected by stepAIC\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n−0.59\n1.88\n−0.31\n0.76\n    ruminTotal\n3.85\n0.66\n5.84\n0.00\n    anxTotal\n3.12\n0.84\n3.69\n0.00\n    attention\n−0.25\n0.08\n−2.98\n0.00\n    activation\n0.21\n0.09\n2.25\n0.03\n    negativeBeliefs\n1.46\n0.85\n1.72\n0.09\n  \n  \n  \n\n\n\n\n\nThe final model selected by the stepAIC function was a model where positiveBeliefs is omitted. If we drop this variable we obtain the best set of variables to predict depression.\n\n\ngt(modelSelection$anova,\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n   cols_hide(columns = c(Df, Deviance, \"Resid. Df\", \"Resid. Dev\")) |&gt;\n  tab_header(title = \"AIC values for all models forward method\")\n\n\n\n\n\n  \n    \n      AIC values for all models forward method\n    \n    \n    \n      Step\n      AIC\n    \n  \n  \n    \n715.80\n    + ruminTotal\n646.46\n    + anxTotal\n626.58\n    + attention\n624.43\n    + activation\n620.49\n    + negativeBeliefs\n619.46\n  \n  \n  \n\n\n\n\nIn the table above, you can see all the AIC values estimated after adding one predictor at the time. The first step label is empty because there was not any predictor, it was an intercept-only model."
  },
  {
    "objectID": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-2",
    "href": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-2",
    "title": "Compare multiple models",
    "section": "Backward, and Forward variable selection: Building models",
    "text": "Backward, and Forward variable selection: Building models\n\nA plot or multiple plots are always useful to interpret the results of the final best model:\n\n\nlibrary(marginaleffects)\nlibrary(patchwork)\n\n\nAttaching package: 'patchwork'\n\n\nThe following object is masked from 'package:MASS':\n\n    area\n\nfinalModel &lt;- lm(depreTotal ~ ruminTotal+\n                   anxTotal+\n                   negativeBeliefs+\n                   attention+\n                   activation, \n                 data = ruminDataNoMissing)\n\n\nrumPlot &lt;- plot_predictions(finalModel, \n                       condition = c(\"ruminTotal\"),\n                 points = 0.5)\n\nanxPlot &lt;- plot_predictions(finalModel, \n                       condition = c(\"anxTotal\"),\n                 points = 0.5)\n\nnegativePlot &lt;- plot_predictions(finalModel, \n                       condition = c(\"negativeBeliefs\"),\n                 points = 0.5)\n\nattentionPlot &lt;- plot_predictions(finalModel, \n                       condition = c(\"attention\"),\n                 points = 0.5)\n\nactivationPlot &lt;- plot_predictions(finalModel, \n                       condition = c(\"activation\"),\n                 points = 0.5)\n\n(rumPlot + anxPlot + negativePlot + attentionPlot)  + activationPlot"
  },
  {
    "objectID": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-3",
    "href": "lecture10/ModelComparasion.html#backward-and-forward-variable-selection-building-models-3",
    "title": "Compare multiple models",
    "section": "Backward, and Forward variable selection: Building models",
    "text": "Backward, and Forward variable selection: Building models\n\nThe backwards procedure, will start with a full model and then, we can reduce the model based on the AIC measure. In the function stepAIC() you have to change direction = \"forward\" for direction = \"backwards\".\nFollowing the previous example we can start with the full model:\n\n\n\nShow the code\nfullModel &lt;- lm(depreTotal ~ ruminTotal+\n                   anxTotal+\n                   negativeBeliefs+\n                   positiveBeliefs+\n                   attention+\n                   activation, \n1                 data = ruminDataNoMissing)\n\n\n\n1\n\nThe full model has all the predictors related to depression. The model will be reduced later.\n\n\n\n\n\nThen, we can compare all the possible models using the function stepAIC():\n\n\n\nShow the code\nbackwardsSelection &lt;- stepAIC(interceptModel,\n                          scope = list(upper = ~ruminTotal+anxTotal+negativeBeliefs+positiveBeliefs+attention+activation, \n                                       lower = ~1), \n                          direction = \"backward\",\n                          trace = FALSE)\n\n\n\ngt(tidy(backwardsSelection),\n   rownames_to_stub= FALSE) |&gt;\n  fmt_number(decimals = 2) |&gt;\n  tab_header(title = \"Final model selected by stepAIC backwards method0\")\n\n\n\n\n\n  \n    \n      Final model selected by stepAIC backwards method0\n    \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n    \n  \n  \n    (Intercept)\n10.34\n0.45\n23.10\n0.00"
  },
  {
    "objectID": "lecture9/AssumptionsAndMore.html#references",
    "href": "lecture9/AssumptionsAndMore.html#references",
    "title": "Correlation and Regression Models",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nField, A., Miles, J., & Field, Z. (2012). Discovering statistics using r. Sage publications.\n\n\nWestfall, P. H., & Arias, A. L. (2020). Understanding regression analysis: A conditional distribution approach. Chapman; Hall/CRC."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html",
    "href": "Lecture1Psyc4310/researchJourney.html",
    "title": "The journey to the begining",
    "section": "",
    "text": "To give an overview of the next steps.\nHighlight the importance of finding reliable scientific resources.\nKeywords to start searching for articles.\nLiterature review and how to make it work."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#aims-in-this-lecture",
    "href": "Lecture1Psyc4310/researchJourney.html#aims-in-this-lecture",
    "title": "The journey to the begining",
    "section": "Aims in this lecture",
    "text": "Aims in this lecture\n\nTo give an overview of the next steps.\nHighlight the importance of finding reliable scientific resources.\nKeywords to start searching for articles.\nLiterature review and how to make it work."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#lets-start-with-a-big-picture",
    "href": "Lecture1Psyc4310/researchJourney.html#lets-start-with-a-big-picture",
    "title": "The journey to the begining",
    "section": "Let’s start with a big picture",
    "text": "Let’s start with a big picture\n\n\n\n\n\nPSYC4310researchProject"
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#how-do-we-start-searching-research-ideas",
    "href": "Lecture1Psyc4310/researchJourney.html#how-do-we-start-searching-research-ideas",
    "title": "The journey to the begining",
    "section": "How do we start searching research ideas ?",
    "text": "How do we start searching research ideas ?\n\nWhere should I start to get better research ideas?\n\nThe answer is always to read more and more!\nWhere are we going to read and find information?\n\nWe will read always SCIENTIFIC articles, keep reading this lecture to know more!\n\n\n\n\n\n\n\nTip\n\n\nSomebody once said: Good ideas are many times the product of the lack of reading."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#where-are-the-scientific-articles-located",
    "href": "Lecture1Psyc4310/researchJourney.html#where-are-the-scientific-articles-located",
    "title": "The journey to the begining",
    "section": "Where are the scientific articles located?",
    "text": "Where are the scientific articles located?\n\n\n\nMany sources on Internet will claim that they provide scientific content. However, not all the content is a good fit for this course.\nFor instance, we can check the website Psychology Today. This website has many articles related to diverse topics in psychology, and many times the authors review scientific articles and translate them into a more digestible narrative. For example, in the article titled “Why It’s Hard to Break Up with a Bad Boyfriend or Girlfriend” the author presents scientific terms such as variable ratio reinforcement schedule.\n\n\n\nIs this article a scientific article? How can we know? Let me dissect this creature in the next slide."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#non-scientific-articles-anatomy",
    "href": "Lecture1Psyc4310/researchJourney.html#non-scientific-articles-anatomy",
    "title": "The journey to the begining",
    "section": "Non-scientific article’s anatomy",
    "text": "Non-scientific article’s anatomy\n\n\n\n\n\nDon’t use this type of source in this course\n\n\n\n\nThe title is written to be a “click-bait”. This means, it was meant to be catchy and easy to click on.\nThere is not an abstract.\nThe article does not have a methods section, results, and discussion.\nThe review process is not double-blinded (I’ll explain later)\nThe pictures are just decorative, they don’t provide information.\nThe narrative in this article tries to be fun to read.\nIt is too brief and takes information from many sources without citations."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#lets-revise-a-scientific-article",
    "href": "Lecture1Psyc4310/researchJourney.html#lets-revise-a-scientific-article",
    "title": "The journey to the begining",
    "section": "Let’s revise a scientific article",
    "text": "Let’s revise a scientific article\n\n\n\nWe can check the article: “Long-term effects of COVID-19 on mental health: A systematic review” as a good example of scientific article.\nThis article shows the following elements:\nThe name of the scientific journal (Journal of Effective Disorders).\nThe layout has Introduction, Methods, Results, and Discussion.\nThe journal was published by a scientific publisher, in this case; Elsevier.\n\n\n\n\n\n\n\n\nNot, all scientific journals are published by a publisher. The journal might be published by private or public universities.\nThe authors included citations, references, and abstract.\nThe figures and tables are meaningful to understand the information.\nThe title is not a click-bait."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#how-a-scientific-article-is-born",
    "href": "Lecture1Psyc4310/researchJourney.html#how-a-scientific-article-is-born",
    "title": "The journey to the begining",
    "section": "How a scientific article is born?",
    "text": "How a scientific article is born?\n\n\n\n\nUntitled Diagram"
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#how-a-scientific-article-is-born-2",
    "href": "Lecture1Psyc4310/researchJourney.html#how-a-scientific-article-is-born-2",
    "title": "The journey to the begining",
    "section": "How a scientific article is born? (2)",
    "text": "How a scientific article is born? (2)\n\nThe key element in scientific publishing is the peer review step.\nIn scientific journals, the scientist who submits an article for publication MUST not know who is the reviewer or reviewers. At the same time, the reviewers MUST not know who is or who are the authors of the article under review.\nWe call this a double-blinded review system. But, as you might imagine this system has been criticized, see this publication, and more in this article."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#where-can-i-download-scientific-articles",
    "href": "Lecture1Psyc4310/researchJourney.html#where-can-i-download-scientific-articles",
    "title": "The journey to the begining",
    "section": "Where can I download scientific articles?",
    "text": "Where can I download scientific articles?\n\nYou could use Google scholar. But not all the articles are free to download. But, at least you could get the reference from there and after that, you can go to the CSU Stan library website.\nOn the CSU Stan library website you’ll find a search bar where you can fetch scientific articles, and download the articles in pdf format."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#generate-keywords",
    "href": "Lecture1Psyc4310/researchJourney.html#generate-keywords",
    "title": "The journey to the begining",
    "section": "Generate Keywords",
    "text": "Generate Keywords\n\nIt is common that at the beginning of a research project you don’t know specific keywords. That’s OK.\nYou can start with a broad idea. For instance, if you are interested in romantic relationships. You can say to yourself:\n\nMy first keywords are: “romantic relationships”\nBut, when? When people are teenagers, adults, or aging adults? heterosexual or gay relationships?\nYou may pick teenagers, now you have an additional keyword.\nYou may also pick heterosexual relationships.\nYou can try: romantic relationships, teenagers, adolescence, heterosexual.\n\nIn your first wide search you may find other possible words such as: sexual roles, stereotypes, aggression. or misconceptions in romantic relationships. These words can also be added to your search.\n\n\n\n\n\n\n\nTip\n\n\nWhen you are looking for keywords you are in an early exploration stage. You need to read and write down the keywords that are more frequent in your search."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#generate-keywords-2",
    "href": "Lecture1Psyc4310/researchJourney.html#generate-keywords-2",
    "title": "The journey to the begining",
    "section": "Generate Keywords (2)",
    "text": "Generate Keywords (2)\n\nYour keywords will be more specific once you define what is your research question.\nIn my example, thanks to my keywords I could formulate the question: What is the influence of sexual stereotypes in romantic relationships in teenagers?\nIt is still a very big question, but it will get refine when you read more. You may find a specific theory that addresses this issue, then you’ll start using the theory’s name as your main keyword or set of keywords.\nFor example, if you study romantic relationship you may come across attachment theory. Then, attachment theory will become a more specific set of keywords.\n\n\n\n\n\n\n\nImportant\n\n\nThe main objective is to find more specific keywords. If after a while you cannot find more specific keywords you are not reading enough or you are looking at the wrong information."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#literature-review-what-is-it",
    "href": "Lecture1Psyc4310/researchJourney.html#literature-review-what-is-it",
    "title": "The journey to the begining",
    "section": "Literature review, what is it?",
    "text": "Literature review, what is it?\n\n\n\nLiterature reviews sound boring, I know. But, this stage is crucial to select a topic you liked.\nResearch is not boring if you select something you enjoy reading. The literature review will help you to find a fun topic for you.\nAfter knowing your keywords, you can start reviewing articles.\nYou may create small summaries for each article where you extract the information that you need to prepare a narrative related to the state of art in your topic."
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#literature-review-what-is-it-2",
    "href": "Lecture1Psyc4310/researchJourney.html#literature-review-what-is-it-2",
    "title": "The journey to the begining",
    "section": "Literature review, what is it? (2)",
    "text": "Literature review, what is it? (2)\n\nIn the literature review phase, you will find different types of research articles:\n\n\n\n\n\nflowchart LR\n    338574[\"Articles\"] --- 497165[\"Empirical\"]\n    338574 --- 407950[\"Reviews\"]\n    497165 --- 525687[\"Experiments\"]\n    497165 --- 226158[\"Non-experiments\"]\n    497165 --- 362031[\"Psychometrics/Methods\"]\n    226158 --- 275202[\"Qualitative \"]\n    226158 --- 586082[\"Quantitative \"]\n    275202 --- 359947[\"Interviews\"]\n    275202 --- 648543[\"Focus groups\"]\n    275202 --- 929715[\"Case Studies\"]\n    275202 --- 727734[\"Ethnographies\"]\n    275202 --- 730820[\"Content Analysis\"]\n    275202 --- 229817[\"Observations\"]"
  },
  {
    "objectID": "Lecture1Psyc4310/researchJourney.html#references",
    "href": "Lecture1Psyc4310/researchJourney.html#references",
    "title": "The journey to the begining",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html",
    "href": "Lecture2Psyc4310/ResearchDesign.html",
    "title": "Designing your research project",
    "section": "",
    "text": "Brief refresher on research designs.\nWhat is a dependent variable?\nWhat is a independent variable?\nHypothesis creation."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#aims-in-this-lecture",
    "href": "Lecture2Psyc4310/ResearchDesign.html#aims-in-this-lecture",
    "title": "Designing your research project",
    "section": "Aims in this lecture",
    "text": "Aims in this lecture\n\nBrief refresher on research designs.\nWhat is a dependent variable?\nWhat is a independent variable?\nHypothesis creation."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#the-big-picture-again",
    "href": "Lecture2Psyc4310/ResearchDesign.html#the-big-picture-again",
    "title": "Designing your research project",
    "section": "The big picture again",
    "text": "The big picture again\n\n\n\n\n\nPSYC4310researchProject"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#big-research-families",
    "href": "Lecture2Psyc4310/ResearchDesign.html#big-research-families",
    "title": "Designing your research project",
    "section": "Big research families",
    "text": "Big research families\n\nAlternative research designs (Creswell & Creswell, 2017)\n\n\n\n\n\n\n\nQuantitative\nQualitative\nMixed Methods\n\n\n\n\nExperimental designs\nNarrative Research\nConvergent\n\n\nNon-experimental\nPhenomenology\nExplanatory sequential\n\n\nLongitudinal Designs\nGrounded Theory\nExploratory sequential\n\n\n\nEthnographies\nComplex designs with embedded core designs\n\n\n\nCase Study"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#big-research-families-iii",
    "href": "Lecture2Psyc4310/ResearchDesign.html#big-research-families-iii",
    "title": "Designing your research project",
    "section": "Big research families III",
    "text": "Big research families III\nQualitative designs\n\nNarrative research: The information retold or restoried by the researcher into a narrative chronology. Often, in the end, the narrative combines views from the participant’s life with those of the researcher’s life in a collaborative narrative\nPhenomenological research: the researcher describes the lived experiences of individuals about a phenomenon as described by participants.\nGrounded theory: is a design of inquiry from sociology in which the researcher derives a general, abstract theory of a process, action, or interaction grounded in the views of participants.\nEthnography: is a design of inquiry coming from anthropology and sociology in which the researcher studies the shared patterns of behaviors, language, and actions of an intact cultural group in a natural setting over a prolonged period of time.\nCase studies: in-depth analysis of a case, often a program, event, activity, process, or one or more individuals."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#what-is-first-a-or-b",
    "href": "Lecture2Psyc4310/ResearchDesign.html#what-is-first-a-or-b",
    "title": "Designing your research project",
    "section": "What is first A or B ?",
    "text": "What is first A or B ?\n\nCausality means that we would expect variable X to cause variable Y.\n\nFor example: Does low self esteem cause depression? How do we know?\n\n\nLet’s take a look at some spurious correlations:"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#experiments-help-us",
    "href": "Lecture2Psyc4310/ResearchDesign.html#experiments-help-us",
    "title": "Designing your research project",
    "section": "Experiments help us!",
    "text": "Experiments help us!\n\nCan we know if A causes B with a survey?\nCan we know if A causes B conducting an experiment?\nWe can manipulate a variable and observe what happens afterwards, but it is good enough?\nDo we need something more on our design?"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#experiments-help-us-ii",
    "href": "Lecture2Psyc4310/ResearchDesign.html#experiments-help-us-ii",
    "title": "Designing your research project",
    "section": "Experiments help us! II",
    "text": "Experiments help us! II\n\nPure experiments need a control group to account for counterfactual information, this also helps to rule out possible confounding variables.\n\nExample: how would you measure the effect of physical activity on cardiovascular fitness? What would be a good experimental design?"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#can-we-assume-causality-in-survey-designs",
    "href": "Lecture2Psyc4310/ResearchDesign.html#can-we-assume-causality-in-survey-designs",
    "title": "Designing your research project",
    "section": "Can we assume causality in survey designs ?",
    "text": "Can we assume causality in survey designs ?\n\nIn survey designs we cannot manipulate the independent variable, but some researchers claim that is possible to make causal inferences when you conduct a longitudinal study.\nIn longitudinal studies you satisfy the temporal requirement, you could evaluate if X = independent variable happens before Y = dependent variable.\n\nFor instance: You could measure a baby’s weight every month and evaluate how many times the baby is breastfed. But, do we need counterfactual information?"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#but-we-havent-defined-cause-and-effect-iii",
    "href": "Lecture2Psyc4310/ResearchDesign.html#but-we-havent-defined-cause-and-effect-iii",
    "title": "Designing your research project",
    "section": "But we haven’t defined cause, and effect III",
    "text": "But we haven’t defined cause, and effect III\nWhat is an effect?\n\nAn effect is better define if we have a counterfactual model.\nA counterfactual is something that is contrary to fact.\nIn an experiment we observe what did happen when people received a treatment.\nThe counterfactual is knowledge of what would have happened to those same people if they simultaneously had not received treatment. An effect is the difference between what did happen and what would have happened.\n\n\n\n\n\n\n\nImportant\n\n\nWe could add a group of participants to a waiting list, do you have any example in mind?"
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#lets-finally-define-causal-relationship",
    "href": "Lecture2Psyc4310/ResearchDesign.html#lets-finally-define-causal-relationship",
    "title": "Designing your research project",
    "section": "Let’s finally define causal relationship",
    "text": "Let’s finally define causal relationship\nShadish et al. (2002) :\n\nThis definition was first coined by John Stuart Mill (19th-century philosopher), a causal relationship exists if:\n\nThe cause preceded the effect.\nThe cause was related to the effect.\nWe can find no plausible alternative explanation for the effect other than the cause.\n\n\n\n\n\n\n\n\nWarning\n\n\nCorrelation does not prove causation!!! We will use this as a mantra in this class."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#independent-variable",
    "href": "Lecture2Psyc4310/ResearchDesign.html#independent-variable",
    "title": "Designing your research project",
    "section": "Independent variable",
    "text": "Independent variable\n\nIndependent variables: are those that influence, or affect outcomes in experimental studies. They are described as “independent” because they are variables that are manipulated in an experiment and thus independent of all other influences.\nHowever, we will use this concept more vaguely, we won’t use it only when talking about experiments. It will be used also for correlational relationships, formally its name in survey designs is predictor."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#dependent-variable",
    "href": "Lecture2Psyc4310/ResearchDesign.html#dependent-variable",
    "title": "Designing your research project",
    "section": "Dependent variable",
    "text": "Dependent variable\n\nDependent variables: are those that depend on the independent variables; they are the outcomes or results of the influence of the independent variables. It is also called outcome in survey designs or correlation designs."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#moderating-variables",
    "href": "Lecture2Psyc4310/ResearchDesign.html#moderating-variables",
    "title": "Designing your research project",
    "section": "Moderating variables",
    "text": "Moderating variables\nModerating variables are predictor variables that affect the direction and/or the strength of the relationship between independent and the dependent variable."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#mediating-variables",
    "href": "Lecture2Psyc4310/ResearchDesign.html#mediating-variables",
    "title": "Designing your research project",
    "section": "Mediating variables",
    "text": "Mediating variables\n\nMediating variables stand between the independent and dependent variables, and they transmit the effect of an independent variable on a dependent variable."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#what-is-a-hypothesis",
    "href": "Lecture2Psyc4310/ResearchDesign.html#what-is-a-hypothesis",
    "title": "Designing your research project",
    "section": "What is a hypothesis?",
    "text": "What is a hypothesis?\n\nThe scientific method proposes that hypotheses are an important component to gather insights about nature.\nA hypothesis is an statement regarding what we believe might be happening in nature, or beliefs about what has happened in nature.\nA hypothesis can be for instance the belief that it rained at night when you wake up the next day and you see wet grass. You may create the statement:\n\n“The last night it rain because I can see the grass is wet”\n\nBut wait, is there an alternative possible explanation? Yes there is:\n\n“The last night I saw the grass wet, I believe the sprinklers were on last night”\n\nThese are observations about nature, and they can be hypotheses. Let’s make them more hypothesis-like:\n\n“The grass is wet because it rained last night”\n“The grass is wet because the sprinklers were turned on last night”\n-These are statements that you can test and see if they are true or false. How? You may wake up earlier the next morning and check if the sprinklers were working.\n\nBut wait! One observation might not be enough, you don’t know at what time the sprinklers are working if they are really working. Then, you must wake up every morning at different times and record if the sprinklers were working or not. After several mornings, you may find that the sprinklers were a possible cause. Why possible? Because you still don’t know with this method, if the first time you saw the wet grass it rained. And here is where we need to precise different research methods."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#what-is-a-hypothesis-2",
    "href": "Lecture2Psyc4310/ResearchDesign.html#what-is-a-hypothesis-2",
    "title": "Designing your research project",
    "section": "What is a hypothesis? (2)",
    "text": "What is a hypothesis? (2)\n\nTalking about grass is not fun, and it is simplistic. It is better if we make more realistic hypotheses in psychology.\nAn example is happiness in married couples. Many researchers have found out that married people report more happiness compare to single people. See this article as example: Does marriage make people happy, or do happy people get married?\n\n\n\n\nLet’s try to generate a hypothesis:\n\nHappy people has a tendency to get married\n\nOr we could state the following hypothesis:\n\nCouples tend to be happier because they are married\n\nWe could also do the following statement:\n\nThere is a correlation between happiness and being married\n\n\n\n\n\n\n\nIf you read carefully you realized that all these statements are measurable, and they can be rejected. Also notice that the second hypothesis is a causal hypothesis, while the other two hypotheses are correlational hypotheses.\nWe need to create hypothesis that are falsifiable."
  },
  {
    "objectID": "Lecture2Psyc4310/ResearchDesign.html#references",
    "href": "Lecture2Psyc4310/ResearchDesign.html#references",
    "title": "Designing your research project",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nCreswell, J. W., & Creswell, J. D. (2017). Research design: Qualitative, quantitative, and mixed methods approaches. Sage publications.\n\n\nShadish, W. R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Houghton, Mifflin; Company."
  },
  {
    "objectID": "PSYC4310.html#supporting-materials",
    "href": "PSYC4310.html#supporting-materials",
    "title": "PSYC4310: Seminar in Social Psychology",
    "section": "",
    "text": "How to use SONA?"
  },
  {
    "objectID": "SONA/sonaTutorial.html",
    "href": "SONA/sonaTutorial.html",
    "title": "SONA Tutorial",
    "section": "",
    "text": "SONA is an online software created by SONA Systems, several universities and private research companies use similar software to keep track of research participants and their compensation after participating in a study.\nThe Department of Psychology and Child Development has its own instance using this software, you may log in by going to the portal on this link.\nIf you are taking my class, it is likely I already added you to SONA’s database, therefore you will be able to create new studies on SONA.\nI will show you on this tutorial how to create a new study on SONA.\n\n\nGo to the psychology log in web page by clicking here.\nThe webpage should look like the picture below:\n\n\n\n\nType your CSU Stan email as your User ID, and the password you set before. You may have recieved an email with a random temporal password. You may continue using the random password or change it to a more friendly and secure password.\n\n\n\nYou will see a page similar to the one showed next. But, in your case it should say that you have two roles, one role is participant and the other is researcher. Select your role as researcher to start creating your new study. In my case, I have two roles, I’m administrator and principal investigator.\n\n\n\n\nOnce you select your role as researcher , you will see the following page:\n\nClick on “Create a new study” as showed in the picture.\n\n\n\nYou will see a new page after clicking on “Create a new study”. In this new page, you will see multiple options. In most cases, the right option is “Online External Study” as shown in the image below:\n\nSelect the option that says “Credit”, we don’t have money to pay for participation . After that, click on “Continue”\n\n\n\nIn this step you will have to type specific information related to your research project:\n\nStudy Name\nBrief Abstract\nDetailed Description\nEligibility Requirements: add requirement such as minimum age to participante.\nDuration in minutes (write how long it takes to finish the survey)\nHow many credits will the participant earn after participating in your study? (You can specify 1 credit).\nPreparation is optional. Keep it blank if you don’t need to add information in this section.\nResearcher: Look for my name and your group members in the list. Double click on the names to add them to the study as researchers. If you have the option to specify a principal investigator you can add me there.\nYOU MUST add your IRB approval number.\nYOU MUST add your IRB approval expiration.\nApproved? This is an option administrated by the general admin. This means that another faculty who is the general admin has to approve your project.\nStudy URL: you should add your Qualtrics link in this section.\n\nYou may ignore the rest of the options and keep the default values.\n\n\n\nAfter creating the study, you should click on “Study Menu” , and then click on “View/Administer Time Slots”:\n\n\n\n\nNow click on “Add a Timeslot”:\n\n\n\n\nFinally, set a Final Participation Date, normally the last day of classes should be appropriate. Set a final time, and type the maximum number of participants. Add a large number such as 300 participants. You will not get 300 students but it is just better if you give a large number to the software.\n\n\n\n\nAfter you finish all these steps, your project will be ready to be delivered on SONA but, there is an extra step: you have to ask for approval to the general admin, you will make the request by clicking: “Send request”. The admin will return the decision with comments if there are problems with the submission."
  },
  {
    "objectID": "SONA/sonaTutorial.html#description",
    "href": "SONA/sonaTutorial.html#description",
    "title": "SONA Tutorial",
    "section": "",
    "text": "SONA is an online software created by SONA Systems, several universities and private research companies use similar software to keep track of research participants and their compensation after participating in a study.\nThe Department of Psychology and Child Development has its own instance using this software, you may log in by going to the portal on this link.\nIf you are taking my class, it is likely I already added you to SONA’s database, therefore you will be able to create new studies on SONA.\nI will show you on this tutorial how to create a new study on SONA.\n\n\nGo to the psychology log in web page by clicking here.\nThe webpage should look like the picture below:\n\n\n\n\nType your CSU Stan email as your User ID, and the password you set before. You may have recieved an email with a random temporal password. You may continue using the random password or change it to a more friendly and secure password.\n\n\n\nYou will see a page similar to the one showed next. But, in your case it should say that you have two roles, one role is participant and the other is researcher. Select your role as researcher to start creating your new study. In my case, I have two roles, I’m administrator and principal investigator.\n\n\n\n\nOnce you select your role as researcher , you will see the following page:\n\nClick on “Create a new study” as showed in the picture.\n\n\n\nYou will see a new page after clicking on “Create a new study”. In this new page, you will see multiple options. In most cases, the right option is “Online External Study” as shown in the image below:\n\nSelect the option that says “Credit”, we don’t have money to pay for participation . After that, click on “Continue”\n\n\n\nIn this step you will have to type specific information related to your research project:\n\nStudy Name\nBrief Abstract\nDetailed Description\nEligibility Requirements: add requirement such as minimum age to participante.\nDuration in minutes (write how long it takes to finish the survey)\nHow many credits will the participant earn after participating in your study? (You can specify 1 credit).\nPreparation is optional. Keep it blank if you don’t need to add information in this section.\nResearcher: Look for my name and your group members in the list. Double click on the names to add them to the study as researchers. If you have the option to specify a principal investigator you can add me there.\nYOU MUST add your IRB approval number.\nYOU MUST add your IRB approval expiration.\nApproved? This is an option administrated by the general admin. This means that another faculty who is the general admin has to approve your project.\nStudy URL: you should add your Qualtrics link in this section.\n\nYou may ignore the rest of the options and keep the default values.\n\n\n\nAfter creating the study, you should click on “Study Menu” , and then click on “View/Administer Time Slots”:\n\n\n\n\nNow click on “Add a Timeslot”:\n\n\n\n\nFinally, set a Final Participation Date, normally the last day of classes should be appropriate. Set a final time, and type the maximum number of participants. Add a large number such as 300 participants. You will not get 300 students but it is just better if you give a large number to the software.\n\n\n\n\nAfter you finish all these steps, your project will be ready to be delivered on SONA but, there is an extra step: you have to ask for approval to the general admin, you will make the request by clicking: “Send request”. The admin will return the decision with comments if there are problems with the submission."
  },
  {
    "objectID": "practice2/practice2.html",
    "href": "practice2/practice2.html",
    "title": "Practice 2",
    "section": "",
    "text": "After completing the first practice, you should have learned these key concepts:\n\nR is an object-oriented programming language.\nR is free and open source, and you are able to install as many packages as you want for free.\nYou learned the concept of “package”.\nYou learned about data frames.\n\nIn this practice we will work with data frames , we will create new variables, and we will use extensively pipes along with the package tidyverse.\n\n\n\n\n\n\nGood News!\n\n\n\nIn this practice you will be able to run R code straight from your web browser. Isn’t it awesome!"
  },
  {
    "objectID": "practice2/practice2.html#what-have-you-learned-so-far-in-r",
    "href": "practice2/practice2.html#what-have-you-learned-so-far-in-r",
    "title": "Practice 2",
    "section": "",
    "text": "After completing the first practice, you should have learned these key concepts:\n\nR is an object-oriented programming language.\nR is free and open source, and you are able to install as many packages as you want for free.\nYou learned the concept of “package”.\nYou learned about data frames.\n\nIn this practice we will work with data frames , we will create new variables, and we will use extensively pipes along with the package tidyverse."
  },
  {
    "objectID": "practice2/practice2.html#lets-put-a-frame-on-that-data.",
    "href": "practice2/practice2.html#lets-put-a-frame-on-that-data.",
    "title": "Practice 2",
    "section": "Let’s put a frame on that data….",
    "text": "Let’s put a frame on that data….\nThe data frame is one type of object in R, and it is useful to represent information.\nThis is a webR-enabled code cell in a Quarto HTML document.\n🟡 Loading\n  webR..."
  },
  {
    "objectID": "practice2/practice2.html#the-package-tidyverse-is-your-friend",
    "href": "practice2/practice2.html#the-package-tidyverse-is-your-friend",
    "title": "Practice 2",
    "section": "The package tidyverse is your friend",
    "text": "The package tidyverse is your friend\nIf you run the following code, you’ll see the variable names in the data set ruminationToClean.csv:\n\n\nI’m reading in the data file from my personal repository, fancy right?\n🟡 Loading\n  webR...\n\n\n  \n\n\nAs you can see, we have a large list of variables. We are using the function colnames() to get a list of column names, which in this case are research variables. Several of these variables could be dropped from the data set, others will need to be renamed.\n\nDropping variables\nIn this example we will use the function select() from the package tidyverse. Remember that you’ll need to call the package by running library(tidyverse).\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nWhat is happening here? You may have noticed that inside the function selection() I am adding the name of the variable I want to delete from the data set, and then I added a minus sign. When you add a minus sign before the name of the variable, the package will delete the variable that you list with a minus sign. You can do the same with several variables at the same time:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nNow the variables, age, school, and grade are not longer in the dataset, if you don’t believe me click on RUN CODE.\n\n\n\n\n\n\nExcercise 1 (30 points)\n\n\n\n\nDownload the data set ruminationToClean.csv then open the data file using ruminationRaw &lt;- read.csv(file.choose()) or you may copy the code to open the data from the online repository. After opening the data in R, delete from the data set all the variables that start with th stem EATQ_R_.\n\n\n\n\n\nChange variable names\nIn some occasions, the research variable was named wrongly or the name does not reflect the content of the variable. In the data set ruminationToClean.csv there are variables with names in Spanish, we can change them:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nIn this example, you can see that I’m using the function rename() from the package dplyr which is inside the package tidyverse. When you use rename() you’ll need to declare the new name first and then the old variable name that will be replaced.\n\n\n\n\n\n\nExcercise 2 (35 points)\n\n\n\n\nIn psychological measurement it is common to implement questionnaires with Likert-response options. We typically sum the items corresponding to the latent variable or construct that we are measuring. For example in the ruminationToClean.csv the construct of “Rumination” is measured by the Children’s Response Styles Questionnaire (CRSQ), however the variable names are not self-explained. Your task is to change the name of the variables with stem CRSQ, the new variables should have the stem rumination_itemNumber. For example rename(CRSQ1=rumination_1). Remember to save the changes in a new object.\n\n\n\n\n\nCompute a new variable\nAs always in programming languages, there are several ways to create a new research variable in a data frame. But, we will use the tidyverse method by using a function that actually works like a verb. The function mutate() will help use to create new variables.\nI mentioned in Exercise 2 psychology studies latent variables also known as hypothetical constructs, they have many names, but at the end we study variables that are implicit; we can measure them but we don’t see them. For example, we have studied variables such depression or rumination in this course, these are latent variables because you cannot capture depression or you don’t see it walking, however you are able to measure depression by asking about symptoms and thoughts.\n\n\n\nExample of a latent variable\n\n\nIn the figure above you can see a representation of a latent variable. In psychology we often assume that the latent variable is the common factor underlying our questions, -in this example- questions related to depression.\nWhen we analyze data that corresponds with latent variables we have two options:\n\nWe can create a Structural Equation Modeling (We won’t cover this topic).\nWe sum all the items to compute a total score. This total score will be our approximation to the latent variable. (Yes, this is covered in this practice).\n\n\n\n\nI’ll show an example in the next chunk of R code:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nIn this example we are using the function mutate() to create a new column named depressionTotal, this new depressionTotal column will contain the depression total score for each participant. In this study we included the “Child’s Depression Inventory (CDI)”, that’s why the columns follow the stem “CDI”. You can see the new column in the next table:\n\n\nCode\nruminationRaw &lt;- read.csv(\"ruminationToClean.csv\", na.string = \"99\")\nruminationDepresionScore &lt;- ruminationRaw |&gt;\n  mutate(\n    depressionTotal = rowSums(across(starts_with(\"CDI\")))\n    )\nruminationDepresionScore |&gt; select(starts_with(\"CDI\"),  \n                                   depressionTotal ) |&gt;\n  gt_preview()\n\n\n\n\n\n\n  \n    \n    \n      \n      CDI1\n      CDI2\n      CDI3\n      CDI4\n      CDI5\n      CDI6\n      CDI7\n      CDI8\n      CDI9\n      CDI10\n      CDI11\n      CDI12\n      CDI13\n      CDI14\n      CDI15\n      CDI16\n      CDI17\n      CDI18\n      CDI19\n      CDI20\n      CDI21\n      CDI22\n      CDI23\n      CDI24\n      CDI25\n      CDI26\n      depressionTotal\n    \n  \n  \n    1\n0\n2\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\n15\n    2\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n2\n0\n1\n0\n2\n1\n1\n0\n0\n1\n2\n1\n0\n0\n15\n    3\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n4\n    4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n6\n    5\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n2\n1\n1\n0\n1\n1\n2\n0\n1\n0\n0\n1\n1\n1\n0\n17\n    6..211\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    212\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n5\n  \n  \n  \n\n\n\n\n\n\n\n\n\n\nExcercise 3 (30 points)\n\n\n\n\nIn the data set ruminationToClean.csv you’ll find seven columns with the stem “DASS”. These columns correspond to the instrument named: Depression, Anxiety, and Stress Scale (DASS). This is an scale that measures three latent variables at the same time, but only the anxiety items were added in this study.\n\nYour task is to change the name of the “DASS” items, you should think a more informative name for these items in the data frame. After that, you will have to compute the total score of anxiety for each participant in the data frame.\n\n\n\n\nLet’s cleanse this data set\n\n\n\nIt is always satisfactory to clean a data set and then see the final product, but so far we have cleaned these data in small steps. I’ll clean the data set in one step. In this example I will delete unnecessary columns (aka research variables), and I will translate the variables to English when necessary:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\nExcercise 4 (10 points)\n\n\n\n\nDescribe the steps taken in the R code above."
  },
  {
    "objectID": "PSYC3000.html#geoweek-2023",
    "href": "PSYC3000.html#geoweek-2023",
    "title": "PSYC3000",
    "section": "GeoWeek 2023",
    "text": "GeoWeek 2023\nGentle Introduction to R",
    "crumbs": [
      "Home",
      "Content",
      "PSYC3000"
    ]
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html",
    "href": "geoWeekPresentation/gentleR.html",
    "title": "Gentle introduction to R",
    "section": "",
    "text": "R is a programming language mostly used in statistics. It was created by statisticians.\nR was inspired by the statistical language S developed by At&T. S stands for “statistics” and it was written based on C language. After S was sold to a small company, S-plus was created with a graphical interface."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#what-is-r",
    "href": "geoWeekPresentation/gentleR.html#what-is-r",
    "title": "Gentle introduction to R",
    "section": "What is R?",
    "text": "What is R?\nR is a programming language mostly used in statistics. It was created by statisticians.\nR was inspired by the statistical language S developed by At&T. S stands for “statistics” and it was written based on C language. After S was sold to a small company, S-plus was created with a graphical interface."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#what-is-r-ii",
    "href": "geoWeekPresentation/gentleR.html#what-is-r-ii",
    "title": "Gentle introduction to R",
    "section": "What is R (II)?",
    "text": "What is R (II)?\n\nR was considered a “statistics” language, but nowadays it can perform more tasks. We will see examples where you can create a website, create a dashboard, create a teaching notebook, and presentation slides!\nR also provides multiple options to create graphics and plots. The options are infinite when you use a programming language."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#why-should-we-use-r",
    "href": "geoWeekPresentation/gentleR.html#why-should-we-use-r",
    "title": "Gentle introduction to R",
    "section": "Why should we use R?",
    "text": "Why should we use R?"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#why-should-we-use-r-1",
    "href": "geoWeekPresentation/gentleR.html#why-should-we-use-r-1",
    "title": "Gentle introduction to R",
    "section": "Why should we use R?",
    "text": "Why should we use R?\n\nTIOBE index of R overtime"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#why-should-we-use-r-2",
    "href": "geoWeekPresentation/gentleR.html#why-should-we-use-r-2",
    "title": "Gentle introduction to R",
    "section": "Why should we use R?",
    "text": "Why should we use R?\n\n\n\n\nR is free and open-source software. R is available as Free Software under the terms of the Free Software Foundation’s GNU General Public License.\n\n\n\n\nThe amount of users grow every second.\n\n\n\n\nIt is friendly with non-programmers (You don’t believe me I know…).\n\n\n\n\nThe amount of packages is growing (19985 packages as today).\n\n\n\n\nYou don’t depend on buying a license.\n\n\n\n\nYou can see what is under the hood.\n\n\n\n\nThere are many jobs where R skills are needed.\n\n\n\n\n\n\nYou’ll have access to cutting-edge quantitative methods and models.\n\n\n\n\n\n\n\n\nMore info\n\n\nSee datacamp.com opinion."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#section",
    "href": "geoWeekPresentation/gentleR.html#section",
    "title": "Gentle introduction to R",
    "section": "",
    "text": "R is an interpreted language, that means you don’t need to compile the code. You will need to use a command-line interpreter.\nIt is an object-oriented programming language. It represents the information using virtual objects."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#packages-are-the-key",
    "href": "geoWeekPresentation/gentleR.html#packages-are-the-key",
    "title": "Gentle introduction to R",
    "section": "Packages are the key",
    "text": "Packages are the key\n\nR has several built-in functions but they are not enough to answer all the possible research questions a researcher will have.\nR users support their data analysis using packages that other members of the community developed.\nThese packages are actually software and they can be installed very easily in R. You don’t have to program anything, there are 19 985 packages as today. But of course, you might need to program some routines if your problem is very specific.\nThe packages are all located in a large repository call Comprehensive R Archive Network (CRAN)"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#editors-and-ide",
    "href": "geoWeekPresentation/gentleR.html#editors-and-ide",
    "title": "Gentle introduction to R",
    "section": "Editors and IDE",
    "text": "Editors and IDE\n\nMost famous IDE (Integrated Development Environment):\n\nRStudio\nVisual Studio Code\n\nNot so famous but still powerful and full open source:\n\nEMACS\nVIM"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#types-of-objects",
    "href": "geoWeekPresentation/gentleR.html#types-of-objects",
    "title": "Gentle introduction to R",
    "section": "Types of objects",
    "text": "Types of objects\n\nObjects in R have properties and names, similar to real objects:\n\nvectors\ndata frame\nlists\narrays\nfunctions\n\nThese are just the most common objects in R. I’ll explain a little bit of each one."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#vectors",
    "href": "geoWeekPresentation/gentleR.html#vectors",
    "title": "Gentle introduction to R",
    "section": "Vectors",
    "text": "Vectors\n\nIt is the most basic object, it is the bones of R.\nIn human language, they look like lists of elements. But, when mixed different type of data (letters mixed with numbers) things get messy:\n\n\n### Let's create a vector with names:\n\nrandomNames &lt;- c(\"Randall\", \"Pablo\", \"Emma\")\n\nprint(randomNames) #You don't need to type print. This is for teaching purposes. \n\n[1] \"Randall\" \"Pablo\"   \"Emma\"   \n\n\nLet’s see what happen’s when I mix numbers and letters:\n\nnumbersNames &lt;- c(\"one\",1, 2, \"two\", 3, \"three\")\nprint(numbersNames)\n\n[1] \"one\"   \"1\"     \"2\"     \"two\"   \"3\"     \"three\"\n\n\nR coerces everything to be a string or character vector.\n\nYou may also subset a vector by using [] as an index indicator\n\n\nnumbersNames[4]\n\n[1] \"two\""
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#data-frames",
    "href": "geoWeekPresentation/gentleR.html#data-frames",
    "title": "Gentle introduction to R",
    "section": "Data frames",
    "text": "Data frames\n\nData frame is the most useful type of object when you conduct data analysis.\nA data frame is several lists combined together, and it looks pretty much like a matrix or a spreadsheet:\n\n\n\nShow the code\nmtcars\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4    2\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3    4\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3    4\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3    4\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3    2\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3    2\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3    4\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3    2\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5    8\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4    2"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#section-1",
    "href": "geoWeekPresentation/gentleR.html#section-1",
    "title": "Gentle introduction to R",
    "section": "",
    "text": "Let’s beautify the data frame output:\n\n\nShow the code\nlibrary(DT)\ndatatable(mtcars,\n          rownames = TRUE)"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#lists",
    "href": "geoWeekPresentation/gentleR.html#lists",
    "title": "Gentle introduction to R",
    "section": "Lists",
    "text": "Lists\n\nLists are flexible and easy to manipulate in R. You can combine different types of objects in a single list:\n\n\n\nShow the code\n### Let's create different types of objects\n\n### Data frame\n\ndata_1 &lt;- data.frame(v1= rnorm(8),\n                     v2 = rnorm(8),\n                     v3 = rnorm(8))\n\n### Vector\n\nmoreNames &lt;- c(\"Bob\", \"Paris\", \"Ana\")\n\n### Numeric vector\n\nnumericVector &lt;- c(1,3,78,90)\n\n### We can group all these objects in a list\n\nlistOfObjects &lt;- list(data_1,\n                      moreNames,\n                      numericVector)\nprint(listOfObjects)\n\n\n[[1]]\n          v1         v2           v3\n1 -1.3846711  1.1342249  0.005603773\n2 -0.6052452  1.7357052  0.157493762\n3  1.1978363 -0.7972630  0.926481685\n4  0.6755754  0.6163520  0.439208389\n5 -1.0040656 -0.7112245 -0.668112481\n6 -1.0667921 -0.4581647  0.228805642\n7  2.1765713  0.7218299 -0.503185122\n8  0.7557314  1.5541713 -0.714680298\n\n[[2]]\n[1] \"Bob\"   \"Paris\" \"Ana\"  \n\n[[3]]\n[1]  1  3 78 90\n\n\nIf you need to access one object in the list you may use its location plus [[]]:\n\n\nShow the code\nlistOfObjects[[2]]\n\n\n[1] \"Bob\"   \"Paris\" \"Ana\""
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#arrays",
    "href": "geoWeekPresentation/gentleR.html#arrays",
    "title": "Gentle introduction to R",
    "section": "Arrays",
    "text": "Arrays\n\nI don’t use arrays in my code, but they are common in in R and other languages.\nAn arrays is a multidimensional object, you can have multiple “slices” of information in on single object.\nIt is similar to a multi-layer object.\n\n\narray(c(matrix(1:4,2,2)), dim=c(2,2,3))\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 3\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#functions",
    "href": "geoWeekPresentation/gentleR.html#functions",
    "title": "Gentle introduction to R",
    "section": "Functions",
    "text": "Functions\n\nA function is a data object that requires input information, in return; it will give an output.\nI have already used several functions (e.g. data.frame(), rnorm()).\nFunctions will always follow the following structure:\n\n\nmyFunction &lt;- function(argument1, argument2, ...){ \n  \n  operation\n  \n  return()\n  \n  }\n\n\nWe can study the following case, where I created a function to estimate your age:\n\n\n\nShow the code\nestimateAge &lt;- function(myBirthday) {\n  ### Function to check if year is a\n  ### leap year.\n  \n  leapyear &lt;- function(year) {\n    return(((year %% 4 == 0) & (year %% 100 != 0)) | (year %% 400 == 0))\n  }\n  \n  ### Information necessary to compute age\n  myBirthday2 &lt;- as.Date(myBirthday)\n  today &lt;- Sys.Date()\n  year &lt;- as.numeric(format(myBirthday2, \"%Y\"))\n  leapCheck &lt;- leapyear(year)\n  \n  \n  if (leapCheck == TRUE) {\n    ## leap year\n    age &lt;- difftime(today,\n                    myBirthday2 ,\n                    units = \"days\") / (365 + 1)\n    \n  } else {\n    ## No leap year\n    age &lt;- difftime(today,\n                    myBirthday2,\n                    units = \"days\") / 365\n    \n  }\n  \n  message(\"Your age is\",\" \", age)\n}\n\n\n\nMy function estimateAge() requires only one argument myBirthday, that argument is passed to the computation inside the function to estimate the age.\n\n\n## Let's enter my date of birth\nestimateAge(\"1986-01-28\") \n\nYour age is 37.8027397260274\n\n\n\nBut don’t worry, you don’t have to compute age like I did. There is already a package that has all the tools to manipulate dates. It is the package lubridate."
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#shinyapps",
    "href": "geoWeekPresentation/gentleR.html#shinyapps",
    "title": "Gentle introduction to R",
    "section": "ShinyApps",
    "text": "ShinyApps\n\nWatch Crime\nWater quality dashboard\nInteractive data visualization\nSuper Zips"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#web-pages",
    "href": "geoWeekPresentation/gentleR.html#web-pages",
    "title": "Gentle introduction to R",
    "section": "Web Pages",
    "text": "Web Pages\n- Andrew Heiss\n- Quantum Jitter\n- Ella Kaye\n- Books\n- University Course"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#scientific-articles-and-reports-in-pdf",
    "href": "geoWeekPresentation/gentleR.html#scientific-articles-and-reports-in-pdf",
    "title": "Gentle introduction to R",
    "section": "Scientific articles and reports in pdf",
    "text": "Scientific articles and reports in pdf\n- Article\n- Report"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#we-can-also-add-r-code-and-run-it-in-our-websites",
    "href": "geoWeekPresentation/gentleR.html#we-can-also-add-r-code-and-run-it-in-our-websites",
    "title": "Gentle introduction to R",
    "section": "We can also add R code and run it in our websites",
    "text": "We can also add R code and run it in our websites"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#happy-penguins",
    "href": "geoWeekPresentation/gentleR.html#happy-penguins",
    "title": "Gentle introduction to R",
    "section": "Happy penguins",
    "text": "Happy penguins\n\n\nShow the code\n### The rule is to write the packages required by your code at the beginning\n## Packages loaded or called\nlibrary(jpeg)             ## reads pictures into R\nlibrary(patchwork)         ## more tools to add features in a plot\nlibrary(ggplot2)          ## creates plots\nlibrary(palmerpenguins)  ## This package has the penguin data\n\npicture &lt;- \"penguins.jpg\"\nimg &lt;- readJPEG(picture, native = TRUE)\n\n### Plotting the data using ggplot2\n\nggplot(penguins, aes(x = flipper_length_mm, \n                     y= body_mass_g,\n                     color = species)) +\n  geom_point() + \n  geom_smooth(se = FALSE, method = \"lm\" ) + \n  theme_classic() +  \n  xlab(\"Flipper Length in milimeters\")+\n  ylab(\"Body Mass in grams\")+\n  inset_element(p = img,\n                left = 0.05,\n                bottom = 0.65,\n                right = 0.5,\n                top = 0.95)"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#you-can-create-art",
    "href": "geoWeekPresentation/gentleR.html#you-can-create-art",
    "title": "Gentle introduction to R",
    "section": "You can create art",
    "text": "You can create art\n\n\nDanielle Navarro’s blog\n\nBasics on generative art"
  },
  {
    "objectID": "geoWeekPresentation/gentleR.html#gis-and-spatial-data",
    "href": "geoWeekPresentation/gentleR.html#gis-and-spatial-data",
    "title": "Gentle introduction to R",
    "section": "GIS and spatial data",
    "text": "GIS and spatial data\n\n\nMaking Middle Earth maps with R"
  },
  {
    "objectID": "advancedMethods/advancedMethods.html",
    "href": "advancedMethods/advancedMethods.html",
    "title": "Advanced Methods",
    "section": "",
    "text": "Lectures\nLecture 1: Introduction to important concepts in Statistics\nLecture 2: “Probability distributions and random variables”\nLecture 3: Compare means and many more stories\n\n\nR activities\nPractice 1\nPractice 2",
    "crumbs": [
      "Home",
      "Content",
      "Advanced Statistical Methods"
    ]
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html",
    "href": "advancedMethods/practice2/practice2.html",
    "title": "Practice 2",
    "section": "",
    "text": "After completing the first practice, you should have learned these key concepts:\n\nR is an object-oriented programming language.\nR is free and open source, and you are able to install as many packages as you want for free.\nYou learned the concept of “package”.\nYou learned about data frames.\n\nIn this practice we will work with data frames , we will create new variables, and we will use extensively pipes along with the package tidyverse.\n\n\n\n\n\n\nGood News!\n\n\n\nIn this practice you will be able to run R code straight from your web browser. Isn’t it awesome!"
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#the-package-tidyverse-is-your-friend",
    "href": "advancedMethods/practice2/practice2.html#the-package-tidyverse-is-your-friend",
    "title": "Practice 2",
    "section": "The package tidyverse is your friend",
    "text": "The package tidyverse is your friend\nIf you run the following code, you’ll see the variable names in the data set ruminationToClean.csv:\n\n\nI’m reading in the data file from my personal repository, fancy right?\n🟡 Loading\n  webR...\n\n\n  \n\n\nAs you can see, we have a large list of variables. We are using the function colnames() to get a list of column names, which in this case are research variables. Several of these variables could be dropped from the data set, others will need to be renamed.\n\nDropping variables\nIn this example we will use the function select() from the package tidyverse. Remember that you’ll need to call the package by running library(tidyverse).\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nWhat is happening here? You may have noticed that inside the function selection() I am adding the name of the variable I want to delete from the data set, and then I added a minus sign. When you add a minus sign before the name of the variable, the package will delete the variable that you list with a minus sign. You can do the same with several variables at the same time:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nNow the variables, age, school, and grade are not longer in the dataset, if you don’t believe me click on RUN CODE.\n\n\n\n\n\n\nExcercise 1 (7 points)\n\n\n\n\nDownload the data set ruminationToClean.csv then open the data file using ruminationRaw &lt;- read.csv(file.choose()) or you may copy the code to open the data from the online repository. After opening the data in R, delete from the data set all the variables that start with th stem EATQ_R_.\n\n\n\n\n\nChange variable names\nIn some occasions, the research variable was named wrongly or the name does not reflect the content of the variable. In the data set ruminationToClean.csv there are variables with names in Spanish, we can change them:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nIn this example, you can see that I’m using the function rename() from the package dplyr which is inside the package tidyverse. When you use rename() you’ll need to declare the new name first and then the old variable name that will be replaced.\n\n\n\n\n\n\nExcercise 2 (7 points)\n\n\n\n\nIn psychological measurement it is common to implement questionnaires with Likert-response options. We typically sum the items corresponding to the latent variable or construct that we are measuring. For example in the ruminationToClean.csv the construct of “Rumination” is measured by the Children’s Response Styles Questionnaire (CRSQ), however the variable names are not self-explained. Your task is to change the name of the variables with stem CRSQ, the new variables should have the stem rumination_itemNumber. For example rename(CRSQ1=rumination_1). Remember to save the changes in a new object.\n\n\n\n\n\nCompute a new variable\nAs always in programming languages, there are several ways to create a new research variable in a data frame. But, we will use the tidyverse method by using a function that actually works like a verb. The function mutate() will help use to create new variables.\nI mentioned in Exercise 2 psychology studies latent variables also known as hypothetical constructs, they have many names, but at the end we study variables that are implicit; we can measure them but we don’t see them. For example, we have studied variables such depression or rumination in this course, these are latent variables because you cannot capture depression or you don’t see it walking, however you are able to measure depression by asking about symptoms and thoughts.\n\n\n\nExample of a latent variable\n\n\nIn the figure above you can see a representation of a latent variable. In psychology we often assume that the latent variable is the common factor underlying our questions, -in this example- questions related to depression.\nWhen we analyze data that corresponds with latent variables we have two options:\n\nWe can create a Structural Equation Modeling (We won’t cover this topic).\nWe sum all the items to compute a total score. This total score will be our approximation to the latent variable. (Yes, this is covered in this practice).\n\n\n\n\nI’ll show an example in the next chunk of R code:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\nIn this example we are using the function mutate() to create a new column named depressionTotal, this new depressionTotal column will contain the depression total score for each participant. In this study we included the “Child’s Depression Inventory (CDI)”, that’s why the columns follow the stem “CDI”. You can see the new column in the next table:\n\n\nCode\nruminationRaw &lt;- read.csv(\"ruminationToClean.csv\", na.string = \"99\")\nruminationDepresionScore &lt;- ruminationRaw |&gt;\n  mutate(\n    depressionTotal = rowSums(across(starts_with(\"CDI\")))\n    )\nruminationDepresionScore |&gt; select(starts_with(\"CDI\"),  \n                                   depressionTotal ) |&gt;\n  gt_preview()\n\n\n\n\n\n\n\n\n\nCDI1\nCDI2\nCDI3\nCDI4\nCDI5\nCDI6\nCDI7\nCDI8\nCDI9\nCDI10\nCDI11\nCDI12\nCDI13\nCDI14\nCDI15\nCDI16\nCDI17\nCDI18\nCDI19\nCDI20\nCDI21\nCDI22\nCDI23\nCDI24\nCDI25\nCDI26\ndepressionTotal\n\n\n\n\n1\n0\n2\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n1\n1\n1\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\n15\n\n\n2\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n1\n0\n2\n0\n1\n0\n2\n1\n1\n0\n0\n1\n2\n1\n0\n0\n15\n\n\n3\n0\n1\n0\n0\n0\n1\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n0\n4\n\n\n4\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n0\n0\n6\n\n\n5\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n2\n1\n1\n0\n1\n1\n2\n0\n1\n0\n0\n1\n1\n1\n0\n17\n\n\n6..211\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n212\n0\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n5\n\n\n\n\n\n\n\n\n\n\n\n\n\nExcercise 3 (7 points)\n\n\n\n\nIn the data set ruminationToClean.csv you’ll find seven columns with the stem “DASS”. These columns correspond to the instrument named: Depression, Anxiety, and Stress Scale (DASS). This is an scale that measures three latent variables at the same time, but only the anxiety items were added in this study.\n\nYour task is to change the name of the “DASS” items, you should think a more informative name for these items in the data frame. After that, you will have to compute the total score of anxiety for each participant in the data frame.\n\n\n\n\nLet’s cleanse this data set\n\n\n\nIt is always satisfactory to clean a data set and then see the final product, but so far we have cleaned these data in small steps. I’ll clean the data set in one step. In this example I will delete unnecessary columns (a.k.a research variables), and I will translate the variables to English when necessary:\n\n🟡 Loading\n  webR...\n\n\n  \n\n\n\n\n\n\n\n\n\nExcercise 4 (7 points)\n\n\n\n\nDescribe the steps taken in the R code above."
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#missing-data-without-missing-the-point.",
    "href": "advancedMethods/practice2/practice2.html#missing-data-without-missing-the-point.",
    "title": "Practice 2",
    "section": "Missing data without missing the point….",
    "text": "Missing data without missing the point….\nIn scientific research is always problematic when a participant forgets to answer a question, it is more problematic when a participant drops from our study when conducting a longitudinal study. These scenarios are a nightmare for researchers.\nI will not demonstrate all the possible missing data treatments, but I will show you how R deals naturally with missing values, and how you can code your missing values in your data set."
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#you-have-an-na-on-your-face",
    "href": "advancedMethods/practice2/practice2.html#you-have-an-na-on-your-face",
    "title": "Practice 2",
    "section": "You have an NA on your face",
    "text": "You have an NA on your face\nIn R you will see that missing values are flagged with NA. In the rumination data set you will see NA values like the ones on red:\n\n\nCode\nruminationRaw |&gt; \n  select(starts_with(\"PSWQC\")) |&gt; \n  gt_preview()|&gt;\n   data_color(\n    columns = c(PSWQC_7, PSWQC_7r, PSWQC_9r),\n    method = \"numeric\",\n    palette = \"grey\",\n    domain = c(0, 50),\n    na_color = \"red\"\n  )\n\n\n\n\n\n\n\n\n\nPSWQC_1\nPSWQC_2\nPSWQC_3\nPSWQC_4\nPSWQC_5\nPSWQC_6\nPSWQC_7\nPSWQC_8\nPSWQC_9\nPSWQC_10\nPSWQC_11\nPSWQC_12\nPSWQC_13\nPSWQC_14\nPSWQC_2r\nPSWQC_7r\nPSWQC_9r\n\n\n\n\n1\n2\n1\n2\n3\n2\n1\n1\n1\n1\n2\n3\n1\n0\n3\n12\n2\n2\n\n\n2\n1\n0\n2\n1\n3\n1\n0\n0\n0\n1\n1\n1\n0\n3\n4\n12\n3\n\n\n3\n0\n0\n3\n3\n3\n3\n0\n3\n0\n3\n3\n3\n3\n3\n24\n3\n3\n\n\n4\n1\n1\n1\n1\n1\n1\nNA\n0\n1\n0\n1\n0\n1\n0\n2\nNA\nNA\n\n\n5\n2\n0\n2\n2\n3\n1\n2\n0\n0\n1\n2\n2\n1\n2\n3\n1\n3\n\n\n6..211\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n212\n0\n1\n1\n1\n0\n0\n3\n0\n2\n0\n3\n0\n0\n0\n3\n2\n0\n\n\n\n\n\n\n\nNA stands for “non-applicable” and it became the flag for missing values. It is already built-in with R. You could change this flag but that is not necessary.\nYou can add NA to any vector, list, or data frame, see the following example:\n\n\nCode\nvectorNames &lt;- c(\"Adrian\", \"Marco\", \"Sofia\")\n### Let's add a missing value\nvectorNames[3] &lt;- NA \nvectorNames\n\n\n[1] \"Adrian\" \"Marco\"  NA      \n\n\nIn this example, I added NA in the third position, but of course you could add NA anywhere in a vector.\nIn R there are functions that will return a logical value. Logical data contains only TRUE or FALSE, or 0 or 1. We can see an example with our vector with missing values.\n\n\nCode\n## is.na() function will be your friend\nis.na(vectorNames)\n\n\n[1] FALSE FALSE  TRUE\n\n\nThe function is.na() returned a new logical vector flagging TRUE where there is a missing value. In this example is the third element.\n\n\n\n\n\n\nExcercise 7 (7 points)\n\n\n\n\nSearch on Internet how to write a command where we ask the opposite: “show me TRUE when the value is not missing” .\n\nHINT: You may need the exclamation character."
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#add-na-when-you-read-in-data",
    "href": "advancedMethods/practice2/practice2.html#add-na-when-you-read-in-data",
    "title": "Practice 2",
    "section": "Add NA when you read in data",
    "text": "Add NA when you read in data\nWhen you are opening a data set in R you will always need to specify how you flagged or coded your missing information. For example, it is usual to observe the number “-99” when values are missing, it is also possible to use “99” like I did in my rumination study. The main point is to select a number that is not possible to be found in your data set. In the rumination data set, it is impossible to expect a value “99” as a valid score. Because all the scores range from zero to maximum 20. Observe what I did at the beginning of this practice:\n\n\nCode\ndataLink &lt;- \"https://raw.githubusercontent.com/blackhill86/mm2/main/dataSets/ruminationToClean.csv\"\n\nruminationRaw &lt;- read.csv(url(dataLink ),\n                          na.strings = \"99\") \n\n\nIn the code above, I’m adding the argument na.string = \"99\" to the function read.csv(). I did it because I flagged my missing values with “99”.\n\n\n\n\n\n\nImportant!!\n\n\n\nIf your external file has empty cells, R will interpret empty cells as missing values NA."
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#usual-questions-when-you-are-learning-data-coding-and-stats",
    "href": "advancedMethods/practice2/practice2.html#usual-questions-when-you-are-learning-data-coding-and-stats",
    "title": "Practice 2",
    "section": "Usual questions when you are learning data coding and stats",
    "text": "Usual questions when you are learning data coding and stats\n\nCan I replace missing values with zeros?\nThe straight answers is a well rounded and pretty NO!!!\nA zero is not equivalent to missing value. When you collect data you need to think first how you will code your missing values.\n\n\n\n\n\nCan I replace missing values with the mean?\nThe short answer is NO! But, I’ll give you a longer answer, I’m a professor after all, it is my weakness!\nWhen you replace a missing value with the mean value of an observed distribution, you are doing mean imputation. Long time ago, when computers where not fast, it was faster to replace missing values using the mean, it was easy, and it didn’t require to have a computer. But, nowadays it is not necessary to follow this practice anymore. However, there are still researchers using this replacement method.\nIf you assume that the estimated mean is a good representation of your missing value you will inflate your distribution with values close to the observed mean, which is wrong. Let’s see an example.\nIn this example, I will simulate a regression model. we haven’t covered the theory related to regression but I ebelieve I can explain some details in simple words.\nThe data-generating process will be the following:\n\\[Y \\sim \\alpha + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{3} + \\epsilon  \\tag{1}\\] You may have noticed that all “X” are uppercase and “Y” is uppercase. This means that any outcome \\(Y\\) will be predicted by \\(X_{1}\\), \\(X_{2}\\), and \\(X_{2}\\). This is the model that generates the data. However, given that this is a simulation we can be in control of the process, we can assume that the “population” model looks like this:\n\\[Y \\sim 1 + (0.2)X_{1} + (0.3)X_{2} + (0.4)X_{2} + \\epsilon  \\tag{2}\\]\nLet’s create 2000 data sets with this model in each data set there will be 100 observations.\n\n\nCode\nlibrary(MASS)\nlibrary(purrr)\nlibrary(patchwork)\nlibrary(ggplot2)\n\n#### Let's simulate some data\nset.seed(1236)\n\ngenerateCompleteData &lt;- function(n){\n\nlibrary(MASS)\nMX &lt;- rep(0, 3)\nSX &lt;- matrix(1, 3, 3)\nSX[1, 2] &lt;- SX[2, 1] &lt;- 0.5\nSX[1, 3] &lt;- SX[3, 1] &lt;- 0.2\nSX[2, 3] &lt;- SX[3, 2] &lt;- 0.3\nX &lt;- mvrnorm(n, MX, SX)\ne &lt;- rnorm(n, 0, sqrt(0.546))\ny &lt;- 1 + 0.2 * X[,1] + 0.3 * X[,2] + 0.4 * X[,3] + e\n\nreg &lt;- lm(y ~ X[,1] +  X[,2] + X[,3])\n\ndataReg &lt;- data.frame(inter = reg$coefficients[[1]],\n                      x1 = reg$coefficients[2],\n                      x2 = reg$coefficients[3],\n                      x3 = reg$coefficients[4])\n                    \n\nreturn(dataReg)\n\n}\n\n### Multiple models\n\nallModels &lt;- map_df(1:2000, ~generateCompleteData(100))\n\n\ninterPlot &lt;- ggplot(allModels, aes(x=inter)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModels$inter),2), color = \"red\")+\n  annotate(\"text\", \n           x = 0.85, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModels$inter),2),\" \", \n                          \"SD=\", round(sd(allModels$inter),2)),\n           color = \"red\")+\n  xlab(\"2000 intercepts\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n  \nx1Plot &lt;- ggplot(allModels, aes(x=x1)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModels$x1),2), color = \"red\")+\n  annotate(\"text\", \n           x = 0.4, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModels$x1),2),\" \", \n                          \"SD=\", round(sd(allModels$x1),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X1\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\n\nx2Plot &lt;- ggplot(allModels, aes(x=x2)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModels$x2),2), color = \"red\")+\n  annotate(\"text\", \n           x = 0.5, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModels$x2),2),\" \", \n                          \"SD=\", round(sd(allModels$x2),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X2\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\nx3Plot &lt;- ggplot(allModels, aes(x=x3)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModels$x3),2), color = \"red\")+\n  annotate(\"text\", \n           x = 0.6, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModels$x3),2),\" \", \n                          \"SD=\", round(sd(allModels$x3),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X3\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\ninterPlot  + x1Plot +\nx2Plot + x3Plot  \n\n\n\n\n\nSimulated values with complet information\n\n\n\n\nAs expected, the density plots for our parameters showed that after replicating the same study 2000 times the mean value is exactly at the target. The target are the values that we set in our “population” model in equation Equation 2.\nIn the next step, I will impose missing values on the data generated, we will see what happens when we replace the values with the mean of the observed distribution.\n\n\nCode\nlibrary(simsem)\n\nset.seed(1236)\n\ngenerateIncompleteData &lt;- function(n){\n\nlibrary(MASS)\nlibrary(simsem)  \n  \nMX &lt;- rep(0, 3)\nSX &lt;- matrix(1, 3, 3)\nSX[1, 2] &lt;- SX[2, 1] &lt;- 0.5\nSX[1, 3] &lt;- SX[3, 1] &lt;- 0.2\nSX[2, 3] &lt;- SX[3, 2] &lt;- 0.3\nX &lt;- mvrnorm(n, MX, SX)\ne &lt;- rnorm(n, 0, sqrt(0.546))\ny &lt;- 1 + 0.2 * X[,1] + 0.3 * X[,2] + 0.4 * X[,3] + e\n\n\ndataGen &lt;- data.frame(outcome = y,\n                      x1 = X[,1],\n                      x2 = X[,2],\n                      x3 = X[,3]) \n\n\ndatMis &lt;- imposeMissing(dataGen, \n                         pmMAR = .15, cov=\"x3\")\n\nmeanOutcome &lt;- mean(datMis$outcome, na.rm = TRUE)\nmeanX1 &lt;- mean(datMis$x1, na.rm = TRUE)\nmeanX2 &lt;- mean(datMis$x2, na.rm = TRUE)\n\ndatMis$outcome[is.na(datMis$outcome)] &lt;- meanOutcome \ndatMis$x1[is.na(datMis$x1)] &lt;- meanX1\ndatMis$x2[is.na(datMis$x2)] &lt;- meanX2\n\n\nreg &lt;- lm(outcome ~ x1 + x2 + x3, data = datMis)\n\ndataReg &lt;- data.frame(inter = reg$coefficients[[1]],\n                      x1 = reg$coefficients[2],\n                      x2 = reg$coefficients[3],\n                      x3 = reg$coefficients[4])\n                    \n\nreturn(dataReg)\n\n}\n\nallModelsMissing &lt;- map_df(1:2000, ~generateIncompleteData(100))\n\n\ninterPlotMissing &lt;- ggplot(allModelsMissing, aes(x=inter)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModelsMissing$inter),2), color = \"red\")+\n  geom_vline(xintercept = 1, color = \"blue\")+\n  annotate(\"text\", \n           x = 0.85, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModelsMissing$inter),2),\" \", \n                          \"SD=\", round(sd(allModelsMissing$inter),2)),\n           color = \"red\")+\n  xlab(\"2000 intercepts\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n  \nx1PlotMissing &lt;- ggplot(allModelsMissing, aes(x=x1)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModelsMissing$x1),2), color = \"red\")+\n  geom_vline(xintercept = 0.2, color = \"blue\")+\n  annotate(\"text\", \n           x = 0.4, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModelsMissing$x1),2),\" \", \n                          \"SD=\", round(sd(allModelsMissing$x1),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X1\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\n\nx2PlotMissing &lt;- ggplot(allModelsMissing, aes(x=x2)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModelsMissing$x2),2), color = \"red\")+\n  geom_vline(xintercept = 0.3, color = \"blue\")+\n  annotate(\"text\", \n           x = 0.5, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModelsMissing$x2),2),\" \", \n                          \"SD=\", round(sd(allModelsMissing$x2),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X2\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\nx3PlotMissing &lt;- ggplot(allModelsMissing, aes(x=x3)) + \n  geom_density(fill = \"blue\",alpha = 0.2 )+\n  geom_vline(xintercept = round(mean(allModelsMissing$x3),2), color = \"red\")+\n  geom_vline(xintercept = 0.4, color = \"blue\")+\n  annotate(\"text\", \n           x = 0.5, \n           y = 5, \n           label = paste0(\"M=\", round(mean(allModelsMissing$x3),2),\" \", \n                          \"SD=\", round(sd(allModelsMissing$x3),2)),\n           color = \"red\")+\n  xlab(\"Estimated slopes X3\")+\n  ylab(\"Likelihood\")+\n  theme_classic()\n\ninterPlotMissing  + x1PlotMissing +\nx2PlotMissing + x3PlotMissing  \n\n\n\n\n\nSimulated values after replacing missing data with the mean\n\n\n\n\nThis time the estimated mean of all the slopes and intercepts moved a little bit far from the target, the blue line is our target value, and the red line is the estimated mean after replacing missing values with mean. You may think, that this is not a big deviation but in more complicated models with more parameters, this deviation from the target can cause harm to the point that you are not sure about your results. This is one reason why we don’t replace missing values with the mean.\n\n\nSo, what should we do instead of using mean imputation?\nYou may estimate your models using listwise deletion, this method deletes any case that contains a missing value. This is actually the default behavior in many software. We can study an example:\nIn this example I’m generating a vector with random numbers, and then I’m esimating the mean:\n\n\nCode\n### Let's create a vector with complete values\nvectorComplete &lt;- c(123,56,96,89,59)\n### We can estimate the mean:\nmean(vectorComplete)\n\n\n[1] 84.6\n\n\nThe mean is M=84.6, but notice what happens when there is a missing value:\n\n\nCode\n### Let's delete some values to investigate what happens\n\nvectorMissing &lt;- c(123,56,NA,89,59)\nmean(vectorMissing)\n\n\n[1] NA\n\n\nNow the mean() function returns nothing, it returns NA, why? Because you cannot compute anything if there is an empty slot. You need to delete the observation that is missing and then compute the mean again. This is what is called listwise deletion. You can enable listwise deletion by adding the argument na.rm = TRUE to the mean() function:\n\n\nCode\n### Let's delete some values to investigate what happens\n\nvectorMissing &lt;- c(123,56,NA,89,59)\nmean(vectorMissing, na.rm = TRUE)\n\n\n[1] 81.75\n\n\n\n\n\n\n\n\nExcercise 8 (1 point)\n\n\n\n\nFix the following code when estimating the correlation:\n\n\n\nCode\n### mtcars is a data set already built into R\ndataCars &lt;- mtcars\n\n### I'm imposing missing value on values located at 6,7,8,9\ndataCars$mpg[c(6,7,8,9)] &lt;- NA \n\ncor(dataCars$mpg, dataCars$hp) ### This function cor() should give us a estimated correlation, what is wrong?\n\n\n[1] NA\n\n\nHINT: You may check the help provided for the function cor() by typing ?cor on your R console. Or search on Internet."
  },
  {
    "objectID": "advancedMethods/practice2/practice2.html#final-note-i-promise-is-the-end",
    "href": "advancedMethods/practice2/practice2.html#final-note-i-promise-is-the-end",
    "title": "Practice 2",
    "section": "Final note (I promise is the end)",
    "text": "Final note (I promise is the end)\nThe best strategy to deal with missing values is something call multiple imputation, it is a statistical model that generates multiple data sets with possible values. Although, this is a topic for a second course in Advanced Statistical Methods."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html",
    "title": "Compare means and many more stories",
    "section": "",
    "text": "We will study the concept of p-value a.k.a significance test.\nI will introduce the concept of hypothesis testing.\nI will also introduce the mean comparison for independent groups.\n\n\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following object is masked from 'package:kableExtra':\n\n    group_rows\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#todays-aims",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#todays-aims",
    "title": "Compare means and many more stories",
    "section": "Today’s aims",
    "text": "Today’s aims\n\nWe will study the concept of p-value a.k.a significance test.\nI will introduce the concept of hypothesis testing.\nI will also introduce the mean comparison for independent groups."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value",
    "title": "Compare means and many more stories",
    "section": "What is a p-value?",
    "text": "What is a p-value?\n\np-value stands for probability value.\nIt was born as a measure to reject a hypothesis.\nIn statistics and science, we always have hypothesis in mind. Statistics translates our hypothesis into evaluations of our hypothesis.\nFor example, we often ask questions to our selves such as: why my boyfriend won’t express her feelings? and then we ask, is this related to gender? Is it true that women share easily their emotions compare to men? If so, does it happen only to me? Is this a coincidence?\nWe can create a hypothesis with these questions, let’s try to write one:\n\n\\(H_{1}\\) = There is a difference in emotional expression between cisgender women and cisgender men.\n\nThis is our alternaive hypothesis but we need a null hypothesis:\n\n\\(H_{0}\\) = There is no difference in emotional expression between cisgender women and cisgender men.\n\nThat was easy you might think, but why do we need a null statement?\n\nScience always starts with a null believe, and what we do as scientist is to collect evidence that might help to reject the null hypothesis. If you collect data to support your alternative hypothesis you would be doing something called “confirmation bias”.\nConfirmation bias consist of collecting information that only supports your alternative hypothesis.\nFor example, you start collecting data that only proofs that all swans are white, instead of looking at information that helps to reject the null: not all swans are white."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-ii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-ii",
    "title": "Compare means and many more stories",
    "section": "What is a p-value? II",
    "text": "What is a p-value? II\n\nWe can write out null hypothesis in a statistical statement:\n\n\\(H_{0}\\) = The mean difference in emotional expression between cisgender women and cisgender men is equal to zero.\n\nIn the previous hypothesis we know that we are focusing in the mean difference, it is more specific.\nThe very first step to test our null hypothesis is to create a null model.\nA null model is a model where there is not any difference between groups, or there is not relationship between variables."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-iii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-iii",
    "title": "Compare means and many more stories",
    "section": "What is a p-value? III",
    "text": "What is a p-value? III\n\nIn my new model I will find a null model for the correlation between rumination and depression.\nTo create our new model we will re sample and shuffle our observed data. This is similar to have two decks of cards and you shuffle your cards multiple times until it is hard to guess which card will come next, and imagine cards have equal probability to be selected.\nThis procedure is called permutations, this will help us to create a distribution of null correlations. This means, all the correlations produced by my null model are produced by chance."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-iv",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-iv",
    "title": "Compare means and many more stories",
    "section": "What is a p-value? IV",
    "text": "What is a p-value? IV\n\nLet’s see the following example, remember the estimated correlation between rumination and depression is \\(r= 0.58\\). This null model will help us to know if the correlation is explained by chance.\n\n\nCodeFigure\n\n\n\nrum &lt;- read.csv(\"ruminationComplete.csv\", na.string = \"99\") ## Imports the data into R\n\nrum_scores &lt;- rum %&gt;% mutate(rumination = rowSums(across(CRQS1:CRSQ13)),\n                             depression =  rowSums(across(CDI1:CDI26))) ### I'm calculating\n                                                                       ## total scores\n\n\ncorr &lt;- cor(rum_scores$rumination, rum_scores$depression,\n            use =  \"pairwise.complete.obs\") ## Correlation between rumination and depression\n\n### Let's create a distribution of null correlations\n\nnsim &lt;- 100000\n\ncor.c &lt;- vector(mode = \"numeric\", length = nsim)\n\nfor(i in 1:nsim){\ndepre &lt;- sample(rum_scores$depression, \n                212, \n                replace = TRUE)\n\nrumia &lt;- sample(rum_scores$rumination, \n                212, \n                replace = TRUE)\n\ncor.c[i] &lt;- cor(depre, rumia, use =  \"pairwise.complete.obs\")\n}\n\n\nhist(cor.c, breaks = 120, \n     xlim= c(min(cor.c), 0.70),\n     main = \"Histograma of null correlations\")\nabline(v = corr, col = \"darkblue\", lwd = 2, lty = 1)\nabline(v = c(quantile(cor.c, .025),quantile(cor.c, .975) ),\n col= \"red\",\n lty = 2,\n lwd = 2)"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-v",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#what-is-a-p-value-v",
    "title": "Compare means and many more stories",
    "section": "What is a p-value? V",
    "text": "What is a p-value? V\nLet’s estimate the probability of seeing \\(r = 0.58\\) according to our null model.\n\npVal &lt;- 2*mean(cor.c &gt;= corr)\npVal\n\n[1] 0\n\n\n\nThe probability is a number close to \\(0.00\\).\nWe now conclude that a correlation as extreme as \\(r = 0.58\\) is not explained by chance alone.\n\n\n\n\n\n\n\nNote:\n\n\nThe ugly rule of thumb is to consider a p-value &lt;= .05 as evidence of small probability."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#mean-difference",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#mean-difference",
    "title": "Compare means and many more stories",
    "section": "Mean difference",
    "text": "Mean difference\n\nIn real life, we don’t have to estimate a null model “by hand” as I did before.\nR and JAMOVI will help us on that because the null model is already programmed.\nIn addition, when we compare independent means, we don’t usually do permutations. We follow something called the \\(t\\) - distribution , let’s study more about this distribution: t-distribution."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#mean-difference-ii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#mean-difference-ii",
    "title": "Compare means and many more stories",
    "section": "Mean difference II",
    "text": "Mean difference II\n\nThe \\(t\\)-distribution helped to develop the test named t-Student Test.\nIn this test we use the \\(t\\)-distribution as our model to calculate the probability to observe a value as extreme as 2.74.\nBut this probability will be estimated following the Cumulative Density Function (CDF) of the \\(t\\)-distribution."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#t-test",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#t-test",
    "title": "Compare means and many more stories",
    "section": "\\(t\\)-Test",
    "text": "\\(t\\)-Test\n\nThe Student’s test is also known as a the “t-test”.\nIn this test, we will transform the mean difference of both groups into a \\(t\\) value.\n\n\\[\\begin{equation}\nt= \\frac{\\bar{X}_{1} - \\bar{X}_{2}}{\\sqrt{\\Big [\\frac{(n_{1}-1)s^{2}_{1}+(n_{2}-1)s^{2}_{2}}{n_{1} + n_{2}-2}\\Big ]\\Big [\\frac{n_{1}+n_{2}}{n_{1}n_{2}} \\Big ]}}\n\\end{equation}\\]\n\nIn this transformation \\(n_{1}\\) is the sample size for group 1, \\(n_{2}\\) is the sample size for group 2, \\(s^2\\) means variance. The \\(\\bar{X}\\) represents the mean.\nThis formula will help us to tranform the oberved difference in means to a value that comes from the \\(t\\)-distribution."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-iii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-iii",
    "title": "Compare means and many more stories",
    "section": "\\(t\\)-Test III",
    "text": "\\(t\\)-Test III\n\nRemember we talked about the \\(t\\)-distribution’s CDF. This CDF will help us to estimate the probability of seeing a value. the y-axis represents probability values."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-iv",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-iv",
    "title": "Compare means and many more stories",
    "section": "\\(t\\)-Test IV",
    "text": "\\(t\\)-Test IV\n\nWe can see how useful is a \\(t\\)-test by presenting a applied example.\nIn this example we will try to reject the null hypothesis that says:\n\nThe rumination score in males is equal to the rumination score in females\n\nWe represent this hypothesis in statistics like this:\n\n\\[\\begin{equation}\nH_{0}: \\mu_{1} = \\mu_{0}\n\\end{equation}\\]\n\nAlso in this example I’m introducing a new function in R named t.test(). This is the function that will helps to know if we can reject the null hypothesis.\nThe function t.test() requires a formula created by using tilde ~.\nIn R the the variable on the right side of ~ is the independent variable, the variable on the left side of ~ is the dependent variable.\nIn a independent samples \\(t\\)-test the independent variable is always the group, and the dependent variable is always any continuous variable.\n\n\nCodePlot\n\n\n\nt.test(rumination ~ sex, data = rum_scores, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2457, df = 203, p-value = 0.0258\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3347849 5.1535481\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nIn this example, we found that the \\(p\\)-value is 0.03 and the \\(t\\)-value is 2.25. This means:\n\n***“IF we repeat the same analysis with multiple samples the probability of finding a*** \\(t\\)-value = 2.25 is p = 0.03, under the assumptions of the null model”.\n\nThis is a very small probability, what do you think? Is 2.25 a value explainable by chance alone?"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-assumptions",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#t-test-assumptions",
    "title": "Compare means and many more stories",
    "section": "\\(t\\)-test Assumptions",
    "text": "\\(t\\)-test Assumptions\n\nWe haven’t talked about the assumptions of the t-test model.\nRemember that all models have assumptions, we have to assume something to study Nature.\nThe \\(t\\)-test model assumes that the difference in means is generated by a normally distributed process.\nIt also assumes that variances are equal on both groups.\nLet’s see what happens when we assume equal variances but the data does not come from a process with equal variances:\n\n\n\nShow the code\nset.seed(1234)\n\nN1 &lt;- 50 ## Sample size group 1\n\nN2 &lt;- 50 ### sample size group 2\n\nMean1 &lt;- 100 ## Mean group 1\n\nMean2 &lt;- 20 ### Mean group 2\n\nresults &lt;- list()\n\n\nfor(i in 1:10000){\ngroup1 &lt;- rnorm(N1, mean = Mean1, sd = 100) ### variances or standard deviation are not equal\n\ngroup2 &lt;-  rnorm(N2, mean = Mean2, sd = 200) ### variances or standard deviation are not equal\n\ndataSim &lt;- data.frame(genValues = c(group1,group2), \n                      groupVar = c(rep(1,N1),rep(0,N2)))\n\nresults[[i]] &lt;- t.test(genValues ~ groupVar, data = dataSim, var.equal = TRUE)$p.value\n}\n\n### Proportion of times we rejected the null hypothesis\n\ncat(\"Proportion of times we rejected the null hypothesis\",sum(unlist(results) &lt;= .05)/length(results)*100)\n\n\nProportion of times we rejected the null hypothesis 70.5\n\n\n\nWe successfully rejected the null hypothesis in only 70.5% of the data sets generated. But in reality the \\(t\\)-test should reject the null hypothesis 100% of the times.\nLet’s check when we assume equal variances in our \\(t\\) - test and the model that generates is actually a process with equal variances:\n\n\n\nShow the code\nset.seed(1234)\n\nN1 &lt;- 50\n\nN2 &lt;- 50\n\nMean1 &lt;- 100\n\nMean2 &lt;- 20\n\nresults_2 &lt;- list()\n\nfor(i in 1:10000){\n  \ngroup1 &lt;- rnorm(N1, mean = Mean1, sd = 5) ## equal variances or standard deviation\n\ngroup2 &lt;-  rnorm(N2, mean = Mean2, sd = 5) ## equal variances or standard deviation\n\ndataSim &lt;- data.frame(genValues = c(group1,group2), \n                      groupVar = c(rep(1,N1),rep(0,N2)))\n\nresults_2[[i]] &lt;- t.test(genValues ~ groupVar, data = dataSim, var.equal = TRUE)$p.value\n}\n\n### Probability of rejecting the null hypothesis\n\ncat(\"Proportion of times we rejected the null hypothesis\", sum(unlist(results_2) &lt;= .05)/length(results_2)*100)\n\n\nProportion of times we rejected the null hypothesis 100\n\n\n\nThis time we are rejecting the null hypothesis 100% of the times. This is what we were looking for! Remember that we generated data from a process where group 1 had a mean of 100, and the group 2 had a mean of 20. The t-test should reject the null hypothesis every time I generate new data sets, but this doesn’t happen when I made the wrong assumption: I assumed equal variances when I should not do it.\nSummary: when we wrongly assume that the variances are equal between groups, we decrease the probability to reject the null hypothesis when the null should be rejected. This is bad!\nThese simulations showed the relevance of respecting the assumptions of the \\(t\\)-test."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption",
    "title": "Compare means and many more stories",
    "section": "How do we know if my observed data holds the assumption ?",
    "text": "How do we know if my observed data holds the assumption ?\n\nThere are tests to evaluate the assumption of equivalence of variance between groups.\nThe most used test to evaluate the homogeneity of variance is the Levene’s Test for Homogeneity of Variance.\nWe can implement this test in R using the function leveneTest() , this function comes with the R package car. You might need to install this package in case you don’t have it installed in your computer, you can run the this line of code to install it: install.packages(\"car\")\nI’m going to test if the variance of rumination holds the assumption of equality of variance by sex:\n\n\nlibrary(car)\n\nleveneTest(rumination ~ as.factor(sex), \n           data = rum_scores) \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.2541 0.6148\n      203               \n\n\n\nIn this test the null hypothesis is “The variances of group 1 and group 2 are equal”, if the \\(p\\)-value is less or equal to 0.05 we reject the null hypothesis. In the output above you can see the p-value under the column Pr(&gt;F).\nIn the case of rumination, the \\(p\\)-value = 0.61, given that the \\(p\\)-value is higher than 0.05, we don’t reject the null hypothesis. We can assume the variances of rumination by sex are equal."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption-ii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#how-do-we-know-if-my-observed-data-holds-the-assumption-ii",
    "title": "Compare means and many more stories",
    "section": "How do we know if my observed data holds the assumption ? II",
    "text": "How do we know if my observed data holds the assumption ? II\n\nWhat happens if the Levene’s Test rejects the null hypothesis of homogeneity of variance?\nCan we continue using the \\(t\\) - test to evaluate my hypothesis?\nThe answer is: Yes you can do a \\(t\\)-test but there is a correction on the degrees of freedom. We will talk more about degrees of freedom in the next sessions.\nIf you cannot assume equality of variances, all you have to do in R is to switch the argument var.equal = TRUE to var.equal = FALSE.\n\n\nt.test(rumination ~ sex, data = rum_scores, var.equal = FALSE)\n\n\n    Welch Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2841, df = 149.72, p-value = 0.02377\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3702483 5.1180847\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nNow , the output says we are performing a Welch Two Sample t-test, Welch was tha mathematician who found the correction."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size",
    "title": "Compare means and many more stories",
    "section": "Effect size",
    "text": "Effect size\n\nUp to this point we have studied how we test our hypothesis when we compare independent means, but we still have to answer the question, how large is a large difference between means? Or, how small is a small difference? In fact, what is considered a small difference?\nThese questions were answered by Jacob Cohen (1923-1998).\nCohen created a standardized measure to quantify the magnitude of the difference between means.\nLet’s see a pretty plot about it in this link."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#time-to-check-more-formulas",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#time-to-check-more-formulas",
    "title": "Compare means and many more stories",
    "section": "Time to check more formulas",
    "text": "Time to check more formulas\nCohen’s \\(d\\) Effect Size:\n\\[\\begin{equation}\nES = \\frac{\\bar{X}_{1}-\\bar{X}_{2}}{\\sqrt{\\Big[\\frac{s^2_{1}+s^2_{2}}{2} \\Big ]}}\n\\end{equation}\\]\nWhere:\n\n\\(ES\\) = is effect size.\n\n\\(\\bar{X}\\) = Mean.\n\n\\(s^2\\) = variance."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example",
    "title": "Compare means and many more stories",
    "section": "Effect size example",
    "text": "Effect size example\n\nLet’s compute an effect size in R. In this example we will resume our \\(t\\)-test example of rumination by sex.\n\n\nt.test(rumination ~ sex, data = rum_scores)\n\n\n    Welch Two Sample t-test\n\ndata:  rumination by sex\nt = 2.2841, df = 149.72, p-value = 0.02377\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.3702483 5.1180847\nsample estimates:\nmean in group 0 mean in group 1 \n       31.20896        28.46479 \n\n\n\nIn the \\(t\\)- test output we have information we can use to compute the effect size. We have the mean for each group \\(M_{(female)} = 31.21\\), \\(M_{(male)}= 28.46\\). We already know that the difference between groups is not explained by chance alone (\\(p\\)= 0.02) because the p-value is smaller than 0.05.\nNow let’s compute the effect size,\n\n\nlibrary(dplyr)\n\n### This will create tidy table by group (sex)\nInfoByBroup &lt;- rum_scores %&gt;% group_by(sex) %&gt;%\n    summarize(Mean = mean(rumination, na.rm = TRUE),  \n          Var = var(rumination, na.rm = TRUE))\n\n\n\n\nRumination mean and variance by sex\n\n\nsex\nMean\nVar\n\n\n\n\n0\n31.21\n71.88\n\n\n1\n28.46\n64.40\n\n\n\n\n\n\n\nEffect size computation\n\n\nShow the code\nmeanFemales &lt;- InfoByBroup$Mean[1]\n\nmeanMales &lt;- InfoByBroup$Mean[2]\n\nvarianceFemales &lt;- InfoByBroup$Var[1]\n\nvarianceMales &lt;- InfoByBroup$Var[2]\n\neffectSize &lt;- (meanFemales-meanMales)/sqrt((varianceFemales+varianceMales)/2)\n\ncat(\"The effect size of sex on rumination is\", \"d =\", round(effectSize,2))\n\n\nThe effect size of sex on rumination is d = 0.33\n\n\n\nAccording to (cohen_statistical_2013?) a small effect size is \\(d = 0.20\\), a medium effect size is \\(d = 0.50\\), a large effect size is \\(d= .80\\)"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-ii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-ii",
    "title": "Compare means and many more stories",
    "section": "Effect size example II",
    "text": "Effect size example II\n\nWe can make this calculation more fun. Let’s imagine you are a clinician, and you need a way to understand how large is the difference in rumination and depression. You need to find a way to quantify which variable you should pay more attention, which variable has larger differences between cisgender males and cisgender females.\nWhy would you care about differences between cisgender males and cisgender females? Why would you care to compare the differences between rumination and depression?\nFirstly, we need to compute the depression score. In the rumination data set you’ll find columns named CDI these are the items that correspond to the Children’s Depression Inventory. We will use this items to compute a total score.\n\n\nrum_scores &lt;- rum_scores %&gt;% \n  mutate(depression = rowSums(across(CDI1:CDI26))) ## Computes total score\n\nWe need to check if the variances our depression score are equal between groups before running a \\(t\\)-test:\n\nlibrary(car)\n\nleveneTest(depression ~ as.factor(sex), \n           data = rum_scores) \n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1  0.8682 0.3526\n      205               \n\n\n\nThe Leven’s Test for Homogeneity of Variance did not reject the null hypothesis of equality of variances. Hence, we can hold the assumption of equality of variances. We don’t need to perform a correction\n\n\n## This test assumes equality of variances after performing the Levene's Test.\nt.test(depression ~ sex, data = rum_scores, var.equal = TRUE) \n\n\n    Two Sample t-test\n\ndata:  depression by sex\nt = 2.5468, df = 205, p-value = 0.01161\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n 0.5257305 4.1298205\nsample estimates:\nmean in group 0 mean in group 1 \n      11.257353        8.929577 \n\n\n\nNice! Look at the result, we found a difference in depression by cisgender (sex), and it is not explained by chance alone.\n\n\n\n\n\n\n\nImportant\n\n\nHow do you know the difference is not explained by chance alone?"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-iii",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-iii",
    "title": "Compare means and many more stories",
    "section": "Effect size example III",
    "text": "Effect size example III\n\nAfter estimating the \\(t-test\\) of depression by sex, we can now quantify the effect’s magnitude:\n\n\nInfoByBroupDepression &lt;- rum_scores %&gt;% group_by(sex) %&gt;%\n    summarize(Mean = mean(depression, na.rm = TRUE),  \n          Var = var(depression, na.rm = TRUE))\n\n\n\n\nDepression mean and variance by sex\n\n\nsex\nMean\nVar\n\n\n\n\n0\n11.26\n36.86\n\n\n1\n8.93\n43.04\n\n\n\n\n\n\n\nEffect size computation\n\n\nShow the code\nmeanFemales &lt;- InfoByBroupDepression$Mean[1]\n\nmeanMales &lt;- InfoByBroupDepression $Mean[2]\n\nvarianceFemales &lt;- InfoByBroupDepression$Var[1]\n\nvarianceMales &lt;- InfoByBroupDepression$Var[2]\n\neffectSizeDepression &lt;- (meanFemales-meanMales)/sqrt((varianceFemales+varianceMales)/2)\n\ncat(\"The effect size of sex on depression is\", \"d =\", round(effectSizeDepression,2))\n\n\nThe effect size of sex on depression is d = 0.37\n\n\n\nAs you can see the effect size of sex on depression is \\(d=\\) 0.37 whereas in rumination the effect size of sex is \\(d=\\) 0.33"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.",
    "title": "Compare means and many more stories",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nWhat is your conclusion as a clinician? Is rumination too much different from depression?\nShould we focus on an intervention that addresses depression or/and rumination differently according to sex?\nShould you create a depression intervention accounting for the effect of sex ? Can you ignore rumination for your intervention?\nShould we focus mainly on women because the difference is large between groups?"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.-1",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.-1",
    "title": "Compare means and many more stories",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nAs always there are other ways to compute the effect size.\nThe next formula takes into account occasions when the sample size is not the same between groups.\n\n\\[\\begin{equation}\nES = \\frac{\\bar{X}_{1} - \\bar{X}_{2}}{\\sqrt{\\Big [\\frac{(n_{1}-1)s^{2}_{1}+(n_{2}-1)s^{2}_{2}}{n_{1} + n_{2}-2}\\Big ]}}\n\\end{equation}\\]\nWhere:\n\\(\\bar{X}\\) = Mean\n\n\\(s^2\\) = variance\n\n\\(n\\) = sample size\n\n\nYou can study more about effect sizes by clicking this link. It is an article with a good explanation and summary. We will talk about effect sizes again when we study Analysis of Variance (ANOVA)."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.-2",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#effect-size-example-cont.-2",
    "title": "Compare means and many more stories",
    "section": "Effect size example (cont.)",
    "text": "Effect size example (cont.)\n\nWe can also estimate the effect sizes with pacakages already programmed to be used in R. In this case we can use the package effectsize.\nYou don’t have this package installed in your R installation. You will have to install the package by running install.packages(\"effectsize\", dependencies = TRUE).\nYou only need to install a package one time. You don’t have to install packages everytime you open RStudio.\n\n\nRuminationDepression\n\n\n\nlibrary(effectsize)\n\ncohens_d(rumination ~ sex,\n         data = rum_scores) ### Equal variances assumed\n\nCohen's d |       95% CI\n------------------------\n0.33      | [0.04, 0.62]\n\n- Estimated using pooled SD.\n\ncohens_d(rumination ~ sex,\n         data = rum_scores, \n         pooled_sd = FALSE) ### does not assume equal variances\n\nCohen's d |       95% CI\n------------------------\n0.33      | [0.04, 0.62]\n\n- Estimated using un-pooled SD.\n\n\n\n\n\ncohens_d(depression ~ sex,\n         data = rum_scores) ### Equal variances assumed\n\nCohen's d |       95% CI\n------------------------\n0.37      | [0.08, 0.66]\n\n- Estimated using pooled SD.\n\ncohens_d(depression ~ sex,\n         data = rum_scores, \n         pooled_sd = FALSE) ### does not assume equal variances\n\nCohen's d |       95% CI\n------------------------\n0.37      | [0.07, 0.66]\n\n- Estimated using un-pooled SD."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#jamovi",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#jamovi",
    "title": "Compare means and many more stories",
    "section": "JAMOVI",
    "text": "JAMOVI\n\nI hope you remember JAMOVI. I mentioned this software in the first class.\nIt is also listed in the syllabus.\nThis is a software that runs R using a interface that does not require to write the R code. JAMOVI will write the R code for you behind scenes, it also runs the code for you. Pretty awesome isn’t it?"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-1",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-1",
    "title": "Compare means and many more stories",
    "section": "Central Limit Theorem (CLT)",
    "text": "Central Limit Theorem (CLT)\n\nThe Central Limit Theory (CLT) is a fact, it describes a rule that happens everywhere in Nature.\n\nThe CLT states: For any population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the distribution of sample means for sample size \\(n\\) will have a mean of \\(\\mu\\) and a standard deviation of \\(\\frac{\\sigma}{\\sqrt{n}}\\) and will approach a normal distribution as \\(n\\) approaches infinity.\n\nThe beauty of this theorem is due to:\nit describes the distribution of sample means for any population, no matter what shape, mean, or standard deviation.\nThe distribution of sample means “approaches” a normal distribution very rapidly.\nIt is better to explain this theorem with a simulation, as always we do in this class."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.",
    "title": "Compare means and many more stories",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nIn this example I’ll generate 1000 “datasets” from a Gaussian process (normal distribution) with M = 60, sd = 100. Each data set will contain 10 observations.\nAfter generating 1000 data sets, I’m estimating the mean of those simulated values.\nImagine you are asking the question how many minutes do you need to take a shower? to 10 participants, you then conduct the same study the next day with a different set of 10 participants every day until you get 1000 sets of 10 participants.\n\n\nCodePlot\n\n\n\nset.seed(563)\nN &lt;- 10\n\nM &lt;- 60\n\nSD &lt;- 100\n\n## Number of data sets or replications\nrep &lt;- 1000\n\nresults &lt;- list()\n\nfor(i in 1:rep){\n\nresults[[i]] &lt;- mean(rnorm(n = N, mean = M, sd = SD))\n}\n\n\n\n\nplot(density(unlist(results)),\n     xlab = \"Sample means\",\n     ylab = \"p(y) or likelikood\",\n     main = \"Density plot of Sample means\")"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-1",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-1",
    "title": "Compare means and many more stories",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nThere is more beautiful qualities about the CLT. This rule applies to all distributions. When you increase the number of observations per sample, and the number of sample approach infinity. The sample of means will look Gaussian distributed (normally distributed).\nRemember the Poisson distribution I mentioned some lectures behind?\nAs you can see the Poisson distribution after generating 1 data set of 10 observations doesn’t look like a Gaussian process at all!\nCan the sample of means from a Poisson process generate a pretty bell shape?\n\n\nCodeFigure or plot\n\n\n\nset.seed(563)\nplot(density(rpois(n = 5,lambda = 8)),\n     xlab = \"Number of times people take a shower in a week\",\n     ylab = \"p(y) or likelihood\",\n     main = \"Density plot of a Poisson distributed variable\")"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-2",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-2",
    "title": "Compare means and many more stories",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nLet’s find out the answer by simulating 1000 data sets:\n\n\nCodePlot\n\n\n\nset.seed(563)\nN &lt;- 10\n\nM &lt;- 10\n\n## Number of data sets or replications\nrep &lt;- 1000\n\nresults &lt;- list()\n\nfor(i in 1:rep){\n\nresults[[i]] &lt;- mean(rpois(n = N, lambda = M))\n}\n\n\n\n\nplot(density(unlist(results)),\n     xlab = \"Sample of means (Number of times people take a shower in a week)\",\n     ylab = \"p(y) or likelihood\",\n     main = \"Density plot of sample of means from a Poisson data process\")\n\n\n\n\n\n\n\n\n\n\n\n\nThere is also something even prettier, the mean estimated with all the means, is actually close to the data generating process. It is lovely! This is a natural rule!\n\n\nmean(unlist(results))\n\n[1] 10.0076"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-3",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#central-limit-theorem-clt-cont.-3",
    "title": "Compare means and many more stories",
    "section": "Central Limit Theorem (CLT) (cont.)",
    "text": "Central Limit Theorem (CLT) (cont.)\n\nWe saw that the mean of sample means is close to the data generating process mean , and it doesn’t matter the type of distribution. However, take into account that there are probability distribution models that will require more than 800000000000 sampled means to approach the Gaussian shape. But eventually the distribution of all the means will look like a Gaussian distribution.\nIn simple terms, what does it mean? It means that the mean is an unbiased estimate on average it tends to be close to the “population” mean. This is why we like the mean as an estimate , and we use it to test hypothesis."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#standard-error",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#standard-error",
    "title": "Compare means and many more stories",
    "section": "Standard Error",
    "text": "Standard Error\n\nThe CLT will be helpful to introduce the concept of standard error.\nThe Standard Error is a measure of how far is the mean of my sample from the true value in the data generating process.\nThe Standard Error in the context of multiple samples is the standard deviation of all sampled means.\nWe should go to another example:\n\n\n\nShow the code\nset.seed(563)\nN &lt;- 10\n\nM &lt;- 60\n\nSD &lt;- 100\n\n## Number of data sets or replications\nrep &lt;- 1000\n\nresults &lt;- list()\n\nfor(i in 1:rep){\n\nresults[[i]] &lt;- mean(rnorm(n = N, mean = M, sd = SD))\n}\n\ncat(\"The Standard Deviation of 1000 sampled means is\", sd(unlist(results)))\n\n\nThe Standard Deviation of 1000 sampled means is 33.25718\n\n\n\nin this example if we estimate the standard deviation of our sample means we get sd = 33.2571782."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#standard-error-cont.",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#standard-error-cont.",
    "title": "Compare means and many more stories",
    "section": "Standard Error (cont.)",
    "text": "Standard Error (cont.)\n\nIn real life, you’ll never know the standard deviation of sample means, because this is a simulated example.\nIn real application you need to estimate how far your observed mean is from the true mean.\nWe can use this estimation \\(\\frac{\\sigma}{\\sqrt(n)}\\) to approximate how far we are from the mean. In this formula \\(\\sigma\\) is the standard deviation and \\(n\\) is the sample size.\nThe standard error decreases when we increase the sample size, let’s see another example:\n\n\nCodePlot\n\n\n\n\nShow the code\nset.seed(563)\n\nN &lt;- seq(10, 10000, by = 10)\n\nM &lt;- 60\n\nSD &lt;- 100\n\nresults &lt;- list()\n\nfor(i in 1:length(N)){\n\nresults[[i]] &lt;- sd(rnorm(n = N[i], mean = M, sd = SD))/sqrt(N[i])\n}\n\n\n\n\n\nplot(x = N,\n     y = results,\n     type = \"l\",\n     xlab = \"Number of observations in each sample\",\n     ylab = \"Standard error\",\n     main = \"Line plot of Standard Error by sample size\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\nIn simple words: The standard error (SE) measures the distance from the true value in the data generating process."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#the-689599.7-rule-for-a-normal-distribution",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#the-689599.7-rule-for-a-normal-distribution",
    "title": "Compare means and many more stories",
    "section": "The 68–95–99.7 Rule for a Normal Distribution",
    "text": "The 68–95–99.7 Rule for a Normal Distribution\n\nThere is an important concept in frequentist statistics called “confidence interval”\nBut before getting into details I need to explain a important property of the normal distribution.\nThe following rule applies for the normal distribution:\n\n68 \\(\\%\\) of values will be between \\(\\mu\\) - \\(\\sigma\\) and \\(\\mu\\) + \\(\\sigma\\) .\n95 \\(\\%\\) of values will be between \\(\\mu\\) - \\(2 \\sigma\\) and \\(\\mu\\) + \\(2 \\sigma\\).\n99.7 \\(\\%\\) of values will be between \\(\\mu\\) - \\(3 \\sigma\\) and \\(\\mu\\) + \\(3 \\sigma\\)\n\nWe can see it by generating values from a Gaussian process:\n\n\nlibrary(ggplot2)\n\nset.seed(3632)\n\ngeneratedValues &lt;- rnorm(200000, mean = 50, sd = 2)\n\n### +-1 standard deviation form the mean\n\nupper &lt;- 50 + 2\n\nlower &lt;- 50 - 2\n\npercent1sigma &lt;- (sum(generatedValues &lt;= upper & generatedValues &gt;= lower)\n/length(generatedValues))*100\n\n### +-2 standard deviation form the mean\n\nupper &lt;- 50 + 2*2\n\nlower &lt;- 50 - 2*2\n\npercent2sigma &lt;- (sum(generatedValues &lt;= upper & generatedValues &gt;= lower)/length(generatedValues))*100\n\n### +-3 standard deviation form the mean\n\nupper &lt;- 50 + 3*2\n\nlower &lt;- 50 - 3*2\n\npercent3sigma &lt;- (sum(generatedValues &lt;= upper & generatedValues &gt;= lower)/length(generatedValues))*100\n\n\n68.35 \\(\\%\\) of observations are located at \\(\\pm 1 \\sigma\\) from the data generating process mean. That is true!\n95.45 \\(\\%\\) of observations are located at \\(\\pm 2 \\sigma\\) from the data generating process mean. That is true!\n99.74 \\(\\%\\) of observations are located at \\(\\pm 3 \\sigma\\) from the data generating process mean. That is true!"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor",
    "title": "Compare means and many more stories",
    "section": "Allow me to use a metaphor",
    "text": "Allow me to use a metaphor\nWestfall & Henning (2013):\n\nImagine there is a lion or perhaps a coyote around your town. Every day the coyote moves around 20 km from the town, forming a circular perimeter. The town is right in the middle of the circle:"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.",
    "title": "Compare means and many more stories",
    "section": "Allow me to use a metaphor (cont.)",
    "text": "Allow me to use a metaphor (cont.)\n\nAs you can imagine the lion or coyote could move further from the town but it is always wondering around town chasing chickens or killing cows.\nWe could collect data, for instance, the distance every day from town, and see on average how far it is from town. We could also estimate the standard deviation (). This could be a good measure to see patterns, and be confident about the interval (circle) where the lion or coyote walks."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.-1",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#allow-me-to-use-a-metaphor-cont.-1",
    "title": "Compare means and many more stories",
    "section": "Allow me to use a metaphor (cont.)",
    "text": "Allow me to use a metaphor (cont.)\n\nLet’s imagine that the town is our data generating process mean value (). Also imagine that each time the lion or coyote walks around the town is an independent observed sample from the data generating process.\nWe need to find a way to be confident that our observed data is closed to \\(\\mu\\) (“population mean”).\nA few minutes ago we saw that according to the a Gaussian distribution, you’ll find 95 \\(\\%\\) of the values around \\(\\pm 2 \\sigma\\) from the mean. However, in our simulated data we found out that in reality you’ll find more than 95 \\(\\%\\). It was actually 95.45 \\(\\%\\).\nWe’ll use this great property of the Gaussian distribution, but we will use \\(\\pm 1.96 \\sigma\\) instead of \\(\\pm 2 \\sigma\\). Why? because we want to be closer to exactly find 95\\(%\\) of the observations. This means, we want to be 95\\(\\%\\) confident that we are close to \\(\\mu\\), or the town following our example.\nWe will estimate the confidence interval by computing:\n\n\\[\\begin{equation}\n\\bar{x} \\pm 1.96 \\frac{\\hat{\\sigma}}{ \\sqrt{n}}\n\\end{equation}\\]\nWhere,\n\\(\\bar{x}\\) is the estimated mean.\n\\(\\hat{\\sigma}\\) is the estimated standard deviation.\n\\(1.96\\) is the 97.5 percentile in the standard normal distribution (Gaussian)."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval",
    "title": "Compare means and many more stories",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nLet’s compute the confidence interval by hand, we can use again the mean of the rumination score:\n\\[\\begin{equation}\n30.25 \\pm 1.96  \\Big (\\frac{8.41}{\\sqrt{205}} \\Big )\n\\end{equation}\\]\nWe need to estimate the mean first:\n\nRumMean &lt;- mean(na.omit(rum_scores$rumination))\nRumMean\n\n[1] 30.25854\n\n\nSecond, we need to estimate the standard deviation:\n\nsd(na.omit(rum_scores$rumination))\n\n[1] 8.406725\n\n\nNow we can estimate the 95\\(\\%\\) confidence interval:\n\nupperCI &lt;- 30.26+1.96*(8.41/sqrt(205))\n\nlowerCI &lt;- 30.26-1.96*(8.41/sqrt(205))\n\ncat(\"The 95% confidence interval for the mean is:\", \n        \"upper-bound=\",\n        round(upperCI,2), \n        \"lower-bound=\",\n          round(lowerCI,2))\n\nThe 95% confidence interval for the mean is: upper-bound= 31.41 lower-bound= 29.11\n\n\nWe can also estimate the confidence interval using the package rcompanion:\n\nlibrary(rcompanion)\ngroupwiseMean(rumination ~ 1,\n              data   = rum_scores,\n              conf   = 0.95,\n              digits = 3,\n              na.rm = TRUE)\n\n   .id   n Mean Conf.level Trad.lower Trad.upper\n1 &lt;NA&gt; 205 30.3       0.95       29.1       31.4\n\n\n\nThere is something to note here. What you are doing is just multipliying the standard error of the mean by 1.96. This value = 1.96 is called critical value.\n\n\n##Standard error multiplied by 1.96\n1.96*(8.41/sqrt(205))\n\n[1] 1.151265\n\n\n\nWith the information above we could interpret our result as follows:\n\nI am approximately 95\\(\\%\\) confident that \\(\\mu\\) is within 1.15 score points of the sample average 30.26."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval-intepretation",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval-intepretation",
    "title": "Compare means and many more stories",
    "section": "Confidence Interval Intepretation",
    "text": "Confidence Interval Intepretation\n\nMany of the concepts studied up to this point come from a theory called “frequentist”.\nThe frequentist theory studies the probability in terms of long run events. In this approach we have to imagine that we repeat the same experiment or study several times. Only if we repeat the same study several times we’ll be able to create a confidence interval.\nThis is, of course theoretical. We are able to calculate confidence intervals with one sample. But, this interval is only an approximation.\nThe interpretation of the confidence interval demands to imagine that you repeat the same study \\(n\\) number of times. Hence the interpretation would be:\n\nSince \\(\\mu\\) will lie within the upper and lower limits of similarly constructed intervals for 95% of the repeated samples,my sample is likely to be one of those samples where \\(\\mu\\) is within the upper and lower limits, and I am therefore 95% confident that \\(\\mu\\) is between 29.1 and 31.4.\n\\[\\begin{equation}\n29.1 \\le \\mu \\le 31.4\n\\end{equation}\\]"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval-multiple-means",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#confidence-interval-multiple-means",
    "title": "Compare means and many more stories",
    "section": "Confidence Interval: multiple means",
    "text": "Confidence Interval: multiple means\n\n\nLet’s imagine a ask the same question: how many minutes do you need to take a shower?\nWe could ask the same question to 50 different people 100 000 times, and then estimate the mean. The frequentist theory says that the true value will be among the 95\\(\\%\\) of the sampled means."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means",
    "title": "Compare means and many more stories",
    "section": "Repeated measures: compare two dependent means",
    "text": "Repeated measures: compare two dependent means\n\nWe have only studied how to test the difference between independent groups.\nHowever, many times we have research designs where we do a pre-test and a post-test.\nFor example, I could create an intervantion to treat depression. If I want to evaluate if there is an effect of the intervention I could measure the depression levels before the intervention, after that my participants will receive my new depression treatment. After they finish the intervention I measure again thei levels of depression.\nIn this example, we would like to see a lower score in depression after the intervention. We would also like to test if the mean difference between time 1 and time 2 is explained by chance alone.\n\nNull hypothesis: \\[\\begin{equation}\nH_{0} = \\mu_{posttest} = \\mu_{pretest}\n\\end{equation}\\]"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.",
    "title": "Compare means and many more stories",
    "section": "Repeated measures: compare two dependent means (cont.)",
    "text": "Repeated measures: compare two dependent means (cont.)\n\\[\\begin{equation}\nt = \\frac{\\sum D}{\\sqrt{\\frac{n \\sum D^2 - \\big (\\sum D \\big )^2}{n-1}}}\n\\end{equation}\\]\n\\(D\\) is the difference between each individual’s score from the first to the second time point.\n\\(\\sum D\\) is the sum of all the differences between groups of scores.\n\\(\\sum D^2\\) is the sum of the differences squared between groups of scores.\n\\(n\\) is the number of pairs of observations."
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.-1",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#repeated-measures-compare-two-dependent-means-cont.-1",
    "title": "Compare means and many more stories",
    "section": "Repeated measures: compare two dependent means (cont.)",
    "text": "Repeated measures: compare two dependent means (cont.)\n\nWe can test if the mean difference in life expectancy is explained by chance.\nLet’s compare years 2019 and 2020:\n\n\nlifeExpec &lt;- read.csv(\"lifeExpect.csv\")\n\nt.test(lifeExpec$X2019, lifeExpec$X2020, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  lifeExpec$X2019 and lifeExpec$X2020\nt = -0.19141, df = 243, p-value = 0.8484\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.07583952  0.06240551\nsample estimates:\nmean difference \n   -0.006717005 \n\n\nWe failed to reject the null hypothesis according to the \\(t\\)-test for dependent means. How did we arrived to this conclusion?\n\n\n\n\n\n\nTip\n\n\nPay attention to the \\(p-value\\). Do you remember the null hypothesis in this case?"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html",
    "href": "advancedMethods/Lecture1/introProbAndStats.html",
    "title": "Introduction to important concepts in Statistics",
    "section": "",
    "text": "It might seem trivial to talk about nature and how science related to nature. However, we will study nature when we study statistics.\nAs Westfall & Henning (2013) mentioned in their book: “Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation—that is, it encompasses those things that we can agree we are observing” (p.1)\nAs psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study constructs, I will talk about constructs frequently.\nStatistics is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or Nature."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#reality-nature-science-and-models",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#reality-nature-science-and-models",
    "title": "Introduction to important concepts in Statistics",
    "section": "Reality, Nature, Science, and Models",
    "text": "Reality, Nature, Science, and Models\n\nIt might seem trivial to talk about nature and how science related to nature. However, we will study nature when we study statistics.\nAs Westfall & Henning (2013) mentioned in their book: “Nature is all aspects of past, present, and future existence. Understanding Nature requires common observation—that is, it encompasses those things that we can agree we are observing” (p.1)\nAs psychologist we study behavior, thoughts, emotions, beliefs, cognition and contextual aspects of all the above. These elements are also part of nature, we mainly study constructs, I will talk about constructs frequently.\nStatistics is the language of science. Statistics concerns the analysis of recorded information or data. Data are commonly observed and subject to common agreement and are therefore more likely to reflect our common reality or Nature."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#reality-nature-science-and-models-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#reality-nature-science-and-models-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Reality, Nature, Science, and Models II",
    "text": "Reality, Nature, Science, and Models II\n\n\n\nTo study and understand Nature we must construct a model for how Nature works.\nA model helps you to understand Nature and also allows you to make predictions about Nature. There is no right or wrong model; they are all wrong! But some are better than others."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-models",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-models",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Models",
    "text": "Statistical Models\n\n\n\nWe will focus on mathematical models integrated by equations and theorems.\nModels cannot reflect exactly how Nature works, but it is close enough to allow us to make predictions and inferences.\nLet’s create a model for driving distance.Imagine you drive at 100 km/hour, then we can predict your driving time \\(y\\) in hours by creating the following model: \\(y = x/100\\)\nYou could plug in any value to replace \\(x\\) and you’ll get a prediction:\nExample:\n\n\\[\\begin{align}\ny &= 300 km/100 \\\\\ny & = 3 hours \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#model-produces-data",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#model-produces-data",
    "title": "Introduction to important concepts in Statistics",
    "section": "Model produces data",
    "text": "Model produces data\n\nPay attention to our model. The model just created produces data, if we replace \\(x\\) by other values, the model will give us new information:\n\n\nR codePlotData\n\n\n\nkm &lt;- c(200,300,400,500,600,900,1000)\n## Our model\ntime &lt;- km/100\n### Let's plot the information\nlibrary(ggplot2)\n\np &lt;- ggplot(data=data.frame(km,time), aes(x=km ,y=time)) +\n    geom_line() +\n    geom_point()+ \n    theme_classic()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    km time\n1  200    2\n2  300    3\n3  400    4\n4  500    5\n5  600    6\n6  900    9\n7 1000   10"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#model-produces-data-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#model-produces-data-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Model produces data II",
    "text": "Model produces data II\n\nA Model is like a recipe, it has some steps and rules that will help you to prepare a cake or a meal. The cake is your data.\n\n\n\n\n\n\n\n\nAlways remember!\n\n\nModels produce data, data does not produce models!\n\n\n\n\n\nYou can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data.\nYou can use data to estimate models, but that does not change the fact that your model comes first, before you ever see any data."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Processes",
    "text": "Statistical Processes\n\n\n\n\n\n\n\nAlways remember!\n\n\nThe order matters, Nature is there before our measurements,the data comes after we establish our way to measure Nature."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Processes II",
    "text": "Statistical Processes II\nWestfall & Henning (2013) are doing an important distinction:\n\nThey present DATA with upper case to differentiate data with lower case. Why?\nWhen we talk about Nature we are talking about infinite numbers of observations, from these infinite number of possibilities we extract a portion of DATA, this is similar to the concept of population. For instance, your DATA could be all the college students in California.\ndata (lowercase) means that we already collected a sample from that DATA. For example, if you get information from only college students from valley, you’ll have one single instance of what is possible in your population.\nYour data is the only way we have to say something about the DATA.\nPrior collecting data, the DATA is random, unknown."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-iii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-iii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Processes III",
    "text": "Statistical Processes III\nMore about models\n\nWe will use the term “probability model” , we will represent this term using: \\(p(y)\\) which translates into “probability of \\(y\\)”.\nLet’s use the flipping coin example. We know that the probability of flipping a coin and getting heads is 50%, same probability can be observed for tails (50%). Then, we can represent this probability by \\(p(heads) = 0.5\\) or \\(p(tail) = 0.5\\).\nThis is actually a good model! Every time, it produces as many random coin flips as you like. Models produce data means: that YOUR model is a model the explains how your DATA will be produced!\nYour model for Nature is that your DATA come from a probability model \\(p(y).\\)"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-iv",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-processes-iv",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Processes IV",
    "text": "Statistical Processes IV\n\nOur model \\(p(y)\\) can be used to predict and explain Nature. A prediction is a guess about unknown events in the past, present or future or about events that might not happen at all.\nIt is more like asking “What-if” .For example, what if I had extra $3000 to pay my credit balance?"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#deterministic-vs.-probabilistic",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#deterministic-vs.-probabilistic",
    "title": "Introduction to important concepts in Statistics",
    "section": "Deterministic vs. Probabilistic",
    "text": "Deterministic vs. Probabilistic\n\nIn science, you’ll find models that are deterministic, that means; our outcome is completely determined by our \\(x\\)\n\nLet’s see this example: Free Fall Calculator\n\nIn physics you’ll find several examples of deterministic models, especially from the Newtonian perspective.\nNormally these models are represented with \\(f(.)\\) instead of \\(p(.)\\), for example the driving distance example can be written as \\(f(x) = x/100\\).\nThis symbol \\(f(x)\\) means mathematical function. The function will give us only one solution in the case of a deterministic mode. We plug in values and we get a solution. (\\(y = f(x)\\))."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#deterministic-vs.-probabilistic-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#deterministic-vs.-probabilistic-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Deterministic vs. Probabilistic II",
    "text": "Deterministic vs. Probabilistic II\n\nProbabilistic models models assume variability , they are not deterministic, probabilistic models produce data that varies, therefore we’ll have distributions.\nProbabilistic models are more realistic to explain phenomena in psychology.\nThe following expresion represents a probabilistic model:\n\n\\[\\begin{equation}\nY \\sim p(y)\n\\end{equation}\\]\n\nThe symbol \\(\\sim\\) can be read aloud either as “produced by” or “distributed as.” In a complete sentence, the mathematical shorthand \\(Y \\sim p(y)\\) states that your \\(DATA\\) \\(Y\\) are produced by a probability model having mathematical form \\(p(y)\\)."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#variability",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#variability",
    "title": "Introduction to important concepts in Statistics",
    "section": "Variability",
    "text": "Variability\n\nWould a deterministic model explain how people feel after a traumatic event?\nCan we plug in values in a deterministic function to predict your attention span while driving?\nYou must use a probabilistic (stochastic) models to study natural variability."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#parameters",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#parameters",
    "title": "Introduction to important concepts in Statistics",
    "section": "Parameters",
    "text": "Parameters\n\nA parameter is a numerical characteristic of the data-generating process, one that is usually unknown but often can be estimated using data.\n\n\\[\\begin{equation}\nY \\sim \\beta_{0} + \\beta_{1}X\n\\end{equation}\\]\nFor instance, in the model showed above, we have two unknown parameters, this model produces data \\(Y\\). The unknown parameters are represented with greek letters, for instance the letter beta in the example above.\n\n\n\n\n\n\n\nMantra\n\n\nModel produces data.\nModel has unknown parameters.\nData reduce the uncertainty about the unknown parameters."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models",
    "title": "Introduction to important concepts in Statistics",
    "section": "Purely Probabilistic Statistical Models",
    "text": "Purely Probabilistic Statistical Models\n\nIn a probabilistic model the variable \\(Y\\) is produced at random. This statement is represented by \\(Y \\sim p(y)\\).\n\\(p(y)\\) is called a probability density function (pdf).\nA pdf assigns a likelihood to your values. I will explain this in the next sessions.\nIMPORTANT: A purely probabilistic statistical model states that a variable \\(Y\\) is produced by a pdf having unknown parameters. In symbolic shorthand, the model is given as \\(Y \\sim p(y|\\theta )\\). This greek letter \\(\\theta\\) is called “theta”."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Purely Probabilistic Statistical Models II",
    "text": "Purely Probabilistic Statistical Models II\n\nLet’s think that Nature is also observable in our class. Our class belongs to the data generating process of grades in stats classes in the world. Let’s also assume that the data generating process of grades is normally distributed with a mean of 78 (out of 100) and standard deviation of 1. We can also imagine that we have a single sample of 100 students that have taken stats classes. Let’s check the graph…\n\n\nR codeFigure\n\n\n\nset.seed(1234)\n\ndataProcess &lt;- rnorm(16000000, \n                     mean = 78, \n                     sd = 1)\ngrades &lt;- rnorm(100,  \n                mean = 78, \n                sd = 1)\n\nplot(density(dataProcess), \n     lwd = 2, \n     col = \"red\",\n     main = \"DATA generating process\", \n     xlab = \"Grade\", \n     ylab = \"p(x)\")\n\nlines(density(grades), \n      col = \"blue\", \n      lwd = 2)\n\nlegend(80, 0.4, \n       legend=c(\"Data Process\", \"My sample\"),\n       col=c(\"red\", \"blue\"), lty=1)"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models-iii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#purely-probabilistic-statistical-models-iii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Purely Probabilistic Statistical Models III",
    "text": "Purely Probabilistic Statistical Models III\n\nI just used the word “assume” , probabilistic models have assumptions. In the previous example we assumed:\n\nThe data generating process is normally distributed.\nWe assumed a value for the mean.\nWe assumed a value for the standard deviation."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-inference",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-inference",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Inference",
    "text": "Statistical Inference\n\nNormally if we flip a coin we have around 0.50 probability of getting tails, and also around 0.50 of getting heads. But, what would happen if we bend the coin? Are you sure you’ll get 0.50 probability? Can we assume the same probability?\n\n\nProbability distribution for a Bent Coin\n\n\nOutcome, \\(y\\)\n\\(p(y)\\)\n\n\n\n\nTails\n1 - \\(\\pi\\)\n\n\nHeads\n\\(\\pi\\)\n\n\nTotal\n1.00\n\n\n\n\nNow we have uncertainty, we can only assume that there is a probability represented by \\(\\pi\\). That’s the only think we know."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#statistical-inference-ii",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#statistical-inference-ii",
    "title": "Introduction to important concepts in Statistics",
    "section": "Statistical Inference II",
    "text": "Statistical Inference II\n\nWe need to collect data to reduce the uncertainty about the unknown parameters.\nThe reduction in uncertainty about model parameters that you achieve when you collect data is called statistical inference"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#estimands-estimates-and-estimators",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#estimands-estimates-and-estimators",
    "title": "Introduction to important concepts in Statistics",
    "section": "Estimands, estimates, and estimators",
    "text": "Estimands, estimates, and estimators\n\nI mentioned before that DATA is RANDOM, this means that there is a distribution of values in the universe for what you are studying.\nFor instance, there is a random distribution of possible romantic relationships for all of you.\nBut, we need to make our research idea more operative. Let me try:\n\nWe need to define an estimand, we could use \\(\\mu\\) as our estimand.\n\n\\(\\mu\\) will be our data generating process mean.\nIn our example it could the average number of romantic relationships in young adults in United States. This process must have a value for \\(\\mu\\) but we don’t know it in real life.\n\nWe will also need a random estimator, in this case we can use the sample mean as our estimator. Why? Because we can assume that there is a distribution of random means. We could assume that each time I ask: how many romantic relationships have you had? I will get a different mean value.\nWe will need also an estimate which is a particular observation on our estimator. For example, if I ask you today how many romantic relationships have you had? The mean could be \\(\\bar{y} = 4\\). This is a fixed data (lowercase) point that comes from our estimator."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time",
    "title": "Introduction to important concepts in Statistics",
    "section": "Simulation time",
    "text": "Simulation time\n\nFollowing my example of number of romantic relationships, we could simulate data using our estimand (\\(\\mu\\)).\nLet’s assume \\(\\mu=5\\) , we want to simulate multiple data sets, let’s simulate 2000 data sets. This is equivalent to conduct the same study 2000 times.\nEach data set contains 13 observations, why? We are assuming we are asking the same question 2000 times to multiple classes similar to our class.\nWe will use our estimator the sampled mean to find out if our data is close the mean of the process which is \\(\\mu=5\\).\n\n\n\nShow the code\n1library(purrr)\n\nset.seed(1236)\n\n2miu &lt;- 5\n3N &lt;- 13\n4iterations &lt;- 2000\n\nmultipleSamples &lt;- map(1:iterations, ~rpois(N,miu)) |&gt;\n5  (function(.) sapply(., mean))()\n\n\n\n1\n\nPackage that helps to repeat the same action.\n\n2\n\nThe estimand.\n\n3\n\nNumber of observations in each simulated data set.\n\n4\n\nNumber of data sets generated\n\n5\n\nThe estimated mean in each data set."
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time-results",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time-results",
    "title": "Introduction to important concepts in Statistics",
    "section": "Simulation time: Results",
    "text": "Simulation time: Results\n\n\nShow the code\nlibrary(ggplot2)\n\nggplot(data.frame(multipleSamples), aes(x=multipleSamples)) + \n  geom_histogram(color=\"black\", fill=\"white\")+\n  geom_vline(aes(xintercept=mean(multipleSamples)),\n            color=\"blue\", \n            linetype=\"dashed\", \n            linewidth=1) +\n  ggtitle(\"Histogram of generated means\")+\n  xlab(\"Sampled means\") +\n  ylab(\"Frequency\")+\n  annotate(geom= \"text\", x=4, y=150, label=\"Mean = 5.002\",\n              color=\"blue\")+\n  theme_classic()"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time-results-1",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#simulation-time-results-1",
    "title": "Introduction to important concepts in Statistics",
    "section": "Simulation time: Results",
    "text": "Simulation time: Results\n\n\n\nTaken from Richard McElreath (2024)"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html",
    "href": "advancedMethods/Lecture2/Distributions.html",
    "title": "Probability distributions and random variables",
    "section": "",
    "text": "In our last session we talked about DATA (uppercase) and data (lowercase).\nWe studied that DATA means all the possible values in Nature for instance, all the college students in US versus a single fix observation called data (lowercase)\nIn this session is important to remember this convention.\nDATA means random variable whereas data means fixed quantities.\nFor instance, in a dice you have 6 possible values, when roll a dice ; let’s imagine you get 5 , then your data is y = 5 while you DATA is any possible value from 1 to 6.\nRandom variables have a distribution, this means DATA is random."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#previous-class",
    "href": "advancedMethods/Lecture2/Distributions.html#previous-class",
    "title": "Probability distributions and random variables",
    "section": "Previous class",
    "text": "Previous class\n\nIn our last session we talked about DATA (uppercase) and data (lowercase).\nWe studied that DATA means all the possible values in Nature for instance, all the college students in US versus a single fix observation called data (lowercase)\nIn this session is important to remember this convention.\nDATA means random variable whereas data means fixed quantities.\nFor instance, in a dice you have 6 possible values, when roll a dice ; let’s imagine you get 5 , then your data is y = 5 while you DATA is any possible value from 1 to 6.\nRandom variables have a distribution, this means DATA is random."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#types-of-data",
    "href": "advancedMethods/Lecture2/Distributions.html#types-of-data",
    "title": "Probability distributions and random variables",
    "section": "Types of DATA",
    "text": "Types of DATA\n\nNominal DATA: These are DATA whose possible values are essentially labels (or, as the word nominal suggests, names), with no numerical value. Ex: marital status, job title, political affiliation.\nContinuous DATA: These are numerical DATA whose possible values lie in a continuum, or in a continuous range. For instance the duration of this class in seconds.\nOrdinal DATA: These types of DATA are intermediate between nominal and continuous. Unlike nominal data, which can be numbers without intrinsic order such as 1 = male and 2 = female, ordinal DATA are numbers that reflect an intrinsic order, hence, the name ordinal. Example, education level, ranking of your favorite dessert."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#nominal-data",
    "href": "advancedMethods/Lecture2/Distributions.html#nominal-data",
    "title": "Probability distributions and random variables",
    "section": "Nominal data",
    "text": "Nominal data\n\n\nShow the code\nlibrary(ggplot2)  ### &lt;- this is a package in R to create pretty plots.\n\nrumination &lt;- read.csv(\"ruminationComplete.csv\") \nrumination$sex &lt;- factor(rumination$sex, labels = c(\"female\", \"male\"))\nggplot(data = rumination, aes(x = sex)) + \n  geom_bar()+ theme_bw()"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#continuous-data",
    "href": "advancedMethods/Lecture2/Distributions.html#continuous-data",
    "title": "Probability distributions and random variables",
    "section": "Continuous data",
    "text": "Continuous data\n\nggplot(data = rumination, aes(x = ageMonths)) + \n  geom_histogram() + theme_bw()"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#ordinal-data",
    "href": "advancedMethods/Lecture2/Distributions.html#ordinal-data",
    "title": "Probability distributions and random variables",
    "section": "Ordinal data",
    "text": "Ordinal data\n\n\nShow the code\nrumination$grade &lt;- factor(rumination$grade, \n                           labels = c(\"seventh\", \n                                      \"eight\", \n                                      \"nineth\", \n                                      \"tenth\", \n                                      \"eleventh\"))\nggplot(data = rumination, aes(x = grade)) + \n  geom_bar()"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#discrete-vs.-continuous",
    "href": "advancedMethods/Lecture2/Distributions.html#discrete-vs.-continuous",
    "title": "Probability distributions and random variables",
    "section": "Discrete vs. Continuous",
    "text": "Discrete vs. Continuous\n\nNominal and Ordinal DATA are considered discrete DATA, this means that values can be listed.\nContinuous DATA cannot be listed because all possible values lie in a continuum."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions",
    "href": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions",
    "text": "Discrete Probability Distribution Functions\n\nIn our previous class I mentioned the concept of probability density function (pdf), in simple words, the pdf is the model for a random variable.\nFor example, you can assume that the pdf of the normal distribution produces you DATA. DATA such as grades in all the statistics classes in US can be assumed to be generated by a normal distributed model.\nWe will revisit more about the normal distribution in this class."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-1",
    "href": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-1",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions",
    "text": "Discrete Probability Distribution Functions\n\nBut let’s focus on discrete distributions first.\nAs I said before, a discrete variable can be listed as showed below:\n\n\n\nIn this table \\(p(y)\\) represents the probability of any variable \\(y\\).\nWhen we talk about discrete distributions we talk about probability mass function , when the variable is continuous we used the concept probability density function."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-ii",
    "href": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-ii",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions II",
    "text": "Discrete Probability Distribution Functions II\n\nA Discrete pdf or probability mass function has several requirements:\n\nThe probabilities are NEVER negative, but they could be zero.\nThe total probability is 1.0 or 100%"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-iii",
    "href": "advancedMethods/Lecture2/Distributions.html#discrete-probability-distribution-functions-iii",
    "title": "Probability distributions and random variables",
    "section": "Discrete Probability Distribution Functions III",
    "text": "Discrete Probability Distribution Functions III\n\nProbability distributions are the most important component in statistics.\nWe always talk about the normal distribution model, however there several different distributions in the universe. But we are going to study only a few of them.\nDo you remember the coin example? The right model for the coin problem is the pdf discovered by Bernoulli. In fact, many people call it the Bernoulli distribution. Its formal representation is as follows:\n\n\\[\\begin{equation}\n\np(y|\\pi) = \\pi^{y}(1-\\pi)^{1-y}\n\n\\end{equation}\\]\n\nNo worries about the interpretation of the formula, we might have time to study this distribution at the end of this course. Let’s focus on how we can use this model."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#simulation-time",
    "href": "advancedMethods/Lecture2/Distributions.html#simulation-time",
    "title": "Probability distributions and random variables",
    "section": "Simulation time!",
    "text": "Simulation time!\n\nRemember the mantra: Models produce data, this means we can produce data using the Bernoulli model:\nLet’s think about the tossing the coin, we can assign numerical values to the the results, for instance each time we get a head we will code it as 1.00, when we get tail we’ll code it as 0.00.\n\n\nset.seed(1236)\ntossCoin &lt;- rbinom(100,1,0.50)\nsum(tossCoin)/length(tossCoin)\n\n[1] 0.48\n\n\n\nIn the R code above, I’m simulating tossing a coin randomly 100 times using the discrete Bernoulli distribution, after doing that I get exactly zeros and ones, as we expected. Now, following the coin toss model, we should have a probability \\(p(heads)= 0.50\\) in the long run.\nBut wait? Why we don’t get 0.50? According to our simulated data we have a probability of 0.48.\n\nLet’s try again but this time lets generate 1006 values:\n\nset.seed(1236)\ntossCoin &lt;- rbinom(1006,1,0.50)\nsum(tossCoin)/length(tossCoin)\n\n[1] 0.5\n\n\n\nThat’s magic! Now we have 0.50 probability of getting heads, what happened?\nWhen we have more observed data we are closer to the DATA generating process value. The value for our unknown parameter is 0.50 according to our model, after adding more data, we are close to that number. This is also called the Law of Large Numbers."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#section",
    "href": "advancedMethods/Lecture2/Distributions.html#section",
    "title": "Probability distributions and random variables",
    "section": "",
    "text": "Bartoš et al. (2023) tested several questions associated to flipping coins.\nSeveral participants flipped coins up to a total of 350,757 flips.\nLet’s play with their data.\nHow many times will the coin land on the same side as where it started?"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#we-dont-simulate-fliping-coins-all-the-time-1",
    "href": "advancedMethods/Lecture2/Distributions.html#we-dont-simulate-fliping-coins-all-the-time-1",
    "title": "Probability distributions and random variables",
    "section": "We don’t simulate fliping coins all the time…",
    "text": "We don’t simulate fliping coins all the time…\n\n\nShow the code\nlibrary(metadat)\nlibrary(ggplot2)\n\nflipsData &lt;- dat.bartos2023\n\nggplot(flipsData, aes(x=flips , y=as.factor(person))) + \n  geom_col(color = \"#B19110\", fill= \"#830A1A\") + \n  xlab(\"Number of flips\") +\n  ylab(\"Participant\")+\n  ggtitle(\"Total number of flips per person\")+\n  theme_classic()"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#we-dont-simulate-fliping-coins-all-the-time-2",
    "href": "advancedMethods/Lecture2/Distributions.html#we-dont-simulate-fliping-coins-all-the-time-2",
    "title": "Probability distributions and random variables",
    "section": "We don’t simulate fliping coins all the time…",
    "text": "We don’t simulate fliping coins all the time…\n\n\nShow the code\nlibrary(tidyverse)\n\nflipsData &lt;- flipsData |&gt;\n  mutate(proportionSame = same / flips)\n\nggplot(flipsData, aes(x=proportionSame)) + \n   geom_density(fill = \"#00609C\", alpha =  0.3)+\n  geom_histogram(color = \"#B19110\",\n                 fill= \"#830A1A\",\n                 bins = 10)+\n  theme_classic()+\n  ggtitle(\"Histogram of proportions of times coin landed on the same side as where it started\")+\n  ylab(\"Frequencies\")+\n  xlab(\"Proportions\")+\n  annotate(\"text\", x = 0.56, y = 30, label = \"Mean = 0.51, SD = 0.02\")"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#simulation-time-ii",
    "href": "advancedMethods/Lecture2/Distributions.html#simulation-time-ii",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! II",
    "text": "Simulation time! II\n\nHopefully, at this point the notion of DATA versus data starts to make sense.\nI want to introduce another model that we might study further if time allows, this new model is the Poisson pdf, a.k.a Poisson distribution.\nThe function form of the Poisson distribution is:\n\n\\[\\begin{equation}\np(y|\\lambda) = \\frac{\\lambda^{y}e^{-\\lambda}}{y!}\n\\end{equation}\\]\n\nLet’s untangle the formula:\n\nThe symbol \\(\\lambda\\) is the Greek letter “lambda”, it represents the theoretical average.\nThe letter \\(e\\) is called Euler’s constant, its numerical value is approximately \\(e = 2.71828...\\). You might remember this symbol when you did exponential functions in high school or college. -The term \\(y!\\) is “y factorial” , you might remember factorials from high school or college, they are used a lot in probability. It can be defined as:\n\n\\[\\begin{equation}\n\ny! =  \\textrm{1 x 2 x 3 x 4 x ...x y}\n\\end{equation}\\]"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#simulation-time-iii",
    "href": "advancedMethods/Lecture2/Distributions.html#simulation-time-iii",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! III",
    "text": "Simulation time! III\nAs always, it is not required to remember the Poisson formulation, but it is more important to understand how we can use this model to explain our observed data:\n\nLet’s imagine that you want to find a model to describe attendance in high school. We could try to generate data that assumes that a high school student has on average 8 absences.\n\n\n\nShow the code\nset.seed(1236)\nabsences &lt;- 8\nN &lt;- 300\ndataGenerated &lt;- rpois(N, absences)\ncat(\"The mean is \", mean(dataGenerated))\n\n\nThe mean is  7.98\n\n\n\nOur model generated 300 observations, with an average number of absences = 7.98. It is likely that this number will be closer to 8 if you simulate more observations."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#simulation-time-iv",
    "href": "advancedMethods/Lecture2/Distributions.html#simulation-time-iv",
    "title": "Probability distributions and random variables",
    "section": "Simulation time! IV",
    "text": "Simulation time! IV\nThe probability of each value in our tiny Poisson simulation can be represented in list form as:\n\n\nThe Poisson pdf is a good model to represent counts such as the number of times you successfully wake up early and do exercise, the number of shoes, the number of family members in each household and many other counting variables."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions",
    "href": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions",
    "text": "Continuous Probability Distribution Functions\n\n\n\n\n\nContinuous distributions have several differences compare to discrete distributions.\n\nWhen the density plot corresponds to a continuous distribution, the \\(y\\)-axis does not show probability.\nWhen you estimate the density of continuous distribution you are estimating the “relative likelihood” For example, in this plot 50 kg has a likelihood of around 0.014, 55 kg has a likelihood of around 0.021. This means that is more likely to observe 55 kg compare to 50 kg."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-ii",
    "href": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-ii",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions II",
    "text": "Continuous Probability Distribution Functions II\n\n\n\n\nImportant: - The probability of a value in a density plot can only be estimated by calculating by estimating the area under the curve.\n\nThe total area under the curve in discrete and continuous functions is always equal to 1.0."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-iii",
    "href": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-iii",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions III",
    "text": "Continuous Probability Distribution Functions III\n\n\n\n\n\nIn this graph Westfall & Henning (2013) changed the metric from kg to metric tons.\nThis helps to show you how to estimate the probability of 0.08 metric tons from a density plot.\nThen the probability of observing a weight (in metric tons) in the range \\(0.08 \\pm 0.005\\) is approximately 17 × 0.01 = 0.17. Or in other words, about 17 out of 100 people will weigh between 0.075 and 0.085 metric tons, equivalently, between 75 and 85 kg.\nBut this is is just an approximation, integrals would give us the exact probability, but we will avoid calculus today."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-iv",
    "href": "advancedMethods/Lecture2/Distributions.html#continuous-probability-distribution-functions-iv",
    "title": "Probability distributions and random variables",
    "section": "Continuous Probability Distribution Functions IV",
    "text": "Continuous Probability Distribution Functions IV\n\n\nRequirements for a Continuous pdf\n\n\nThe values on the \\(y\\)-axis are always positive, it means the likelihood is always positive.\nGiven that the area under curve represents PROBABILITY, the total area is always equals to 1.0."
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution",
    "text": "More distributions: normal distribution\n\n\n\\[\\begin{align*}\np(y|\\mu, \\sigma^2) &= \\frac{1}{\\sqrt{2\\pi\\sigma}} exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}\\\\\n\\end{align*}\\]\nAt this point it looks a little bit esoteric and dark, I will untangle the components of the normal distribution formulation.\n\nThe component \\(\\frac{1}{\\sqrt{2\\pi\\sigma}}\\) is what we call a constant, it is a fixed quantity. Every probability density function will have a constant. This constant number helps to make sure that the area under the curve of a distribution is equal to 1 (believe me this is true!).\n\n\n\nThe second component \\(exp \\left\\{ \\frac{-(y-\\mu)^2}{2\\sigma^2} \\right\\}\\) is what is called kernel it is like the “core” of the area under the curve.\nThe third important component is \\(p(y|\\mu, \\sigma^2)\\), in this expression; \\(y\\) represents any numeric value, \\(\\mu\\) represents the mean of the distribution and \\(\\sigma\\) is the variance of the distribution. Now we can read this third component as “probability of any number given a mean, and a variance”"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-ii",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-ii",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution II",
    "text": "More distributions: normal distribution II\n\nAs always, let me simulate some values using R, imagine we need to find a model that best describes the weight distribution in College students. For this task, we could assume the normal distribution model with a mean of 65 kg and standard deviation equals to 1.0.\n\n\n\nShow the code\n### Random starting number\nset.seed(359)\n### Mean or average in Kg \nMean &lt;- 65\n## Standard Deviation\nSD &lt;- 1\n## Number of observations\nN &lt;-  300\n### Generated values from the normal distribution\ndata_1 &lt;-  rnorm(n = N, mean = Mean, sd = SD )\ndata_1\n\n\n  [1] 65.90960 64.71476 63.83066 63.44413 65.46502 65.31280 66.48675 65.87327\n  [9] 64.84400 64.67924 62.12089 63.97994 64.42622 65.01382 64.85000 65.35106\n [17] 64.86783 64.98873 64.54516 64.14890 63.47823 64.45281 66.13445 64.45036\n [25] 65.66304 65.57285 62.93050 63.46320 63.99792 64.49344 65.89572 64.24975\n [33] 65.90253 65.29621 65.85397 64.34964 65.16825 64.80732 65.56075 66.39590\n [41] 65.68950 65.33977 64.06039 66.64493 64.47345 65.04675 65.85947 67.92296\n [49] 66.34708 65.26185 64.95761 65.18777 64.03615 65.27033 65.84343 64.16392\n [57] 65.61956 64.48135 67.40605 67.23284 65.12713 64.07411 63.98220 65.70498\n [65] 64.88308 65.93083 64.72232 66.42429 65.23638 64.81533 66.98712 63.34341\n [73] 64.93697 63.42742 65.41067 64.40579 64.70454 64.31235 65.16420 63.88185\n [81] 64.43832 64.63143 64.33169 66.17669 65.05522 64.00396 65.43599 66.05379\n [89] 65.39605 63.63870 66.18342 66.61002 65.84883 64.91560 65.79335 65.32865\n [97] 65.93242 64.77581 64.21611 65.83586 67.52724 64.90828 64.33488 64.42051\n[105] 65.95834 66.25977 64.91274 65.06262 64.47752 64.62498 65.13740 64.24417\n[113] 65.36593 67.36820 65.70180 64.33536 65.96453 64.97927 68.24824 65.39837\n[121] 64.00358 65.19193 64.43649 64.03446 63.96788 64.99354 64.32949 66.39454\n[129] 65.46359 66.03655 64.66212 64.30517 64.21899 64.55146 63.81233 65.18873\n[137] 65.30952 64.32071 65.17876 64.50110 64.66146 65.01361 64.74170 65.08205\n[145] 64.42346 64.32447 65.23062 64.46689 66.69426 64.94310 65.24772 65.00605\n[153] 63.93441 67.81550 64.76292 66.26200 64.15244 64.70025 65.06429 65.57770\n[161] 63.39030 65.24833 64.62538 65.88314 64.93269 65.01867 64.83311 64.16538\n[169] 65.08892 65.60057 65.15178 64.51507 64.65132 63.98354 64.43078 63.79769\n[177] 64.79643 65.25849 63.31048 65.06860 64.70367 64.48503 64.58913 65.50374\n[185] 62.96262 64.38087 66.59937 66.05083 63.79779 66.04224 64.68900 65.01352\n[193] 64.49640 63.70097 63.88770 67.28460 66.72947 63.65179 65.44370 65.67320\n[201] 65.26623 64.82400 64.67668 64.78615 64.89735 65.16560 64.86284 63.69684\n[209] 64.36736 65.50050 64.68430 67.10004 65.13415 65.07864 65.75652 66.96074\n[217] 64.67984 65.33965 64.32788 64.99155 63.72848 64.85579 64.81778 64.67416\n[225] 66.54865 65.22833 65.81325 64.69429 66.10983 64.80612 63.39200 65.88671\n[233] 64.01276 64.33288 63.46912 66.61400 64.11193 63.11924 64.71432 63.55637\n[241] 65.91217 64.50794 64.35599 67.67487 66.97199 65.09739 66.49791 65.46359\n[249] 65.92001 65.91692 63.40351 64.27549 66.08439 64.05049 65.84046 63.69535\n[257] 64.29846 65.15456 63.72143 66.81415 65.73656 63.93622 63.74965 65.40493\n[265] 64.74493 64.89682 64.95052 62.67580 65.33750 65.48994 64.24487 65.38974\n[273] 66.34487 63.93986 66.39666 64.63785 65.39375 65.40279 65.28578 65.01735\n[281] 66.74379 66.87585 64.91307 66.32705 64.25904 64.07473 65.96503 65.33417\n[289] 63.96063 65.42224 62.30688 64.32297 66.52509 63.99097 64.86705 65.34434\n[297] 64.17483 65.01194 64.99918 65.14846"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iii",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iii",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution III",
    "text": "More distributions: normal distribution III\n\nWe can check the density distribution of this sample that comes from the normal distribution model:\n\n\nplot(density(data_1), \n     xlab = \"Weight (kg)\", \n     ylab = \"p(y) or likelihood\", \nmain = \"Normal density function of college student's weight\" \n)"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iv",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iv",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution IV",
    "text": "More distributions: normal distribution IV\n\nWe can also imagine that we need a model to describe the weight of hospital patients. But, this time probably, we will see more overweight individuals because of diverse health problems. Then, a mean = 65 kg is not realistic (143.3 lbs), probably we should assume a mean = 90 kg (198.42 lbs), and probably the observed data will be spread out, so we can use a larger values for the standard deviation, perhaps something around 10.\n\n\n\nShow the code\n### Random starting number\nset.seed(359)\n### Mean or average in Kg \nMean &lt;- 90\n## Standard Deviation\nSD &lt;- 10\n## Number of observations\nN &lt;-  300\n### Generated values from the normal distribution\ndata_2 &lt;-  rnorm(n = N, mean = Mean, sd = SD )\ndata_2\n\n\n  [1]  99.09603  87.14758  78.30664  74.44127  94.65025  93.12797 104.86753\n  [8]  98.73265  88.43998  86.79235  61.20891  79.79938  84.26221  90.13819\n [15]  88.49998  93.51056  88.67827  89.88726  85.45157  81.48895  74.78235\n [22]  84.52814 101.34451  84.50357  96.63045  95.72855  69.30499  74.63198\n [29]  79.97919  84.93441  98.95721  82.49745  99.02532  92.96213  98.53972\n [36]  83.49641  91.68247  88.07319  95.60747 103.95905  96.89498  93.39768\n [43]  80.60386 106.44925  84.73451  90.46749  98.59471 119.22961 103.47075\n [50]  92.61846  89.57613  91.87772  80.36150  92.70328  98.43425  81.63920\n [57]  96.19561  84.81348 114.06050 112.32838  91.27126  80.74112  79.82199\n [64]  97.04981  88.83082  99.30830  87.22319 104.24293  92.36383  88.15335\n [71] 109.87124  73.43411  89.36973  74.27417  94.10665  84.05789  87.04545\n [78]  83.12347  91.64199  78.81849  84.38325  86.31432  83.31688 101.76688\n [85]  90.55224  80.03963  94.35986 100.53789  93.96049  76.38699 101.83422\n [92] 106.10019  98.48829  89.15598  97.93345  93.28650  99.32421  87.75811\n [99]  82.16114  98.35859 115.27235  89.08279  83.34876  84.20506  99.58339\n[106] 102.59770  89.12740  90.62618  84.77518  86.24982  91.37400  82.44167\n[113]  93.65933 113.68198  97.01798  83.35357  99.64534  89.79270 122.48244\n[120]  93.98373  80.03576  91.91926  84.36490  80.34458  79.67885  89.93544\n[127]  83.29487 103.94544  94.63586 100.36548  86.62125  83.05169  82.18990\n[134]  85.51461  78.12330  91.88730  93.09520  83.20710  91.78756  85.01100\n[141]  86.61460  90.13608  87.41698  90.82048  84.23460  83.24472  92.30620\n[148]  84.66887 106.94258  89.43101  92.47715  90.06048  79.34405 118.15496\n[155]  87.62915 102.61997  81.52439  87.00250  90.64290  95.77699  73.90302\n[162]  92.48329  86.25377  98.83139  89.32694  90.18671  88.33110  81.65385\n[169]  90.88921  96.00573  91.51776  85.15071  86.51324  79.83544  84.30776\n[176]  77.97693  87.96432  92.58489  73.10476  90.68604  87.03665  84.85029\n[183]  85.89134  95.03740  69.62624  83.80868 105.99374 100.50826  77.97789\n[190] 100.42236  86.88997  90.13518  84.96399  77.00973  78.87698 112.84603\n[197] 107.29474  76.51790  94.43699  96.73195  92.66227  88.23995  86.76681\n[204]  87.86150  88.97352  91.65598  88.62836  76.96842  83.67362  95.00498\n[211]  86.84302 111.00035  91.34154  90.78639  97.56522 109.60737  86.79841\n[218]  93.39651  83.27879  89.91554  77.28481  88.55791  88.17783  86.74163\n[225] 105.48651  92.28331  98.13252  86.94293 101.09826  88.06121  73.91998\n[232]  98.86712  80.12760  83.32878  74.69121 106.14002  81.11932  71.19242\n[239]  87.14321  75.56373  99.12171  85.07938  83.55992 116.74873 109.71989\n[246]  90.97392 104.97909  94.63592  99.20012  99.16920  74.03514  82.75492\n[253] 100.84388  80.50494  98.40457  76.95353  82.98457  91.54557  77.21432\n[260] 108.14149  97.36556  79.36224  77.49653  94.04927  87.44927  88.96823\n[267]  89.50524  66.75803  93.37497  94.89937  82.44871  93.89736 103.44874\n[274]  79.39864 103.96662  86.37846  93.93755  94.02794  92.85782  90.17349\n[281] 107.43790 108.75847  89.13072 103.27050  82.59045  80.74730  99.65032\n[288]  93.34172  79.60634  94.22238  63.06883  83.22968 105.25093  79.90966\n[295]  88.67048  93.44337  81.74835  90.11941  89.99179  91.48458"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iv-1",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-iv-1",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution IV",
    "text": "More distributions: normal distribution IV\n\nWe can check now the probability density function of our simulated hospital sample:\n\n\nplot(density(data_2), \n     xlab = \"Weight (kg)\", \n     ylab = \"p(y) or likelihood\", \nmain = \"Normal density function hospital patients' weight\" \n)"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-v",
    "href": "advancedMethods/Lecture2/Distributions.html#more-distributions-normal-distribution-v",
    "title": "Probability distributions and random variables",
    "section": "More distributions: normal distribution V",
    "text": "More distributions: normal distribution V\n\nR codePlot\n\n\n\nlibrary(ggplot2) ### &lt;- this is a package in R to create pretty plots.\n\ndataMerged &lt;- data.frame(\n  group =c(rep(\"College\", 300),\n           rep(\"Hospital\", 300)),\n  weight = c(data_1, data_2))\n\nggplot(dataMerged , aes(x=weight, fill=group)) +\n  geom_density(alpha=.25) + \n  theme_bw()+\n  labs(title = \"College and Hospital Weight Density Function\") + \n  xlab(\"Weight (kg)\") + \n  ylab(\"p(y) or likelihood\")"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#what-is-the-main-message-here",
    "href": "advancedMethods/Lecture2/Distributions.html#what-is-the-main-message-here",
    "title": "Probability distributions and random variables",
    "section": "What is the main message here?",
    "text": "What is the main message here?\n\nThe main message is: Models produce data!\nRemember we were talking about NATURE and DATA (uppercase)?\n\nOur statistical model (e.g. normal distribution) is our approximation to DATA, we create models that will generate observed data. That’s why in simulations sometimes our model is called “population model”, and from our data generating model we produce observed data.\n\nRemember when we talked about random variables vs. fixed quantities?\n\nWhen we refer to the DATA (population model) the variables are random, but when we generate data like we did in R that data are fixed quantities similar to collect data on campus related to dating strategies, once you collect your data, those numbers are fixed quantities that were generate by a natural process (data generating process). We cannot control the natural process like we do in simulation.\n\nWe reduce uncertainty when we add more observations. We will study more about it in the next classes."
  },
  {
    "objectID": "advancedMethods/practice1/practice1.html",
    "href": "advancedMethods/practice1/practice1.html",
    "title": "Practice 1",
    "section": "",
    "text": "R is a programming language mostly used in statistics. It was actually created by programmers who were also statisticians. As Matloff (2011) mentions in his book; R was inspired by the statistical language S developed by At&T. S stands for “statistics” and it was written based on C language. After S was sold to a small company, S-plus was created with a graphical interface.\n\n\n\n\n\n\n\n\n\nThere are many reasons, but I’ll list just a few of them:\n\nR is an open source language, you are always able to check what is behind the code. You could even create your own language based on R if you need it.\nR is free and freely distributed. You don’t have to pay for anything. You just download the installer, and then you start playing with data!\nR is superior and more powerful than many commercial software.\nYou can install R on multiple operated systems such as Windows, Mac, Linux, and Chrome OS (you need Linux behind scenes).\nR is not only useful for data analysis, you can generate automatic reports in pdf, Word or create a webpage or dashboard to display your results. In fact, this document and my presentations were all created using R.\nThe R community is the biggest community of users in statistics. You can search on the Internet any problem, and you will find thousands of possible answers for free by thousands of users.\nR has one of the largest repositories with 19000 free packages!\nIf you learn R you will feel more comfortable learning new scripting software or languages."
  },
  {
    "objectID": "advancedMethods/practice1/practice1.html#what-is-r-programming-language",
    "href": "advancedMethods/practice1/practice1.html#what-is-r-programming-language",
    "title": "Practice 1",
    "section": "",
    "text": "R is a programming language mostly used in statistics. It was actually created by programmers who were also statisticians. As Matloff (2011) mentions in his book; R was inspired by the statistical language S developed by At&T. S stands for “statistics” and it was written based on C language. After S was sold to a small company, S-plus was created with a graphical interface.\n\n\n\n\n\n\n\n\n\nThere are many reasons, but I’ll list just a few of them:\n\nR is an open source language, you are always able to check what is behind the code. You could even create your own language based on R if you need it.\nR is free and freely distributed. You don’t have to pay for anything. You just download the installer, and then you start playing with data!\nR is superior and more powerful than many commercial software.\nYou can install R on multiple operated systems such as Windows, Mac, Linux, and Chrome OS (you need Linux behind scenes).\nR is not only useful for data analysis, you can generate automatic reports in pdf, Word or create a webpage or dashboard to display your results. In fact, this document and my presentations were all created using R.\nThe R community is the biggest community of users in statistics. You can search on the Internet any problem, and you will find thousands of possible answers for free by thousands of users.\nR has one of the largest repositories with 19000 free packages!\nIf you learn R you will feel more comfortable learning new scripting software or languages."
  },
  {
    "objectID": "advancedMethods/practice1/practice1.html#lets-jump-into-r",
    "href": "advancedMethods/practice1/practice1.html#lets-jump-into-r",
    "title": "Practice 1",
    "section": "Let’s jump into R!",
    "text": "Let’s jump into R!\nThat was just a tiny explanation to introduce R. In this part, I’ll ask you to replicate some exercises. Don’t feel like navigating alone, I’ll create videos to show you how to solve them. Also, I’m going to assume that you already know how to install R and RStudio. If not, this link will help.\n\nR is an object oriented language\nProgramming languages such as Python or R are languages that create objects. All the elements in R will be virtual objects. Check the following case:\n\nnames_of_people &lt;- c(\"Karla\", \"Pedro\", \"Andrea\", \"Esteban\") \n\nIn this chunk of R code, I created an object called names_of_people. This object will have properties, similar to physical objects in real life.\n\n\n\n\n\n\nImportant\n\n\n\nNotice the presence of the characters &lt;-. This arrow assigns information to the object. You can press ALT + - in your keyboard to insert this arrow, Mac users should press CMD + -.\n\n\nOne of the properties of this type of object is the ability to print the contents in your console by typing its name and then run the code:\n\nnames_of_people\n\n[1] \"Karla\"   \"Pedro\"   \"Andrea\"  \"Esteban\"\n\n\n\n\nYou can run the code pressing CTRL + Enter on Windows or CMD + Enter on Mac.\n\n\n\n\n\n\nExcercise 1\n\n\n\n\nNow is your turn to create your own object. Copy my code an replace the names with names of countries. Then, call the object to print the content in the console. Copy your answer in a Word document or Google document. (8.33 points)\n\n\n\nI hope that was easy to do, we are going to walk slowly when learning R. The next property of our object is sub-setting. You can take one or two elements inside your object and print only a few elements saved in your object:\n\nnames_of_people[1]\n\n[1] \"Karla\"\n\n\nNotice that I’m indicating that I want to print only the first element inside my object. I’m using square brackets [] to indicate I want a “slice” of my object. I can do the same and indicate I want to print two elements:\n\nnames_of_people[c(1,3)]\n\n[1] \"Karla\"  \"Andrea\"\n\n\nIn this example I’m printing the elements in located in the first position and the third position.\n\n\n\n\n\n\nExcercise 2\n\n\n\n\nDo the same with your object containing names of countries. Print only the first and the third element of your object. (8.33 points)\n\n\n\nI haven’t told you what’s the name of this type of object. Similar to real life, objects can be classified into categories. In this case, this example is a “character vector”. In R when you use letters they should be wrapped with quotes. Also, by using the command c(), you are creating a vector. Do you remember the concept of vector in physics? This is something similar, vector could represent a vertical space or a horizontal space. In this case is just a horizontal vector with characters inside.\nVectors can also contain numbers, I’ll create a numeric vector containing the year of release of the main Star War movies\n\nstar_wars_years &lt;- c(1999,2002,2005,1977,1980,1983,2015,2017,2019)\n\nNice! Now we have a vector with numbers, we can also print only a few elements if we need it:\n\nstar_wars_years[c(1,2,5,7)]\n\n[1] 1999 2002 1980 2015\n\n\nIn this example I’m printing only the elements located in the positions 1,2,5, and 7.\n\n\n\n\n\n\nExcercise 3\n\n\n\n\nCreate a numeric vector with the years of Marvel movies corresponding to The Infinity Saga reported in this link CLICK HERE. After that, print only the elements located in positions 5,8, and 9. (8.33 points)\n\n\n\n\n\nOperations on objects\nObjects in R are elements that can be manipulated and transformed exactly like objects in real life. For instance pay attention to the following example:\n\nmath_score &lt;- c(50,86,96,87)\n\nenglish_score &lt;- c(10,25,36,56)\n\nenglish_score + math_score\n\n[1]  60 111 132 143\n\n\nIn the code above, I created two vectors reflecting the academic scores of two four students. The first student had a score of 50 in math whereas the score in English was 10. You may have noticed that I sum both vectors, the final result reflects the result of adding the first math_score plus the first english_score, then R does the same with the other elements in the vectors.\n\n\n\n\n\n\nExcercise 4\n\n\n\n\nCopy the code above, replace the + sign for a - (minus) sign. Then run the code, What happened when you did that ? (8.33 points)"
  },
  {
    "objectID": "advancedMethods/practice1/practice1.html#we-need-to-study-more-important-objects",
    "href": "advancedMethods/practice1/practice1.html#we-need-to-study-more-important-objects",
    "title": "Practice 1",
    "section": "We need to study more important objects",
    "text": "We need to study more important objects\nR has several types of objects. We will not study all of them because this is not a computer programming class (I wish!). Instead, I’ll introduce the most important objects to understand my assignments and code.\n\nData frames\nData frames are the most useful objects in this class, please read the information about data frame objects on this link.\n\n\n\n\n\n\nExcercise 5\n\n\n\n\nCreate a data frame object by copying the code below. Change the object’s name, you may named it “expenses”, then change the variable names in the example (e.g. variable1). Finally run the code. How many rows does this data frame have? How many columns does this data frame have? Can you tell what happened after running the function head() (8.33 points)\n\n\nExample &lt;- data.frame(variable1 = c(30,63,96),\n               variable2 = c(63,25,45),\n               variable3 = c(78,100,100),\n               variable4 = c(56,89,88))\n\nhead(Example)\n\n\n\n\n\n\n\n\n\n\nYou probably noticed that objects can have any name, right? It doesn’t matter the human language.\n\n\nEverything is an object… and everything is a function\n\n\nIt can be convenient to revisit this topic on this link. But, if you don’t have time, I’ll explain the concept of functions. A function is an object that performs an operation based on an input. The function will ask for input information and after that, the function will give an output.\nFunctions can be created with the command function() which is in simple words a function that creates other functions. Sounds redundant but it is an accurate statement!\n\n\n\n\n\n\n\n\nFor instance, we can create a function that calculates your age:\n\n1estimateAge &lt;- function(myBirthday){\n\nmyBirthday2 &lt;- as.Date(myBirthday)  \ntoday &lt;- Sys.Date() \n\n2age &lt;- difftime(today,\n                myBirthday2, \n                units = \"days\")/365\n\nmessage(\"Your age is\",\" \", age)\n}\n\n\n1\n\nThe argument is called “myBirthday” just type your date of birth (“Year-MM-Day)\n\n2\n\nThe function difftime() does the magic for us.\n\n\n\n\nThe new function estimateAge() only needs one argument, and that is any date of birth. That’s the input information that will help the function to give you a output, in this case a message with your estimated age.\n\n## Let's enter my date of birth\nestimateAge(\"1986-01-28\") \n\nYour age is 38.0602739726027\n\n\n\n\nI hope you are feeling fine, if not please free to insert a meme expressing how you feel in your answers for 3 extra points.\nYou might be thinking: Wait a second! Do we have to create our own functions all the time? The answer is NO!. R already provides tons of functions already programmed an ready to be used. If the function you need is not available in base R, you can download a package and install the package in your computer.\n\n\n\n\n\n\nImportant\n\n\n\nYou should check more information about functions and packages CLICKING HERE\n\n\n\n\n\n\n\n\nExcercise 6 (8.33 points)\n\n\n\n\nYou probably went to the link I recommended before, if not go a read it here. After reading the explanation about packages install the package tidyverse then, call the package. You can copy the following code and paste the code in your RStudio session. Remember to install the package first:\n\n\n## Installs the package tidyverse\ninstall.packages(\"tidyverse\") \n\n\nlibrary(tidyverse)\n\n\n1\n\nThe function summarise_all() comes from tidyverse package.\n\n\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nvar1 &lt;- rnorm(200)\nvar2 &lt;- rnorm(200)\nvar3 &lt;- rnorm(200)\n\ndata.frame(var1, \n           var2,\n           var3) |&gt;\n1  summarise_all(mean)\n\n         var1         var2       var3\n1 -0.08712924 -0.009195269 0.08312085\n\n\nCopy your code and output in a Word document or Google document.\n\n\nCheck Practice 2 very soon for more R exercises!"
  },
  {
    "objectID": "advancedMethods/Lecture1/introProbAndStats.html#references",
    "href": "advancedMethods/Lecture1/introProbAndStats.html#references",
    "title": "Introduction to important concepts in Statistics",
    "section": "References",
    "text": "References\n\n\n\n\nVisit my website\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "advancedMethods/Lecture2/Distributions.html#references",
    "href": "advancedMethods/Lecture2/Distributions.html#references",
    "title": "Probability distributions and random variables",
    "section": "References",
    "text": "References\n\n\n\n\nVisit my website\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nBartoš, F., Sarafoglou, A., Godmann, H. R., Sahrani, A., Leunk, D. K., Gui, P. Y., Voss, D., Ullah, K., Zoubek, M. J., Nippold, F., Aust, F., Vieira, F. F., Islam, C.-G., Zoubek, A. J., Shabani, S., Petter, J., Roos, I. B., Finnemann, A., Lob, A. B., … Wagenmakers, E.-J. (2023). Fair coins tend to land on the same side they started: Evidence from 350,757 flips. https://arxiv.org/abs/2310.04153\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  },
  {
    "objectID": "advancedMethods/Lecture3/hypothesisTesting.html#references",
    "href": "advancedMethods/Lecture3/hypothesisTesting.html#references",
    "title": "Compare means and many more stories",
    "section": "References",
    "text": "References\n\n\n\n\nVisit my website\n\n\n\n\n\n\n\n \n\n\n \n\n\n \n\n\n\n\nWestfall, P. H., & Henning, K. S. (2013). Understanding advanced statistical methods. CRC Press Boca Raton, FL, USA:"
  }
]